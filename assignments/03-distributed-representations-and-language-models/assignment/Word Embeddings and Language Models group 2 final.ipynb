{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data, you can download the file on canvas under files/03-lab-data/wiki-corpus.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import toolz\n",
    "from toolz import keyfilter, valfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def corpus_reader(data_path):\n",
    "    with open(data_path) as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom torch Dataset\n",
    "class ContextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 raw_texts,\n",
    "                 window_size=4,\n",
    "                 tokenizer=None,\n",
    "                 lower=True,\n",
    "                 min_freq=0,\n",
    "                 unk_label='<unk>',\n",
    "                 pad_label='<pad>',\n",
    "                 verbose=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_texts: list of texts\n",
    "        \"\"\"\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        \n",
    "        self.window_size= 4\n",
    "        self.min_freq = min_freq\n",
    "        self.unk_label, self.unk_idx = unk_label, 0\n",
    "        self.pad_label, self.pad_idx = pad_label, 1\n",
    "        \n",
    "        \n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = lambda x: x.replace('.',' ').replace(',', ' ').replace('\"', ' ').split() #simple punctuation and whitespace\n",
    "        self.lower = lower\n",
    "        \n",
    "        # Load Data\n",
    "        self.center, self.context = self.create_dataset(raw_texts)\n",
    "        \n",
    "        # Vocabulary\n",
    "        self.word_to_idx = dict()\n",
    "        self.word_to_idx[self.unk_label] = self.unk_idx\n",
    "        self.word_to_idx[self.pad_label] = self.pad_idx\n",
    "        self.word_to_idx.update({word:idx+max(self.word_to_idx.values())+1 for idx, word in enumerate(np.unique(self.center))})\n",
    "\n",
    "        # Word Counts\n",
    "        # center represents every instance of every word in the text\n",
    "        self.word_counts = pd.value_counts(self.center).to_dict()\n",
    "        \n",
    "        # Apply min_freq - this was Adam's fix, we do not use it though because it was implemented after we had trained our model\n",
    "        # on 50k sentences and we did not want to spend this much time and energy on retraining it for this assignment.\n",
    "        if min_freq:\n",
    "            to_remove = valfilter(lambda x: x <= min_freq, self.word_counts)\n",
    "            self.word_to_idx = keyfilter(lambda x: x not in to_remove.keys(), self.word_to_idx)\n",
    "            self.word_to_idx = {k:i for i, k in enumerate(self.word_to_idx.keys())}\n",
    "            \n",
    "        self.idx_to_word = {v:k for k,v in self.word_to_idx.items()}   \n",
    "    \n",
    "    def get_window_context(self, position_index, sentence_array, window_size):\n",
    "        \"\"\"\n",
    "        returns: center_word, context_array\n",
    "\n",
    "        sentence_array = ['this', 'is', 'a', 'lab', 'filler', 'filler']\n",
    "        window_size = 4\n",
    "        position_index = 1\n",
    "\n",
    "        reutrns: is, [this, a, lab]\n",
    "        \"\"\"\n",
    "        window_range = int(np.ceil(window_size/2)) # Assume window_size is even otherwise round up. (3 -> 4)\n",
    "        context_indices = [int(i) for i in range(position_index-window_range, position_index+window_range+1) if (i>=0 and i!=position_index and i<len(sentence_array))] \n",
    "        context_array = [sentence_array[i] for i in context_indices]\n",
    "\n",
    "        return sentence_array[position_index], context_array\n",
    "    \n",
    "    def create_dataset(self, raw_texts):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        c = 0\n",
    "        contexts_dfs = []\n",
    "        for sent_id, text in enumerate(raw_texts):\n",
    "            \n",
    "            if self.verbose:\n",
    "                c+=1\n",
    "                if c%1000 == 0:\n",
    "                    t2 = time.time()\n",
    "                    print(f'{c}/{len(raw_texts)}, dt = {round(t2-t1,3)}')\n",
    "                    t1 = time.time()\n",
    "                \n",
    "            text = self.tokenize(text)\n",
    "            \n",
    "            windowed_contexts = pd.DataFrame([self.get_window_context(i, text, self.window_size) for i,_ in enumerate(text)], columns = ['center', 'context'])\n",
    "            windowed_contexts['sentence_id'] = sent_id\n",
    "            contexts_dfs.append(windowed_contexts) \n",
    "        \n",
    "        dataset = pd.concat(contexts_dfs, axis=0, ignore_index=True)\n",
    "        \n",
    "        return np.array(dataset['center']), np.array(dataset['context'])\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        if self.lower:\n",
    "            string = string.lower()\n",
    "        return np.array(self.tokenizer(string))\n",
    "\n",
    "    def idx2word(self, idx_or_list):\n",
    "        try:\n",
    "            len(idx_or_list)\n",
    "            return [self.idx_to_word.get(idx, self.unk_label) for idx in idx_or_list]\n",
    "        except:\n",
    "            return self.idx_to_word.get(idx_or_list, self.unk_label)\n",
    "    \n",
    "    def word2idx(self, word_or_list):\n",
    "        \n",
    "        if isinstance(word_or_list, str):\n",
    "            return self.word_to_idx.get(word_or_list, self.unk_idx)\n",
    "        else:\n",
    "            return [self.word_to_idx.get(w, self.unk_idx) for w in word_or_list]\n",
    "        \n",
    "    def get_shuffled_data(self, seed=None): \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        rng = np.arange(len(self))\n",
    "        np.random.shuffle(rng)\n",
    "        return self.center[rng], self.context[rng]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.center[idx], self.context[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.center)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST data parser\n",
      "----\n",
      "get_window_context test: ('is', ['this', 'a', 'lab'])\n",
      "center:context\n",
      "a : ['bomb', 'was']\n",
      "bomb : ['a', 'was', 'thrown']\n",
      "was : ['a', 'bomb', 'thrown', 'by']\n",
      "thrown : ['bomb', 'was', 'by', 'an']\n",
      "by : ['was', 'thrown', 'an', 'unknown']\n",
      "{'<unk>': 0, '<pad>': 1, 'a': 2, 'an': 3, 'bomb': 4, 'by': 5, 'coherence': 6, 'hypothesizes': 7, 'limited': 8, 'party': 9, 'that': 10, 'theory': 11, 'thrown': 12, 'unknown': 13, 'was': 14}\n"
     ]
    }
   ],
   "source": [
    "print('TEST data parser')\n",
    "test_dataset = ContextDataset(['A bomb was thrown by an unknown party', 'coherence theory hypothesizes that a limited'], window_size=4)\n",
    "print('----')\n",
    "print('get_window_context test:', test_dataset.get_window_context(1, ['this', 'is', 'a', 'lab', 'filler', 'filler'], 4))\n",
    "        \n",
    "\n",
    "print('center:context')\n",
    "for i,j in zip(test_dataset.center[0:5], test_dataset.context[0:5]):\n",
    "    print(i,':', j)\n",
    "\n",
    "print(test_dataset.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/50000, dt = 1.074\n",
      "2000/50000, dt = 1.019\n",
      "3000/50000, dt = 1.094\n",
      "4000/50000, dt = 1.019\n",
      "5000/50000, dt = 1.149\n",
      "6000/50000, dt = 0.995\n",
      "7000/50000, dt = 1.209\n",
      "8000/50000, dt = 1.008\n",
      "9000/50000, dt = 1.25\n",
      "10000/50000, dt = 1.021\n",
      "11000/50000, dt = 1.071\n",
      "12000/50000, dt = 1.175\n",
      "13000/50000, dt = 1.001\n",
      "14000/50000, dt = 0.999\n",
      "15000/50000, dt = 1.007\n",
      "16000/50000, dt = 1.261\n",
      "17000/50000, dt = 1.0\n",
      "18000/50000, dt = 1.026\n",
      "19000/50000, dt = 1.003\n",
      "20000/50000, dt = 1.42\n",
      "21000/50000, dt = 1.035\n",
      "22000/50000, dt = 1.009\n",
      "23000/50000, dt = 0.973\n",
      "24000/50000, dt = 0.944\n",
      "25000/50000, dt = 0.966\n",
      "26000/50000, dt = 1.281\n",
      "27000/50000, dt = 0.971\n",
      "28000/50000, dt = 0.941\n",
      "29000/50000, dt = 0.937\n",
      "30000/50000, dt = 0.929\n",
      "31000/50000, dt = 0.949\n",
      "32000/50000, dt = 0.937\n",
      "33000/50000, dt = 0.922\n",
      "34000/50000, dt = 1.526\n",
      "35000/50000, dt = 0.93\n",
      "36000/50000, dt = 0.977\n",
      "37000/50000, dt = 0.954\n",
      "38000/50000, dt = 0.969\n",
      "39000/50000, dt = 0.966\n",
      "40000/50000, dt = 0.973\n",
      "41000/50000, dt = 0.98\n",
      "42000/50000, dt = 0.959\n",
      "43000/50000, dt = 1.575\n",
      "44000/50000, dt = 0.975\n",
      "45000/50000, dt = 0.978\n",
      "46000/50000, dt = 0.963\n",
      "47000/50000, dt = 0.961\n",
      "48000/50000, dt = 0.966\n",
      "49000/50000, dt = 0.965\n",
      "50000/50000, dt = 0.958\n"
     ]
    }
   ],
   "source": [
    "# Actual Data\n",
    "\n",
    "data_path = './wiki-corpus.50000.txt'\n",
    "raw_texts = corpus_reader(data_path)\n",
    "dataset = ContextDataset(raw_texts, window_size=4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchist : ['historian', 'george']\n",
      "historian : ['anarchist', 'george', 'woodcock']\n",
      "george : ['anarchist', 'historian', 'woodcock', 'reports']\n",
      "woodcock : ['historian', 'george', 'reports', 'that']\n",
      "reports : ['george', 'woodcock', 'that', 'the']\n",
      "that : ['woodcock', 'reports', 'the', 'annual']\n",
      "the : ['reports', 'that', 'annual', 'congress']\n",
      "annual : ['that', 'the', 'congress', 'of']\n",
      "congress : ['the', 'annual', 'of', 'the']\n",
      "of : ['annual', 'congress', 'the', 'international']\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(dataset.center[0:10], dataset.context[0:10]):\n",
    "    print(i,':', j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOOD**\n",
    "- It is fair too assume wikipedia doesnt contain too much human bias (opinions, racist/discriminatory/abusive language).\n",
    "- There is a chance for a big variety of topics, ergo, a big variety in vocabulary.  \n",
    "\n",
    "**BAD**\n",
    "- 50k sentences spread out over all of wikipedia might too small a sample, as the contexts can vary a lot.\n",
    "- We may only get some contexts for certain words that have more than one meaning (e.g. if we do not get sentences about banks as financial institution but only about riverbanks, then the \"meaning\" of bank will be skewed). We do not get the full range of the use of a word. \n",
    "- The model may be missing a lot of the basic vocabulary if the sentences happen to describe highly abstract or complicated topics (so we will have the word for photosynthesis, but not for chair, for example).\n",
    "- There is a small chance of ending up with sentences from only one semantic area which would also limit how general our model can be.\n",
    "- The writing in Wikipedia reflects only one style of language really (this can be a + too depending on what the aim of the model is).\n",
    "- We get a lot of numbers, symbols, and non-English words which is not great for English language modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loader\n",
    "\n",
    "from collections import namedtuple\n",
    "from toolz import take\n",
    "from itertools import tee\n",
    "\n",
    "def get_loader(dataset, batch_size, shuffle=True):\n",
    "    \n",
    "    if shuffle:\n",
    "        centers, contexts = dataset.get_shuffled_data() \n",
    "    else:\n",
    "        centers = dataset.center\n",
    "        contexts = dataset.context\n",
    "        \n",
    "    centers = iter(centers)\n",
    "    contexts = iter(contexts)\n",
    "    Batch = namedtuple('Batch', ['contexts', 'centers'])\n",
    "    for i in range(int(len(dataset)/batch_size)+1):\n",
    "        batch_centers = take(batch_size, centers)\n",
    "        if not batch_centers:\n",
    "            continue\n",
    "        batch_contexts, bc2 = tee(take(batch_size, contexts))\n",
    "        \n",
    "        #padding and word2idx - this part we think is refered to as Collate in torch.\n",
    "        batch_centers = torch.tensor(dataset.word2idx(batch_centers))\n",
    "        max_len = max([len(x) for x in bc2])\n",
    "        batch_contexts = torch.tensor([dataset.word2idx(s)+[dataset.pad_idx]*(max_len-len(s)) for s in batch_contexts])\n",
    "\n",
    "        yield Batch(batch_contexts, batch_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST (CUSTOM) LOADER\n",
      "torch.Size([5, 4])\n",
      "[2, 4, 14, 12, 5]\n",
      "['a', 'bomb', 'was', 'thrown', 'by']\n",
      "[['bomb', 'was', '<pad>', '<pad>'], ['a', 'was', 'thrown', '<pad>'], ['a', 'bomb', 'thrown', 'by'], ['bomb', 'was', 'by', 'an'], ['was', 'thrown', 'an', 'unknown']]\n",
      "\n",
      "torch.Size([5, 4])\n",
      "[3, 13, 9, 6, 11]\n",
      "['an', 'unknown', 'party', 'coherence', 'theory']\n",
      "[['thrown', 'by', 'unknown', 'party'], ['by', 'an', 'party', '<pad>'], ['an', 'unknown', '<pad>', '<pad>'], ['theory', 'hypothesizes', '<pad>', '<pad>'], ['coherence', 'hypothesizes', 'that', '<pad>']]\n",
      "\n",
      "torch.Size([4, 4])\n",
      "[7, 10, 2, 8]\n",
      "['hypothesizes', 'that', 'a', 'limited']\n",
      "[['coherence', 'theory', 'that', 'a'], ['theory', 'hypothesizes', 'a', 'limited'], ['hypothesizes', 'that', 'limited', '<pad>'], ['that', 'a', '<pad>', '<pad>']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TEST (CUSTOM) LOADER')\n",
    "test_loader = get_loader(test_dataset, 5, shuffle=False)\n",
    "for cont, cent in test_loader:\n",
    "    print(np.shape(cont)) # Note: batch_first\n",
    "    print(cent.tolist())\n",
    "    print(test_dataset.idx2word(cent.tolist()))\n",
    "    print([test_dataset.idx2word(c) for c in cont.tolist()])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good**\n",
    "\n",
    "+ We will not get separate instances for the same word depending on whether or not it was the first word in the sentence - e.g. dog and Dog will not be separate embeddings, which is good, because it is the same word.\n",
    "+ If we use a more advanced tokenizer, it may be able to keep proper nouns capitalized to avoid the phenomenon described in the \"bad\" section.\n",
    "\n",
    "**Bad**\n",
    "\n",
    "+ There may be cases where the uppercase and the lowercase of the same spelling are, in fact, different words: Patty is short for Patricia, but a patty is what you put in a burger; Billy is a name, but a billy goat is a male goat; Brown is a surname, but brown is a color; New York is a city, but a new york is someone's new Yorkshire terrier perhaps (funnily enough in Poland there is a big city the name of which literally translates to \"boat\" - prime example of a situation where context and uppercase matters). Or, even better, it and IT will be grouped together when they are definitely not the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, dataset, vocab_size, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.prediction = nn.Linear(embedding_dim, vocab_size) # we dont need context_size*embedding_dim because we will apply summation before.\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def forward(self, context):\n",
    "        # context (B, S)\n",
    "        embedded_context = self.embeddings(context) #(B, S, D)\n",
    "        projection = self.projection_function(embedded_context)\n",
    "        predictions = self.prediction(projection)\n",
    "        #return predictions\n",
    "        output = F.softmax(predictions, dim=0) # why do we need this? what about log softmax?\n",
    "        return output\n",
    "        \n",
    "    def projection_function(self, xs):\n",
    "        #This is alternative to another linear layer. Basically linear layer with same coefficient.\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs, dim=1) \n",
    "        return xs_sum\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        idx_word = self.dataset.word2idx(word)\n",
    "        word_embedding = self.embeddings(torch.tensor(idx_word))\n",
    "        \n",
    "        return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "# what do these actually mean and how do changes in them affect our results? what are optimal/commonly used values? \n",
    "word_embeddings_hyperparameters = {'epochs':3,\n",
    "                                   'batch_size':16,\n",
    "                                   'embedding_size':128, #What is this?\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':128}\n",
    "\n",
    "word_embeddings_hyperparameters = {'epochs':5,\n",
    "                                   'batch_size':4096,\n",
    "                                   'learning_rate':0.005,\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 : Average Loss = 11.22902\n",
      "Batch 1 : Average Loss = 11.22902\n",
      "Batch 2 : Average Loss = 11.22901\n",
      "Batch 3 : Average Loss = 11.22898\n",
      "Batch 4 : Average Loss = 11.22897\n",
      "Batch 5 : Average Loss = 11.22894\n",
      "Batch 6 : Average Loss = 11.22891\n",
      "Batch 7 : Average Loss = 11.22888\n",
      "Batch 8 : Average Loss = 11.22882\n",
      "Batch 9 : Average Loss = 11.22874\n",
      "Batch 10 : Average Loss = 11.22862\n",
      "Batch 11 : Average Loss = 11.22846\n",
      "Batch 12 : Average Loss = 11.22826\n",
      "Batch 13 : Average Loss = 11.22805\n",
      "Batch 14 : Average Loss = 11.22782\n",
      "Batch 15 : Average Loss = 11.22753\n",
      "Batch 16 : Average Loss = 11.22723\n",
      "Batch 17 : Average Loss = 11.22686\n",
      "Batch 18 : Average Loss = 11.22647\n",
      "Batch 19 : Average Loss = 11.22604\n",
      "Batch 20 : Average Loss = 11.22565\n",
      "Batch 21 : Average Loss = 11.22527\n",
      "Batch 22 : Average Loss = 11.22495\n",
      "Batch 23 : Average Loss = 11.22461\n",
      "Batch 24 : Average Loss = 11.22419\n",
      "Batch 25 : Average Loss = 11.22381\n",
      "Batch 26 : Average Loss = 11.22345\n",
      "Batch 27 : Average Loss = 11.2231\n",
      "Batch 28 : Average Loss = 11.22272\n",
      "Batch 29 : Average Loss = 11.22236\n",
      "Batch 30 : Average Loss = 11.22202\n",
      "Batch 31 : Average Loss = 11.22172\n",
      "Batch 32 : Average Loss = 11.2214\n",
      "Batch 33 : Average Loss = 11.22112\n",
      "Batch 34 : Average Loss = 11.22082\n",
      "Batch 35 : Average Loss = 11.22053\n",
      "Batch 36 : Average Loss = 11.22031\n",
      "Batch 37 : Average Loss = 11.22002\n",
      "Batch 38 : Average Loss = 11.21972\n",
      "Batch 39 : Average Loss = 11.21944\n",
      "Batch 40 : Average Loss = 11.21916\n",
      "Batch 41 : Average Loss = 11.21888\n",
      "Batch 42 : Average Loss = 11.2186\n",
      "Batch 43 : Average Loss = 11.21838\n",
      "Batch 44 : Average Loss = 11.21824\n",
      "Batch 45 : Average Loss = 11.21804\n",
      "Batch 46 : Average Loss = 11.21782\n",
      "Batch 47 : Average Loss = 11.21761\n",
      "Batch 48 : Average Loss = 11.21738\n",
      "Batch 49 : Average Loss = 11.21716\n",
      "Batch 50 : Average Loss = 11.21699\n",
      "Batch 51 : Average Loss = 11.21677\n",
      "Batch 52 : Average Loss = 11.2166\n",
      "Batch 53 : Average Loss = 11.21639\n",
      "Batch 54 : Average Loss = 11.21621\n",
      "Batch 55 : Average Loss = 11.216\n",
      "Batch 56 : Average Loss = 11.21586\n",
      "Batch 57 : Average Loss = 11.21566\n",
      "Batch 58 : Average Loss = 11.21549\n",
      "Batch 59 : Average Loss = 11.21534\n",
      "Batch 60 : Average Loss = 11.21515\n",
      "Batch 61 : Average Loss = 11.21501\n",
      "Batch 62 : Average Loss = 11.2149\n",
      "Batch 63 : Average Loss = 11.21474\n",
      "Batch 64 : Average Loss = 11.2146\n",
      "Batch 65 : Average Loss = 11.21444\n",
      "Batch 66 : Average Loss = 11.2143\n",
      "Batch 67 : Average Loss = 11.21415\n",
      "Batch 68 : Average Loss = 11.21401\n",
      "Batch 69 : Average Loss = 11.21387\n",
      "Batch 70 : Average Loss = 11.21371\n",
      "Batch 71 : Average Loss = 11.21355\n",
      "Batch 72 : Average Loss = 11.21341\n",
      "Batch 73 : Average Loss = 11.21325\n",
      "Batch 74 : Average Loss = 11.21312\n",
      "Batch 75 : Average Loss = 11.21298\n",
      "Batch 76 : Average Loss = 11.21286\n",
      "Batch 77 : Average Loss = 11.21274\n",
      "Batch 78 : Average Loss = 11.21263\n",
      "Batch 79 : Average Loss = 11.21255\n",
      "Batch 80 : Average Loss = 11.21244\n",
      "Batch 81 : Average Loss = 11.2123\n",
      "Batch 82 : Average Loss = 11.21218\n",
      "Batch 83 : Average Loss = 11.21208\n",
      "Batch 84 : Average Loss = 11.21196\n",
      "Batch 85 : Average Loss = 11.21183\n",
      "Batch 86 : Average Loss = 11.21171\n",
      "Batch 87 : Average Loss = 11.21156\n",
      "Batch 88 : Average Loss = 11.21146\n",
      "Batch 89 : Average Loss = 11.21135\n",
      "Batch 90 : Average Loss = 11.21125\n",
      "Batch 91 : Average Loss = 11.21112\n",
      "Batch 92 : Average Loss = 11.21099\n",
      "Batch 93 : Average Loss = 11.2109\n",
      "Batch 94 : Average Loss = 11.21079\n",
      "Batch 95 : Average Loss = 11.2107\n",
      "Batch 96 : Average Loss = 11.21059\n",
      "Batch 97 : Average Loss = 11.21052\n",
      "Batch 98 : Average Loss = 11.21042\n",
      "Batch 99 : Average Loss = 11.21033\n",
      "Batch 100 : Average Loss = 11.21026\n",
      "Batch 101 : Average Loss = 11.21015\n",
      "Batch 102 : Average Loss = 11.21008\n",
      "Batch 103 : Average Loss = 11.20999\n",
      "Batch 104 : Average Loss = 11.2099\n",
      "Batch 105 : Average Loss = 11.20981\n",
      "Batch 106 : Average Loss = 11.20972\n",
      "Batch 107 : Average Loss = 11.20965\n",
      "Batch 108 : Average Loss = 11.20959\n",
      "Batch 109 : Average Loss = 11.20949\n",
      "Batch 110 : Average Loss = 11.20941\n",
      "Batch 111 : Average Loss = 11.20933\n",
      "Batch 112 : Average Loss = 11.20925\n",
      "Batch 113 : Average Loss = 11.20918\n",
      "Batch 114 : Average Loss = 11.20911\n",
      "Batch 115 : Average Loss = 11.20902\n",
      "Batch 116 : Average Loss = 11.20896\n",
      "Batch 117 : Average Loss = 11.20888\n",
      "Batch 118 : Average Loss = 11.2088\n",
      "Batch 119 : Average Loss = 11.20872\n",
      "Batch 120 : Average Loss = 11.20865\n",
      "Batch 121 : Average Loss = 11.20857\n",
      "Batch 122 : Average Loss = 11.20851\n",
      "Batch 123 : Average Loss = 11.20842\n",
      "Batch 124 : Average Loss = 11.20836\n",
      "Batch 125 : Average Loss = 11.2083\n",
      "Batch 126 : Average Loss = 11.20823\n",
      "Batch 127 : Average Loss = 11.20818\n",
      "Batch 128 : Average Loss = 11.20812\n",
      "Batch 129 : Average Loss = 11.20806\n",
      "Batch 130 : Average Loss = 11.208\n",
      "Batch 131 : Average Loss = 11.20795\n",
      "Batch 132 : Average Loss = 11.20788\n",
      "Batch 133 : Average Loss = 11.20781\n",
      "Batch 134 : Average Loss = 11.20774\n",
      "Batch 135 : Average Loss = 11.20768\n",
      "Batch 136 : Average Loss = 11.20762\n",
      "Batch 137 : Average Loss = 11.20758\n",
      "Batch 138 : Average Loss = 11.20752\n",
      "Batch 139 : Average Loss = 11.20749\n",
      "Batch 140 : Average Loss = 11.20745\n",
      "Batch 141 : Average Loss = 11.2074\n",
      "Batch 142 : Average Loss = 11.20735\n",
      "Batch 143 : Average Loss = 11.20728\n",
      "Batch 144 : Average Loss = 11.20722\n",
      "Batch 145 : Average Loss = 11.20718\n",
      "Batch 146 : Average Loss = 11.20713\n",
      "Batch 147 : Average Loss = 11.2071\n",
      "Batch 148 : Average Loss = 11.20706\n",
      "Batch 149 : Average Loss = 11.20699\n",
      "Batch 150 : Average Loss = 11.20694\n",
      "Batch 151 : Average Loss = 11.20687\n",
      "Batch 152 : Average Loss = 11.20681\n",
      "Batch 153 : Average Loss = 11.20676\n",
      "Batch 154 : Average Loss = 11.20672\n",
      "Batch 155 : Average Loss = 11.20668\n",
      "Batch 156 : Average Loss = 11.20663\n",
      "Batch 157 : Average Loss = 11.20662\n",
      "Batch 158 : Average Loss = 11.20658\n",
      "Batch 159 : Average Loss = 11.20652\n",
      "Batch 160 : Average Loss = 11.20646\n",
      "Batch 161 : Average Loss = 11.20642\n",
      "Batch 162 : Average Loss = 11.20636\n",
      "Batch 163 : Average Loss = 11.20631\n",
      "Batch 164 : Average Loss = 11.20625\n",
      "Batch 165 : Average Loss = 11.20621\n",
      "Batch 166 : Average Loss = 11.20617\n",
      "Batch 167 : Average Loss = 11.20614\n",
      "Batch 168 : Average Loss = 11.20608\n",
      "Batch 169 : Average Loss = 11.20605\n",
      "Batch 170 : Average Loss = 11.206\n",
      "Batch 171 : Average Loss = 11.20596\n",
      "Batch 172 : Average Loss = 11.20592\n",
      "Batch 173 : Average Loss = 11.20588\n",
      "Batch 174 : Average Loss = 11.20584\n",
      "Batch 175 : Average Loss = 11.20577\n",
      "Batch 176 : Average Loss = 11.20574\n",
      "Batch 177 : Average Loss = 11.20569\n",
      "Batch 178 : Average Loss = 11.20566\n",
      "Batch 179 : Average Loss = 11.20562\n",
      "Batch 180 : Average Loss = 11.20556\n",
      "Batch 181 : Average Loss = 11.20552\n",
      "Batch 182 : Average Loss = 11.20548\n",
      "Batch 183 : Average Loss = 11.20541\n",
      "Batch 184 : Average Loss = 11.20537\n",
      "Batch 185 : Average Loss = 11.20531\n",
      "Batch 186 : Average Loss = 11.20527\n",
      "Batch 187 : Average Loss = 11.20523\n",
      "Batch 188 : Average Loss = 11.20518\n",
      "Batch 189 : Average Loss = 11.20513\n",
      "Batch 190 : Average Loss = 11.20511\n",
      "Batch 191 : Average Loss = 11.20507\n",
      "Batch 192 : Average Loss = 11.20504\n",
      "Batch 193 : Average Loss = 11.205\n",
      "Batch 194 : Average Loss = 11.20496\n",
      "Batch 195 : Average Loss = 11.20491\n",
      "Batch 196 : Average Loss = 11.20487\n",
      "Batch 197 : Average Loss = 11.20483\n",
      "Batch 198 : Average Loss = 11.20478\n",
      "Batch 199 : Average Loss = 11.20475\n",
      "Batch 200 : Average Loss = 11.20469\n",
      "Batch 201 : Average Loss = 11.20467\n",
      "Batch 202 : Average Loss = 11.20463\n",
      "Batch 203 : Average Loss = 11.20458\n",
      "Batch 204 : Average Loss = 11.20454\n",
      "Batch 205 : Average Loss = 11.20451\n",
      "Batch 206 : Average Loss = 11.20447\n",
      "Batch 207 : Average Loss = 11.20442\n",
      "Batch 208 : Average Loss = 11.20439\n",
      "Batch 209 : Average Loss = 11.20436\n",
      "Batch 210 : Average Loss = 11.20432\n",
      "Batch 211 : Average Loss = 11.20427\n",
      "Batch 212 : Average Loss = 11.20422\n",
      "Batch 213 : Average Loss = 11.20417\n",
      "Batch 214 : Average Loss = 11.20415\n",
      "Batch 215 : Average Loss = 11.20412\n",
      "Batch 216 : Average Loss = 11.2041\n",
      "Batch 217 : Average Loss = 11.20405\n",
      "Batch 218 : Average Loss = 11.20403\n",
      "Batch 219 : Average Loss = 11.20399\n",
      "Batch 220 : Average Loss = 11.20396\n",
      "Batch 221 : Average Loss = 11.20393\n",
      "Batch 222 : Average Loss = 11.20389\n",
      "Batch 223 : Average Loss = 11.20385\n",
      "Batch 224 : Average Loss = 11.20381\n",
      "Batch 225 : Average Loss = 11.20378\n",
      "Batch 226 : Average Loss = 11.20373\n",
      "Batch 227 : Average Loss = 11.2037\n",
      "Batch 228 : Average Loss = 11.20367\n",
      "Batch 229 : Average Loss = 11.20363\n",
      "Batch 230 : Average Loss = 11.20361\n",
      "Batch 231 : Average Loss = 11.20356\n",
      "Batch 232 : Average Loss = 11.20352\n",
      "Batch 233 : Average Loss = 11.20349\n",
      "Batch 234 : Average Loss = 11.20345\n",
      "Batch 235 : Average Loss = 11.20342\n",
      "Batch 236 : Average Loss = 11.20339\n",
      "Batch 237 : Average Loss = 11.20335\n",
      "Batch 238 : Average Loss = 11.20333\n",
      "Batch 239 : Average Loss = 11.20329\n",
      "Batch 240 : Average Loss = 11.20326\n",
      "Batch 241 : Average Loss = 11.20323\n",
      "Batch 242 : Average Loss = 11.2032\n",
      "Batch 243 : Average Loss = 11.20317\n",
      "Batch 244 : Average Loss = 11.20315\n",
      "Batch 245 : Average Loss = 11.2031\n",
      "Batch 246 : Average Loss = 11.20307\n",
      "Batch 247 : Average Loss = 11.20304\n",
      "Batch 248 : Average Loss = 11.20301\n",
      "Batch 249 : Average Loss = 11.20299\n",
      "Batch 250 : Average Loss = 11.20294\n",
      "Batch 251 : Average Loss = 11.20291\n",
      "Batch 252 : Average Loss = 11.20287\n",
      "Batch 253 : Average Loss = 11.20284\n",
      "Batch 254 : Average Loss = 11.2028\n",
      "Batch 255 : Average Loss = 11.20277\n",
      "Batch 256 : Average Loss = 11.20274\n",
      "Batch 257 : Average Loss = 11.20271\n",
      "Batch 258 : Average Loss = 11.20268\n",
      "Batch 259 : Average Loss = 11.20265\n",
      "Batch 260 : Average Loss = 11.20262\n",
      "Batch 261 : Average Loss = 11.20259\n",
      "Batch 262 : Average Loss = 11.20257\n",
      "Batch 263 : Average Loss = 11.20254\n",
      "Batch 264 : Average Loss = 11.20252\n",
      "Batch 265 : Average Loss = 11.20249\n",
      "Batch 266 : Average Loss = 11.20246\n",
      "Batch 267 : Average Loss = 11.20242\n",
      "Batch 268 : Average Loss = 11.2024\n",
      "Batch 269 : Average Loss = 11.20237\n",
      "Batch 270 : Average Loss = 11.20234\n",
      "Batch 271 : Average Loss = 11.20232\n",
      "Batch 272 : Average Loss = 11.2023\n",
      "Batch 273 : Average Loss = 11.20226\n",
      "Batch 274 : Average Loss = 11.20222\n",
      "Batch 275 : Average Loss = 11.2022\n",
      "Batch 276 : Average Loss = 11.20217\n",
      "Batch 277 : Average Loss = 11.20214\n",
      "Batch 278 : Average Loss = 11.20212\n",
      "Batch 279 : Average Loss = 11.20209\n",
      "Batch 280 : Average Loss = 11.20207\n",
      "Batch 281 : Average Loss = 11.20202\n",
      "Epoch 0 : Average Loss = 11.20202\n",
      "Batch 0 : Average Loss = 11.17103\n",
      "Batch 1 : Average Loss = 11.17194\n",
      "Batch 2 : Average Loss = 11.17291\n",
      "Batch 3 : Average Loss = 11.17172\n",
      "Batch 4 : Average Loss = 11.17154\n",
      "Batch 5 : Average Loss = 11.17233\n",
      "Batch 6 : Average Loss = 11.17185\n",
      "Batch 7 : Average Loss = 11.17175\n",
      "Batch 8 : Average Loss = 11.17248\n",
      "Batch 9 : Average Loss = 11.1721\n",
      "Batch 10 : Average Loss = 11.17183\n",
      "Batch 11 : Average Loss = 11.17163\n",
      "Batch 12 : Average Loss = 11.1717\n",
      "Batch 13 : Average Loss = 11.1718\n",
      "Batch 14 : Average Loss = 11.17195\n",
      "Batch 15 : Average Loss = 11.17176\n",
      "Batch 16 : Average Loss = 11.17167\n",
      "Batch 17 : Average Loss = 11.17131\n",
      "Batch 18 : Average Loss = 11.17136\n",
      "Batch 19 : Average Loss = 11.17141\n",
      "Batch 20 : Average Loss = 11.17123\n",
      "Batch 21 : Average Loss = 11.1712\n",
      "Batch 22 : Average Loss = 11.17101\n",
      "Batch 23 : Average Loss = 11.1711\n",
      "Batch 24 : Average Loss = 11.17109\n",
      "Batch 25 : Average Loss = 11.17107\n",
      "Batch 26 : Average Loss = 11.17103\n",
      "Batch 27 : Average Loss = 11.17104\n",
      "Batch 28 : Average Loss = 11.17095\n",
      "Batch 29 : Average Loss = 11.17111\n",
      "Batch 30 : Average Loss = 11.17101\n",
      "Batch 31 : Average Loss = 11.17096\n",
      "Batch 32 : Average Loss = 11.17104\n",
      "Batch 33 : Average Loss = 11.17101\n",
      "Batch 34 : Average Loss = 11.1712\n",
      "Batch 35 : Average Loss = 11.17114\n",
      "Batch 36 : Average Loss = 11.17116\n",
      "Batch 37 : Average Loss = 11.1711\n",
      "Batch 38 : Average Loss = 11.17113\n",
      "Batch 39 : Average Loss = 11.17126\n",
      "Batch 40 : Average Loss = 11.17134\n",
      "Batch 41 : Average Loss = 11.17133\n",
      "Batch 42 : Average Loss = 11.17143\n",
      "Batch 43 : Average Loss = 11.1714\n",
      "Batch 44 : Average Loss = 11.17145\n",
      "Batch 45 : Average Loss = 11.17145\n",
      "Batch 46 : Average Loss = 11.17144\n",
      "Batch 47 : Average Loss = 11.17148\n",
      "Batch 48 : Average Loss = 11.17147\n",
      "Batch 49 : Average Loss = 11.17144\n",
      "Batch 50 : Average Loss = 11.17145\n",
      "Batch 51 : Average Loss = 11.17142\n",
      "Batch 52 : Average Loss = 11.17133\n",
      "Batch 53 : Average Loss = 11.1714\n",
      "Batch 54 : Average Loss = 11.17132\n",
      "Batch 55 : Average Loss = 11.1713\n",
      "Batch 56 : Average Loss = 11.1713\n",
      "Batch 57 : Average Loss = 11.17125\n",
      "Batch 58 : Average Loss = 11.17107\n",
      "Batch 59 : Average Loss = 11.17113\n",
      "Batch 60 : Average Loss = 11.17108\n",
      "Batch 61 : Average Loss = 11.17117\n",
      "Batch 62 : Average Loss = 11.17126\n",
      "Batch 63 : Average Loss = 11.17121\n",
      "Batch 64 : Average Loss = 11.17118\n",
      "Batch 65 : Average Loss = 11.17112\n",
      "Batch 66 : Average Loss = 11.17115\n",
      "Batch 67 : Average Loss = 11.17113\n",
      "Batch 68 : Average Loss = 11.17114\n",
      "Batch 69 : Average Loss = 11.17123\n",
      "Batch 70 : Average Loss = 11.17125\n",
      "Batch 71 : Average Loss = 11.17132\n",
      "Batch 72 : Average Loss = 11.17131\n",
      "Batch 73 : Average Loss = 11.17138\n",
      "Batch 74 : Average Loss = 11.17138\n",
      "Batch 75 : Average Loss = 11.17136\n",
      "Batch 76 : Average Loss = 11.1714\n",
      "Batch 77 : Average Loss = 11.1714\n",
      "Batch 78 : Average Loss = 11.17143\n",
      "Batch 79 : Average Loss = 11.17143\n",
      "Batch 80 : Average Loss = 11.17138\n",
      "Batch 81 : Average Loss = 11.17145\n",
      "Batch 82 : Average Loss = 11.1715\n",
      "Batch 83 : Average Loss = 11.17155\n",
      "Batch 84 : Average Loss = 11.17151\n",
      "Batch 85 : Average Loss = 11.17153\n",
      "Batch 86 : Average Loss = 11.17149\n",
      "Batch 87 : Average Loss = 11.17152\n",
      "Batch 88 : Average Loss = 11.17152\n",
      "Batch 89 : Average Loss = 11.17152\n",
      "Batch 90 : Average Loss = 11.17155\n",
      "Batch 91 : Average Loss = 11.17158\n",
      "Batch 92 : Average Loss = 11.17162\n",
      "Batch 93 : Average Loss = 11.17163\n",
      "Batch 94 : Average Loss = 11.17167\n",
      "Batch 95 : Average Loss = 11.17173\n",
      "Batch 96 : Average Loss = 11.17172\n",
      "Batch 97 : Average Loss = 11.1717\n",
      "Batch 98 : Average Loss = 11.17174\n",
      "Batch 99 : Average Loss = 11.17172\n",
      "Batch 100 : Average Loss = 11.17172\n",
      "Batch 101 : Average Loss = 11.17172\n",
      "Batch 102 : Average Loss = 11.17168\n",
      "Batch 103 : Average Loss = 11.17168\n",
      "Batch 104 : Average Loss = 11.17168\n",
      "Batch 105 : Average Loss = 11.17173\n",
      "Batch 106 : Average Loss = 11.17174\n",
      "Batch 107 : Average Loss = 11.17175\n",
      "Batch 108 : Average Loss = 11.17175\n",
      "Batch 109 : Average Loss = 11.17177\n",
      "Batch 110 : Average Loss = 11.17171\n",
      "Batch 111 : Average Loss = 11.17168\n",
      "Batch 112 : Average Loss = 11.17169\n",
      "Batch 113 : Average Loss = 11.17172\n",
      "Batch 114 : Average Loss = 11.17174\n",
      "Batch 115 : Average Loss = 11.17172\n",
      "Batch 116 : Average Loss = 11.17173\n",
      "Batch 117 : Average Loss = 11.17174\n",
      "Batch 118 : Average Loss = 11.17178\n",
      "Batch 119 : Average Loss = 11.17183\n",
      "Batch 120 : Average Loss = 11.17183\n",
      "Batch 121 : Average Loss = 11.17185\n",
      "Batch 122 : Average Loss = 11.17185\n",
      "Batch 123 : Average Loss = 11.17184\n",
      "Batch 124 : Average Loss = 11.17187\n",
      "Batch 125 : Average Loss = 11.17187\n",
      "Batch 126 : Average Loss = 11.17186\n",
      "Batch 127 : Average Loss = 11.17184\n",
      "Batch 128 : Average Loss = 11.17182\n",
      "Batch 129 : Average Loss = 11.17182\n",
      "Batch 130 : Average Loss = 11.17185\n",
      "Batch 131 : Average Loss = 11.17187\n",
      "Batch 132 : Average Loss = 11.17191\n",
      "Batch 133 : Average Loss = 11.17192\n",
      "Batch 134 : Average Loss = 11.17194\n",
      "Batch 135 : Average Loss = 11.17192\n",
      "Batch 136 : Average Loss = 11.17195\n",
      "Batch 137 : Average Loss = 11.17195\n",
      "Batch 138 : Average Loss = 11.17194\n",
      "Batch 139 : Average Loss = 11.17194\n",
      "Batch 140 : Average Loss = 11.17192\n",
      "Batch 141 : Average Loss = 11.17192\n",
      "Batch 142 : Average Loss = 11.17196\n",
      "Batch 143 : Average Loss = 11.17198\n",
      "Batch 144 : Average Loss = 11.17198\n",
      "Batch 145 : Average Loss = 11.17201\n",
      "Batch 146 : Average Loss = 11.17203\n",
      "Batch 147 : Average Loss = 11.17206\n",
      "Batch 148 : Average Loss = 11.17204\n",
      "Batch 149 : Average Loss = 11.17204\n",
      "Batch 150 : Average Loss = 11.17208\n",
      "Batch 151 : Average Loss = 11.17207\n",
      "Batch 152 : Average Loss = 11.17209\n",
      "Batch 153 : Average Loss = 11.17212\n",
      "Batch 154 : Average Loss = 11.17211\n",
      "Batch 155 : Average Loss = 11.17208\n",
      "Batch 156 : Average Loss = 11.17207\n",
      "Batch 157 : Average Loss = 11.17208\n",
      "Batch 158 : Average Loss = 11.17208\n",
      "Batch 159 : Average Loss = 11.17209\n",
      "Batch 160 : Average Loss = 11.17212\n",
      "Batch 161 : Average Loss = 11.17212\n",
      "Batch 162 : Average Loss = 11.17211\n",
      "Batch 163 : Average Loss = 11.17214\n",
      "Batch 164 : Average Loss = 11.17217\n",
      "Batch 165 : Average Loss = 11.1722\n",
      "Batch 166 : Average Loss = 11.1722\n",
      "Batch 167 : Average Loss = 11.17219\n",
      "Batch 168 : Average Loss = 11.17218\n",
      "Batch 169 : Average Loss = 11.17222\n",
      "Batch 170 : Average Loss = 11.17219\n",
      "Batch 171 : Average Loss = 11.17222\n",
      "Batch 172 : Average Loss = 11.17224\n",
      "Batch 173 : Average Loss = 11.17226\n",
      "Batch 174 : Average Loss = 11.17225\n",
      "Batch 175 : Average Loss = 11.17225\n",
      "Batch 176 : Average Loss = 11.17224\n",
      "Batch 177 : Average Loss = 11.17224\n",
      "Batch 178 : Average Loss = 11.17228\n",
      "Batch 179 : Average Loss = 11.1723\n",
      "Batch 180 : Average Loss = 11.17231\n",
      "Batch 181 : Average Loss = 11.17229\n",
      "Batch 182 : Average Loss = 11.17231\n",
      "Batch 183 : Average Loss = 11.17233\n",
      "Batch 184 : Average Loss = 11.17233\n",
      "Batch 185 : Average Loss = 11.17235\n",
      "Batch 186 : Average Loss = 11.17236\n",
      "Batch 187 : Average Loss = 11.17238\n",
      "Batch 188 : Average Loss = 11.17242\n",
      "Batch 189 : Average Loss = 11.17242\n",
      "Batch 190 : Average Loss = 11.17243\n",
      "Batch 191 : Average Loss = 11.17244\n",
      "Batch 192 : Average Loss = 11.17245\n",
      "Batch 193 : Average Loss = 11.17246\n",
      "Batch 194 : Average Loss = 11.17249\n",
      "Batch 195 : Average Loss = 11.17251\n",
      "Batch 196 : Average Loss = 11.17253\n",
      "Batch 197 : Average Loss = 11.17255\n",
      "Batch 198 : Average Loss = 11.17255\n",
      "Batch 199 : Average Loss = 11.17255\n",
      "Batch 200 : Average Loss = 11.17257\n",
      "Batch 201 : Average Loss = 11.17258\n",
      "Batch 202 : Average Loss = 11.17258\n",
      "Batch 203 : Average Loss = 11.17258\n",
      "Batch 204 : Average Loss = 11.17259\n",
      "Batch 205 : Average Loss = 11.17261\n",
      "Batch 206 : Average Loss = 11.17261\n",
      "Batch 207 : Average Loss = 11.17264\n",
      "Batch 208 : Average Loss = 11.17263\n",
      "Batch 209 : Average Loss = 11.17264\n",
      "Batch 210 : Average Loss = 11.17265\n",
      "Batch 211 : Average Loss = 11.17266\n",
      "Batch 212 : Average Loss = 11.17268\n",
      "Batch 213 : Average Loss = 11.1727\n",
      "Batch 214 : Average Loss = 11.17271\n",
      "Batch 215 : Average Loss = 11.17271\n",
      "Batch 216 : Average Loss = 11.17275\n",
      "Batch 217 : Average Loss = 11.17274\n",
      "Batch 218 : Average Loss = 11.17273\n",
      "Batch 219 : Average Loss = 11.17272\n",
      "Batch 220 : Average Loss = 11.1727\n",
      "Batch 221 : Average Loss = 11.1727\n",
      "Batch 222 : Average Loss = 11.17272\n",
      "Batch 223 : Average Loss = 11.17272\n",
      "Batch 224 : Average Loss = 11.17273\n",
      "Batch 225 : Average Loss = 11.17273\n",
      "Batch 226 : Average Loss = 11.17275\n",
      "Batch 227 : Average Loss = 11.17275\n",
      "Batch 228 : Average Loss = 11.17277\n",
      "Batch 229 : Average Loss = 11.17279\n",
      "Batch 230 : Average Loss = 11.1728\n",
      "Batch 231 : Average Loss = 11.17282\n",
      "Batch 232 : Average Loss = 11.17282\n",
      "Batch 233 : Average Loss = 11.17285\n",
      "Batch 234 : Average Loss = 11.17287\n",
      "Batch 235 : Average Loss = 11.17286\n",
      "Batch 236 : Average Loss = 11.17288\n",
      "Batch 237 : Average Loss = 11.17289\n",
      "Batch 238 : Average Loss = 11.17287\n",
      "Batch 239 : Average Loss = 11.17286\n",
      "Batch 240 : Average Loss = 11.17288\n",
      "Batch 241 : Average Loss = 11.17288\n",
      "Batch 242 : Average Loss = 11.17288\n",
      "Batch 243 : Average Loss = 11.1729\n",
      "Batch 244 : Average Loss = 11.17293\n",
      "Batch 245 : Average Loss = 11.17291\n",
      "Batch 246 : Average Loss = 11.17293\n",
      "Batch 247 : Average Loss = 11.17294\n",
      "Batch 248 : Average Loss = 11.17293\n",
      "Batch 249 : Average Loss = 11.17293\n",
      "Batch 250 : Average Loss = 11.17294\n",
      "Batch 251 : Average Loss = 11.17292\n",
      "Batch 252 : Average Loss = 11.1729\n",
      "Batch 253 : Average Loss = 11.17291\n",
      "Batch 254 : Average Loss = 11.17292\n",
      "Batch 255 : Average Loss = 11.17293\n",
      "Batch 256 : Average Loss = 11.17293\n",
      "Batch 257 : Average Loss = 11.17292\n",
      "Batch 258 : Average Loss = 11.17293\n",
      "Batch 259 : Average Loss = 11.17294\n",
      "Batch 260 : Average Loss = 11.17294\n",
      "Batch 261 : Average Loss = 11.17294\n",
      "Batch 262 : Average Loss = 11.17295\n",
      "Batch 263 : Average Loss = 11.17295\n",
      "Batch 264 : Average Loss = 11.17297\n",
      "Batch 265 : Average Loss = 11.17297\n",
      "Batch 266 : Average Loss = 11.17298\n",
      "Batch 267 : Average Loss = 11.17299\n",
      "Batch 268 : Average Loss = 11.173\n",
      "Batch 269 : Average Loss = 11.17301\n",
      "Batch 270 : Average Loss = 11.17302\n",
      "Batch 271 : Average Loss = 11.17302\n",
      "Batch 272 : Average Loss = 11.17302\n",
      "Batch 273 : Average Loss = 11.17303\n",
      "Batch 274 : Average Loss = 11.17305\n",
      "Batch 275 : Average Loss = 11.17306\n",
      "Batch 276 : Average Loss = 11.17306\n",
      "Batch 277 : Average Loss = 11.17307\n",
      "Batch 278 : Average Loss = 11.17307\n",
      "Batch 279 : Average Loss = 11.17306\n",
      "Batch 280 : Average Loss = 11.17309\n",
      "Batch 281 : Average Loss = 11.17307\n",
      "Epoch 1 : Average Loss = 11.17307\n",
      "Batch 0 : Average Loss = 11.12011\n",
      "Batch 1 : Average Loss = 11.12001\n",
      "Batch 2 : Average Loss = 11.12053\n",
      "Batch 3 : Average Loss = 11.11951\n",
      "Batch 4 : Average Loss = 11.11927\n",
      "Batch 5 : Average Loss = 11.12056\n",
      "Batch 6 : Average Loss = 11.11964\n",
      "Batch 7 : Average Loss = 11.11918\n",
      "Batch 8 : Average Loss = 11.11927\n",
      "Batch 9 : Average Loss = 11.11927\n",
      "Batch 10 : Average Loss = 11.11969\n",
      "Batch 11 : Average Loss = 11.12\n",
      "Batch 12 : Average Loss = 11.11995\n",
      "Batch 13 : Average Loss = 11.11973\n",
      "Batch 14 : Average Loss = 11.11933\n",
      "Batch 15 : Average Loss = 11.11885\n",
      "Batch 16 : Average Loss = 11.11914\n",
      "Batch 17 : Average Loss = 11.11914\n",
      "Batch 18 : Average Loss = 11.11909\n",
      "Batch 19 : Average Loss = 11.11857\n",
      "Batch 20 : Average Loss = 11.11825\n",
      "Batch 21 : Average Loss = 11.1184\n",
      "Batch 22 : Average Loss = 11.11843\n",
      "Batch 23 : Average Loss = 11.11869\n",
      "Batch 24 : Average Loss = 11.11861\n",
      "Batch 25 : Average Loss = 11.11897\n",
      "Batch 26 : Average Loss = 11.11912\n",
      "Batch 27 : Average Loss = 11.11914\n",
      "Batch 28 : Average Loss = 11.11892\n",
      "Batch 29 : Average Loss = 11.11912\n",
      "Batch 30 : Average Loss = 11.11907\n",
      "Batch 31 : Average Loss = 11.11889\n",
      "Batch 32 : Average Loss = 11.11881\n",
      "Batch 33 : Average Loss = 11.11879\n",
      "Batch 34 : Average Loss = 11.11876\n",
      "Batch 35 : Average Loss = 11.11869\n",
      "Batch 36 : Average Loss = 11.11884\n",
      "Batch 37 : Average Loss = 11.11896\n",
      "Batch 38 : Average Loss = 11.11907\n",
      "Batch 39 : Average Loss = 11.11917\n",
      "Batch 40 : Average Loss = 11.11907\n",
      "Batch 41 : Average Loss = 11.11909\n",
      "Batch 42 : Average Loss = 11.11897\n",
      "Batch 43 : Average Loss = 11.11882\n",
      "Batch 44 : Average Loss = 11.11866\n",
      "Batch 45 : Average Loss = 11.11849\n",
      "Batch 46 : Average Loss = 11.11845\n",
      "Batch 47 : Average Loss = 11.11843\n",
      "Batch 48 : Average Loss = 11.11861\n",
      "Batch 49 : Average Loss = 11.11868\n",
      "Batch 50 : Average Loss = 11.11864\n",
      "Batch 51 : Average Loss = 11.11868\n",
      "Batch 52 : Average Loss = 11.11873\n",
      "Batch 53 : Average Loss = 11.11874\n",
      "Batch 54 : Average Loss = 11.11872\n",
      "Batch 55 : Average Loss = 11.11883\n",
      "Batch 56 : Average Loss = 11.11892\n",
      "Batch 57 : Average Loss = 11.11899\n",
      "Batch 58 : Average Loss = 11.11895\n",
      "Batch 59 : Average Loss = 11.11884\n",
      "Batch 60 : Average Loss = 11.11892\n",
      "Batch 61 : Average Loss = 11.11893\n",
      "Batch 62 : Average Loss = 11.11889\n",
      "Batch 63 : Average Loss = 11.11888\n",
      "Batch 64 : Average Loss = 11.11897\n",
      "Batch 65 : Average Loss = 11.11904\n",
      "Batch 66 : Average Loss = 11.11906\n",
      "Batch 67 : Average Loss = 11.11913\n",
      "Batch 68 : Average Loss = 11.11922\n",
      "Batch 69 : Average Loss = 11.11924\n",
      "Batch 70 : Average Loss = 11.11927\n",
      "Batch 71 : Average Loss = 11.11929\n",
      "Batch 72 : Average Loss = 11.11924\n",
      "Batch 73 : Average Loss = 11.11934\n",
      "Batch 74 : Average Loss = 11.11925\n",
      "Batch 75 : Average Loss = 11.11928\n",
      "Batch 76 : Average Loss = 11.11924\n",
      "Batch 77 : Average Loss = 11.11921\n",
      "Batch 78 : Average Loss = 11.11917\n",
      "Batch 79 : Average Loss = 11.11914\n",
      "Batch 80 : Average Loss = 11.11913\n",
      "Batch 81 : Average Loss = 11.11916\n",
      "Batch 82 : Average Loss = 11.11918\n",
      "Batch 83 : Average Loss = 11.11927\n",
      "Batch 84 : Average Loss = 11.1192\n",
      "Batch 85 : Average Loss = 11.11927\n",
      "Batch 86 : Average Loss = 11.11936\n",
      "Batch 87 : Average Loss = 11.1194\n",
      "Batch 88 : Average Loss = 11.11943\n",
      "Batch 89 : Average Loss = 11.1194\n",
      "Batch 90 : Average Loss = 11.11943\n",
      "Batch 91 : Average Loss = 11.11945\n",
      "Batch 92 : Average Loss = 11.11943\n",
      "Batch 93 : Average Loss = 11.11953\n",
      "Batch 94 : Average Loss = 11.11955\n",
      "Batch 95 : Average Loss = 11.1196\n",
      "Batch 96 : Average Loss = 11.11959\n",
      "Batch 97 : Average Loss = 11.11964\n",
      "Batch 98 : Average Loss = 11.11967\n",
      "Batch 99 : Average Loss = 11.1197\n",
      "Batch 100 : Average Loss = 11.11975\n",
      "Batch 101 : Average Loss = 11.11969\n",
      "Batch 102 : Average Loss = 11.11961\n",
      "Batch 103 : Average Loss = 11.11973\n",
      "Batch 104 : Average Loss = 11.11974\n",
      "Batch 105 : Average Loss = 11.11972\n",
      "Batch 106 : Average Loss = 11.11972\n",
      "Batch 107 : Average Loss = 11.11968\n",
      "Batch 108 : Average Loss = 11.11978\n",
      "Batch 109 : Average Loss = 11.11983\n",
      "Batch 110 : Average Loss = 11.1198\n",
      "Batch 111 : Average Loss = 11.11983\n",
      "Batch 112 : Average Loss = 11.11987\n",
      "Batch 113 : Average Loss = 11.1199\n",
      "Batch 114 : Average Loss = 11.11985\n",
      "Batch 115 : Average Loss = 11.11984\n",
      "Batch 116 : Average Loss = 11.11983\n",
      "Batch 117 : Average Loss = 11.11985\n",
      "Batch 118 : Average Loss = 11.11985\n",
      "Batch 119 : Average Loss = 11.11988\n",
      "Batch 120 : Average Loss = 11.11986\n",
      "Batch 121 : Average Loss = 11.11984\n",
      "Batch 122 : Average Loss = 11.11988\n",
      "Batch 123 : Average Loss = 11.11987\n",
      "Batch 124 : Average Loss = 11.11989\n",
      "Batch 125 : Average Loss = 11.11984\n",
      "Batch 126 : Average Loss = 11.11983\n",
      "Batch 127 : Average Loss = 11.11984\n",
      "Batch 128 : Average Loss = 11.11987\n",
      "Batch 129 : Average Loss = 11.11985\n",
      "Batch 130 : Average Loss = 11.11985\n",
      "Batch 131 : Average Loss = 11.11987\n",
      "Batch 132 : Average Loss = 11.11988\n",
      "Batch 133 : Average Loss = 11.11989\n",
      "Batch 134 : Average Loss = 11.11992\n",
      "Batch 135 : Average Loss = 11.11987\n",
      "Batch 136 : Average Loss = 11.11988\n",
      "Batch 137 : Average Loss = 11.11986\n",
      "Batch 138 : Average Loss = 11.11995\n",
      "Batch 139 : Average Loss = 11.11997\n",
      "Batch 140 : Average Loss = 11.12005\n",
      "Batch 141 : Average Loss = 11.12008\n",
      "Batch 142 : Average Loss = 11.12009\n",
      "Batch 143 : Average Loss = 11.12008\n",
      "Batch 144 : Average Loss = 11.12011\n",
      "Batch 145 : Average Loss = 11.12009\n",
      "Batch 146 : Average Loss = 11.12012\n",
      "Batch 147 : Average Loss = 11.12012\n",
      "Batch 148 : Average Loss = 11.12015\n",
      "Batch 149 : Average Loss = 11.12017\n",
      "Batch 150 : Average Loss = 11.1202\n",
      "Batch 151 : Average Loss = 11.12017\n",
      "Batch 152 : Average Loss = 11.12019\n",
      "Batch 153 : Average Loss = 11.12017\n",
      "Batch 154 : Average Loss = 11.12019\n",
      "Batch 155 : Average Loss = 11.12019\n",
      "Batch 156 : Average Loss = 11.12019\n",
      "Batch 157 : Average Loss = 11.12024\n",
      "Batch 158 : Average Loss = 11.12026\n",
      "Batch 159 : Average Loss = 11.12027\n",
      "Batch 160 : Average Loss = 11.12032\n",
      "Batch 161 : Average Loss = 11.12036\n",
      "Batch 162 : Average Loss = 11.1204\n",
      "Batch 163 : Average Loss = 11.12039\n",
      "Batch 164 : Average Loss = 11.12041\n",
      "Batch 165 : Average Loss = 11.12042\n",
      "Batch 166 : Average Loss = 11.12044\n",
      "Batch 167 : Average Loss = 11.12043\n",
      "Batch 168 : Average Loss = 11.12045\n",
      "Batch 169 : Average Loss = 11.12052\n",
      "Batch 170 : Average Loss = 11.12052\n",
      "Batch 171 : Average Loss = 11.12052\n",
      "Batch 172 : Average Loss = 11.1205\n",
      "Batch 173 : Average Loss = 11.12053\n",
      "Batch 174 : Average Loss = 11.12054\n",
      "Batch 175 : Average Loss = 11.12053\n",
      "Batch 176 : Average Loss = 11.12052\n",
      "Batch 177 : Average Loss = 11.12055\n",
      "Batch 178 : Average Loss = 11.12055\n",
      "Batch 179 : Average Loss = 11.1206\n",
      "Batch 180 : Average Loss = 11.12063\n",
      "Batch 181 : Average Loss = 11.12058\n",
      "Batch 182 : Average Loss = 11.12058\n",
      "Batch 183 : Average Loss = 11.1206\n",
      "Batch 184 : Average Loss = 11.12061\n",
      "Batch 185 : Average Loss = 11.12062\n",
      "Batch 186 : Average Loss = 11.12059\n",
      "Batch 187 : Average Loss = 11.12059\n",
      "Batch 188 : Average Loss = 11.12062\n",
      "Batch 189 : Average Loss = 11.1206\n",
      "Batch 190 : Average Loss = 11.12061\n",
      "Batch 191 : Average Loss = 11.12061\n",
      "Batch 192 : Average Loss = 11.1206\n",
      "Batch 193 : Average Loss = 11.12063\n",
      "Batch 194 : Average Loss = 11.1206\n",
      "Batch 195 : Average Loss = 11.12063\n",
      "Batch 196 : Average Loss = 11.12059\n",
      "Batch 197 : Average Loss = 11.12065\n",
      "Batch 198 : Average Loss = 11.12067\n",
      "Batch 199 : Average Loss = 11.12066\n",
      "Batch 200 : Average Loss = 11.12069\n",
      "Batch 201 : Average Loss = 11.12073\n",
      "Batch 202 : Average Loss = 11.12076\n",
      "Batch 203 : Average Loss = 11.12075\n",
      "Batch 204 : Average Loss = 11.12073\n",
      "Batch 205 : Average Loss = 11.12074\n",
      "Batch 206 : Average Loss = 11.12074\n",
      "Batch 207 : Average Loss = 11.12077\n",
      "Batch 208 : Average Loss = 11.12078\n",
      "Batch 209 : Average Loss = 11.1208\n",
      "Batch 210 : Average Loss = 11.12082\n",
      "Batch 211 : Average Loss = 11.12082\n",
      "Batch 212 : Average Loss = 11.12085\n",
      "Batch 213 : Average Loss = 11.12085\n",
      "Batch 214 : Average Loss = 11.12089\n",
      "Batch 215 : Average Loss = 11.12085\n",
      "Batch 216 : Average Loss = 11.12084\n",
      "Batch 217 : Average Loss = 11.12089\n",
      "Batch 218 : Average Loss = 11.12091\n",
      "Batch 219 : Average Loss = 11.12088\n",
      "Batch 220 : Average Loss = 11.12087\n",
      "Batch 221 : Average Loss = 11.12086\n",
      "Batch 222 : Average Loss = 11.12085\n",
      "Batch 223 : Average Loss = 11.12085\n",
      "Batch 224 : Average Loss = 11.12088\n",
      "Batch 225 : Average Loss = 11.12086\n",
      "Batch 226 : Average Loss = 11.12086\n",
      "Batch 227 : Average Loss = 11.12087\n",
      "Batch 228 : Average Loss = 11.12084\n",
      "Batch 229 : Average Loss = 11.12088\n",
      "Batch 230 : Average Loss = 11.12089\n",
      "Batch 231 : Average Loss = 11.12089\n",
      "Batch 232 : Average Loss = 11.12092\n",
      "Batch 233 : Average Loss = 11.12094\n",
      "Batch 234 : Average Loss = 11.12094\n",
      "Batch 235 : Average Loss = 11.12095\n",
      "Batch 236 : Average Loss = 11.12095\n",
      "Batch 237 : Average Loss = 11.12095\n",
      "Batch 238 : Average Loss = 11.12094\n",
      "Batch 239 : Average Loss = 11.12095\n",
      "Batch 240 : Average Loss = 11.12097\n",
      "Batch 241 : Average Loss = 11.12096\n",
      "Batch 242 : Average Loss = 11.12097\n",
      "Batch 243 : Average Loss = 11.12096\n",
      "Batch 244 : Average Loss = 11.12097\n",
      "Batch 245 : Average Loss = 11.12098\n",
      "Batch 246 : Average Loss = 11.12099\n",
      "Batch 247 : Average Loss = 11.12098\n",
      "Batch 248 : Average Loss = 11.12097\n",
      "Batch 249 : Average Loss = 11.12097\n",
      "Batch 250 : Average Loss = 11.121\n",
      "Batch 251 : Average Loss = 11.121\n",
      "Batch 252 : Average Loss = 11.12102\n",
      "Batch 253 : Average Loss = 11.12103\n",
      "Batch 254 : Average Loss = 11.12104\n",
      "Batch 255 : Average Loss = 11.12104\n",
      "Batch 256 : Average Loss = 11.12107\n",
      "Batch 257 : Average Loss = 11.12106\n",
      "Batch 258 : Average Loss = 11.12108\n",
      "Batch 259 : Average Loss = 11.12107\n",
      "Batch 260 : Average Loss = 11.12109\n",
      "Batch 261 : Average Loss = 11.12109\n",
      "Batch 262 : Average Loss = 11.12108\n",
      "Batch 263 : Average Loss = 11.1211\n",
      "Batch 264 : Average Loss = 11.12112\n",
      "Batch 265 : Average Loss = 11.12113\n",
      "Batch 266 : Average Loss = 11.12114\n",
      "Batch 267 : Average Loss = 11.12111\n",
      "Batch 268 : Average Loss = 11.12112\n",
      "Batch 269 : Average Loss = 11.12113\n",
      "Batch 270 : Average Loss = 11.12112\n",
      "Batch 271 : Average Loss = 11.12112\n",
      "Batch 272 : Average Loss = 11.12111\n",
      "Batch 273 : Average Loss = 11.12112\n",
      "Batch 274 : Average Loss = 11.12114\n",
      "Batch 275 : Average Loss = 11.12114\n",
      "Batch 276 : Average Loss = 11.12114\n",
      "Batch 277 : Average Loss = 11.12116\n",
      "Batch 278 : Average Loss = 11.12117\n",
      "Batch 279 : Average Loss = 11.12118\n",
      "Batch 280 : Average Loss = 11.12122\n",
      "Batch 281 : Average Loss = 11.1212\n",
      "Epoch 2 : Average Loss = 11.1212\n",
      "Batch 0 : Average Loss = 11.10747\n",
      "Batch 1 : Average Loss = 11.10806\n",
      "Batch 2 : Average Loss = 11.10634\n",
      "Batch 3 : Average Loss = 11.10625\n",
      "Batch 4 : Average Loss = 11.10507\n",
      "Batch 5 : Average Loss = 11.1045\n",
      "Batch 6 : Average Loss = 11.10352\n",
      "Batch 7 : Average Loss = 11.10318\n",
      "Batch 8 : Average Loss = 11.10383\n",
      "Batch 9 : Average Loss = 11.104\n",
      "Batch 10 : Average Loss = 11.10424\n",
      "Batch 11 : Average Loss = 11.10421\n",
      "Batch 12 : Average Loss = 11.1043\n",
      "Batch 13 : Average Loss = 11.10424\n",
      "Batch 14 : Average Loss = 11.10416\n",
      "Batch 15 : Average Loss = 11.10414\n",
      "Batch 16 : Average Loss = 11.10392\n",
      "Batch 17 : Average Loss = 11.10405\n",
      "Batch 18 : Average Loss = 11.10358\n",
      "Batch 19 : Average Loss = 11.10379\n",
      "Batch 20 : Average Loss = 11.10404\n",
      "Batch 21 : Average Loss = 11.10411\n",
      "Batch 22 : Average Loss = 11.10416\n",
      "Batch 23 : Average Loss = 11.10448\n",
      "Batch 24 : Average Loss = 11.10451\n",
      "Batch 25 : Average Loss = 11.1045\n",
      "Batch 26 : Average Loss = 11.10445\n",
      "Batch 27 : Average Loss = 11.10427\n",
      "Batch 28 : Average Loss = 11.10446\n",
      "Batch 29 : Average Loss = 11.10435\n",
      "Batch 30 : Average Loss = 11.1042\n",
      "Batch 31 : Average Loss = 11.10447\n",
      "Batch 32 : Average Loss = 11.10436\n",
      "Batch 33 : Average Loss = 11.10426\n",
      "Batch 34 : Average Loss = 11.10429\n",
      "Batch 35 : Average Loss = 11.10443\n",
      "Batch 36 : Average Loss = 11.10436\n",
      "Batch 37 : Average Loss = 11.10417\n",
      "Batch 38 : Average Loss = 11.10418\n",
      "Batch 39 : Average Loss = 11.10417\n",
      "Batch 40 : Average Loss = 11.10422\n",
      "Batch 41 : Average Loss = 11.10412\n",
      "Batch 42 : Average Loss = 11.10415\n",
      "Batch 43 : Average Loss = 11.1041\n",
      "Batch 44 : Average Loss = 11.10424\n",
      "Batch 45 : Average Loss = 11.10415\n",
      "Batch 46 : Average Loss = 11.10414\n",
      "Batch 47 : Average Loss = 11.1041\n",
      "Batch 48 : Average Loss = 11.10397\n",
      "Batch 49 : Average Loss = 11.10398\n",
      "Batch 50 : Average Loss = 11.10401\n",
      "Batch 51 : Average Loss = 11.10398\n",
      "Batch 52 : Average Loss = 11.10416\n",
      "Batch 53 : Average Loss = 11.10404\n",
      "Batch 54 : Average Loss = 11.10412\n",
      "Batch 55 : Average Loss = 11.10427\n",
      "Batch 56 : Average Loss = 11.10416\n",
      "Batch 57 : Average Loss = 11.10417\n",
      "Batch 58 : Average Loss = 11.10413\n",
      "Batch 59 : Average Loss = 11.10408\n",
      "Batch 60 : Average Loss = 11.1041\n",
      "Batch 61 : Average Loss = 11.10405\n",
      "Batch 62 : Average Loss = 11.10398\n",
      "Batch 63 : Average Loss = 11.10402\n",
      "Batch 64 : Average Loss = 11.10394\n",
      "Batch 65 : Average Loss = 11.10387\n",
      "Batch 66 : Average Loss = 11.10389\n",
      "Batch 67 : Average Loss = 11.10391\n",
      "Batch 68 : Average Loss = 11.1039\n",
      "Batch 69 : Average Loss = 11.1039\n",
      "Batch 70 : Average Loss = 11.10385\n",
      "Batch 71 : Average Loss = 11.10391\n",
      "Batch 72 : Average Loss = 11.10391\n",
      "Batch 73 : Average Loss = 11.10398\n",
      "Batch 74 : Average Loss = 11.10395\n",
      "Batch 75 : Average Loss = 11.1039\n",
      "Batch 76 : Average Loss = 11.10383\n",
      "Batch 77 : Average Loss = 11.10383\n",
      "Batch 78 : Average Loss = 11.10382\n",
      "Batch 79 : Average Loss = 11.10371\n",
      "Batch 80 : Average Loss = 11.10361\n",
      "Batch 81 : Average Loss = 11.10368\n",
      "Batch 82 : Average Loss = 11.1036\n",
      "Batch 83 : Average Loss = 11.10356\n",
      "Batch 84 : Average Loss = 11.10355\n",
      "Batch 85 : Average Loss = 11.10357\n",
      "Batch 86 : Average Loss = 11.10359\n",
      "Batch 87 : Average Loss = 11.10354\n",
      "Batch 88 : Average Loss = 11.10358\n",
      "Batch 89 : Average Loss = 11.10361\n",
      "Batch 90 : Average Loss = 11.10359\n",
      "Batch 91 : Average Loss = 11.10369\n",
      "Batch 92 : Average Loss = 11.10368\n",
      "Batch 93 : Average Loss = 11.10375\n",
      "Batch 94 : Average Loss = 11.10383\n",
      "Batch 95 : Average Loss = 11.10387\n",
      "Batch 96 : Average Loss = 11.10387\n",
      "Batch 97 : Average Loss = 11.10391\n",
      "Batch 98 : Average Loss = 11.1039\n",
      "Batch 99 : Average Loss = 11.10395\n",
      "Batch 100 : Average Loss = 11.104\n",
      "Batch 101 : Average Loss = 11.10398\n",
      "Batch 102 : Average Loss = 11.10393\n",
      "Batch 103 : Average Loss = 11.10388\n",
      "Batch 104 : Average Loss = 11.1039\n",
      "Batch 105 : Average Loss = 11.10388\n",
      "Batch 106 : Average Loss = 11.1039\n",
      "Batch 107 : Average Loss = 11.10383\n",
      "Batch 108 : Average Loss = 11.10385\n",
      "Batch 109 : Average Loss = 11.1039\n",
      "Batch 110 : Average Loss = 11.10391\n",
      "Batch 111 : Average Loss = 11.10384\n",
      "Batch 112 : Average Loss = 11.10381\n",
      "Batch 113 : Average Loss = 11.10383\n",
      "Batch 114 : Average Loss = 11.10379\n",
      "Batch 115 : Average Loss = 11.10382\n",
      "Batch 116 : Average Loss = 11.10393\n",
      "Batch 117 : Average Loss = 11.10392\n",
      "Batch 118 : Average Loss = 11.10387\n",
      "Batch 119 : Average Loss = 11.10392\n",
      "Batch 120 : Average Loss = 11.10395\n",
      "Batch 121 : Average Loss = 11.10394\n",
      "Batch 122 : Average Loss = 11.10395\n",
      "Batch 123 : Average Loss = 11.10398\n",
      "Batch 124 : Average Loss = 11.10402\n",
      "Batch 125 : Average Loss = 11.10401\n",
      "Batch 126 : Average Loss = 11.10405\n",
      "Batch 127 : Average Loss = 11.10407\n",
      "Batch 128 : Average Loss = 11.10407\n",
      "Batch 129 : Average Loss = 11.1041\n",
      "Batch 130 : Average Loss = 11.10407\n",
      "Batch 131 : Average Loss = 11.10412\n",
      "Batch 132 : Average Loss = 11.10409\n",
      "Batch 133 : Average Loss = 11.10413\n",
      "Batch 134 : Average Loss = 11.10412\n",
      "Batch 135 : Average Loss = 11.1041\n",
      "Batch 136 : Average Loss = 11.10408\n",
      "Batch 137 : Average Loss = 11.10405\n",
      "Batch 138 : Average Loss = 11.10409\n",
      "Batch 139 : Average Loss = 11.10412\n",
      "Batch 140 : Average Loss = 11.10417\n",
      "Batch 141 : Average Loss = 11.10419\n",
      "Batch 142 : Average Loss = 11.10421\n",
      "Batch 143 : Average Loss = 11.10421\n",
      "Batch 144 : Average Loss = 11.10419\n",
      "Batch 145 : Average Loss = 11.10424\n",
      "Batch 146 : Average Loss = 11.10425\n",
      "Batch 147 : Average Loss = 11.10423\n",
      "Batch 148 : Average Loss = 11.10426\n",
      "Batch 149 : Average Loss = 11.10426\n",
      "Batch 150 : Average Loss = 11.10433\n",
      "Batch 151 : Average Loss = 11.10431\n",
      "Batch 152 : Average Loss = 11.1043\n",
      "Batch 153 : Average Loss = 11.1043\n",
      "Batch 154 : Average Loss = 11.10432\n",
      "Batch 155 : Average Loss = 11.10436\n",
      "Batch 156 : Average Loss = 11.10438\n",
      "Batch 157 : Average Loss = 11.1044\n",
      "Batch 158 : Average Loss = 11.1044\n",
      "Batch 159 : Average Loss = 11.10441\n",
      "Batch 160 : Average Loss = 11.10442\n",
      "Batch 161 : Average Loss = 11.10441\n",
      "Batch 162 : Average Loss = 11.10438\n",
      "Batch 163 : Average Loss = 11.10437\n",
      "Batch 164 : Average Loss = 11.10434\n",
      "Batch 165 : Average Loss = 11.10434\n",
      "Batch 166 : Average Loss = 11.10435\n",
      "Batch 167 : Average Loss = 11.1044\n",
      "Batch 168 : Average Loss = 11.10443\n",
      "Batch 169 : Average Loss = 11.10443\n",
      "Batch 170 : Average Loss = 11.10444\n",
      "Batch 171 : Average Loss = 11.10447\n",
      "Batch 172 : Average Loss = 11.10451\n",
      "Batch 173 : Average Loss = 11.10452\n",
      "Batch 174 : Average Loss = 11.10455\n",
      "Batch 175 : Average Loss = 11.10456\n",
      "Batch 176 : Average Loss = 11.10455\n",
      "Batch 177 : Average Loss = 11.10458\n",
      "Batch 178 : Average Loss = 11.10461\n",
      "Batch 179 : Average Loss = 11.10461\n",
      "Batch 180 : Average Loss = 11.10461\n",
      "Batch 181 : Average Loss = 11.10462\n",
      "Batch 182 : Average Loss = 11.10461\n",
      "Batch 183 : Average Loss = 11.10463\n",
      "Batch 184 : Average Loss = 11.10466\n",
      "Batch 185 : Average Loss = 11.10466\n",
      "Batch 186 : Average Loss = 11.10466\n",
      "Batch 187 : Average Loss = 11.10467\n",
      "Batch 188 : Average Loss = 11.10469\n",
      "Batch 189 : Average Loss = 11.10468\n",
      "Batch 190 : Average Loss = 11.10468\n",
      "Batch 191 : Average Loss = 11.10466\n",
      "Batch 192 : Average Loss = 11.10467\n",
      "Batch 193 : Average Loss = 11.10468\n",
      "Batch 194 : Average Loss = 11.10466\n",
      "Batch 195 : Average Loss = 11.10467\n",
      "Batch 196 : Average Loss = 11.10465\n",
      "Batch 197 : Average Loss = 11.1047\n",
      "Batch 198 : Average Loss = 11.1047\n",
      "Batch 199 : Average Loss = 11.10473\n",
      "Batch 200 : Average Loss = 11.10473\n",
      "Batch 201 : Average Loss = 11.10475\n",
      "Batch 202 : Average Loss = 11.10475\n",
      "Batch 203 : Average Loss = 11.10475\n",
      "Batch 204 : Average Loss = 11.10479\n",
      "Batch 205 : Average Loss = 11.10479\n",
      "Batch 206 : Average Loss = 11.10479\n",
      "Batch 207 : Average Loss = 11.10478\n",
      "Batch 208 : Average Loss = 11.1048\n",
      "Batch 209 : Average Loss = 11.10482\n",
      "Batch 210 : Average Loss = 11.10486\n",
      "Batch 211 : Average Loss = 11.10487\n",
      "Batch 212 : Average Loss = 11.10488\n",
      "Batch 213 : Average Loss = 11.10486\n",
      "Batch 214 : Average Loss = 11.10486\n",
      "Batch 215 : Average Loss = 11.10487\n",
      "Batch 216 : Average Loss = 11.10487\n",
      "Batch 217 : Average Loss = 11.10487\n",
      "Batch 218 : Average Loss = 11.10488\n",
      "Batch 219 : Average Loss = 11.10485\n",
      "Batch 220 : Average Loss = 11.10487\n",
      "Batch 221 : Average Loss = 11.10492\n",
      "Batch 222 : Average Loss = 11.1049\n",
      "Batch 223 : Average Loss = 11.10491\n",
      "Batch 224 : Average Loss = 11.10492\n",
      "Batch 225 : Average Loss = 11.10494\n",
      "Batch 226 : Average Loss = 11.10494\n",
      "Batch 227 : Average Loss = 11.10497\n",
      "Batch 228 : Average Loss = 11.10499\n",
      "Batch 229 : Average Loss = 11.105\n",
      "Batch 230 : Average Loss = 11.10504\n",
      "Batch 231 : Average Loss = 11.10505\n",
      "Batch 232 : Average Loss = 11.10506\n",
      "Batch 233 : Average Loss = 11.10506\n",
      "Batch 234 : Average Loss = 11.10506\n",
      "Batch 235 : Average Loss = 11.10505\n",
      "Batch 236 : Average Loss = 11.1051\n",
      "Batch 237 : Average Loss = 11.10512\n",
      "Batch 238 : Average Loss = 11.1051\n",
      "Batch 239 : Average Loss = 11.10513\n",
      "Batch 240 : Average Loss = 11.10515\n",
      "Batch 241 : Average Loss = 11.10517\n",
      "Batch 242 : Average Loss = 11.10514\n",
      "Batch 243 : Average Loss = 11.10515\n",
      "Batch 244 : Average Loss = 11.10517\n",
      "Batch 245 : Average Loss = 11.10517\n",
      "Batch 246 : Average Loss = 11.10518\n",
      "Batch 247 : Average Loss = 11.10517\n",
      "Batch 248 : Average Loss = 11.10518\n",
      "Batch 249 : Average Loss = 11.10521\n",
      "Batch 250 : Average Loss = 11.10524\n",
      "Batch 251 : Average Loss = 11.10522\n",
      "Batch 252 : Average Loss = 11.10524\n",
      "Batch 253 : Average Loss = 11.10524\n",
      "Batch 254 : Average Loss = 11.10523\n",
      "Batch 255 : Average Loss = 11.10523\n",
      "Batch 256 : Average Loss = 11.10525\n",
      "Batch 257 : Average Loss = 11.10527\n",
      "Batch 258 : Average Loss = 11.10529\n",
      "Batch 259 : Average Loss = 11.10529\n",
      "Batch 260 : Average Loss = 11.10529\n",
      "Batch 261 : Average Loss = 11.10531\n",
      "Batch 262 : Average Loss = 11.1053\n",
      "Batch 263 : Average Loss = 11.10529\n",
      "Batch 264 : Average Loss = 11.1053\n",
      "Batch 265 : Average Loss = 11.10531\n",
      "Batch 266 : Average Loss = 11.10533\n",
      "Batch 267 : Average Loss = 11.10532\n",
      "Batch 268 : Average Loss = 11.10534\n",
      "Batch 269 : Average Loss = 11.10534\n",
      "Batch 270 : Average Loss = 11.10532\n",
      "Batch 271 : Average Loss = 11.10531\n",
      "Batch 272 : Average Loss = 11.10536\n",
      "Batch 273 : Average Loss = 11.10538\n",
      "Batch 274 : Average Loss = 11.10541\n",
      "Batch 275 : Average Loss = 11.10541\n",
      "Batch 276 : Average Loss = 11.10539\n",
      "Batch 277 : Average Loss = 11.10539\n",
      "Batch 278 : Average Loss = 11.10541\n",
      "Batch 279 : Average Loss = 11.10542\n",
      "Batch 280 : Average Loss = 11.10543\n",
      "Batch 281 : Average Loss = 11.10542\n",
      "Epoch 3 : Average Loss = 11.10542\n",
      "Batch 0 : Average Loss = 11.09345\n",
      "Batch 1 : Average Loss = 11.09646\n",
      "Batch 2 : Average Loss = 11.09522\n",
      "Batch 3 : Average Loss = 11.0947\n",
      "Batch 4 : Average Loss = 11.09484\n",
      "Batch 5 : Average Loss = 11.09475\n",
      "Batch 6 : Average Loss = 11.09587\n",
      "Batch 7 : Average Loss = 11.09566\n",
      "Batch 8 : Average Loss = 11.09525\n",
      "Batch 9 : Average Loss = 11.09503\n",
      "Batch 10 : Average Loss = 11.09614\n",
      "Batch 11 : Average Loss = 11.09642\n",
      "Batch 12 : Average Loss = 11.09675\n",
      "Batch 13 : Average Loss = 11.09694\n",
      "Batch 14 : Average Loss = 11.09734\n",
      "Batch 15 : Average Loss = 11.09757\n",
      "Batch 16 : Average Loss = 11.09757\n",
      "Batch 17 : Average Loss = 11.09695\n",
      "Batch 18 : Average Loss = 11.09706\n",
      "Batch 19 : Average Loss = 11.09738\n",
      "Batch 20 : Average Loss = 11.09721\n",
      "Batch 21 : Average Loss = 11.0974\n",
      "Batch 22 : Average Loss = 11.09755\n",
      "Batch 23 : Average Loss = 11.09738\n",
      "Batch 24 : Average Loss = 11.09752\n",
      "Batch 25 : Average Loss = 11.09753\n",
      "Batch 26 : Average Loss = 11.09762\n",
      "Batch 27 : Average Loss = 11.09752\n",
      "Batch 28 : Average Loss = 11.09772\n",
      "Batch 29 : Average Loss = 11.09785\n",
      "Batch 30 : Average Loss = 11.09738\n",
      "Batch 31 : Average Loss = 11.09748\n",
      "Batch 32 : Average Loss = 11.09749\n",
      "Batch 33 : Average Loss = 11.09734\n",
      "Batch 34 : Average Loss = 11.09732\n",
      "Batch 35 : Average Loss = 11.0972\n",
      "Batch 36 : Average Loss = 11.09693\n",
      "Batch 37 : Average Loss = 11.09695\n",
      "Batch 38 : Average Loss = 11.09708\n",
      "Batch 39 : Average Loss = 11.09702\n",
      "Batch 40 : Average Loss = 11.09713\n",
      "Batch 41 : Average Loss = 11.09717\n",
      "Batch 42 : Average Loss = 11.09712\n",
      "Batch 43 : Average Loss = 11.09725\n",
      "Batch 44 : Average Loss = 11.09718\n",
      "Batch 45 : Average Loss = 11.09698\n",
      "Batch 46 : Average Loss = 11.09707\n",
      "Batch 47 : Average Loss = 11.09698\n",
      "Batch 48 : Average Loss = 11.09682\n",
      "Batch 49 : Average Loss = 11.09696\n",
      "Batch 50 : Average Loss = 11.09709\n",
      "Batch 51 : Average Loss = 11.09707\n",
      "Batch 52 : Average Loss = 11.09714\n",
      "Batch 53 : Average Loss = 11.09716\n",
      "Batch 54 : Average Loss = 11.09726\n",
      "Batch 55 : Average Loss = 11.09722\n",
      "Batch 56 : Average Loss = 11.09708\n",
      "Batch 57 : Average Loss = 11.0971\n",
      "Batch 58 : Average Loss = 11.09711\n",
      "Batch 59 : Average Loss = 11.09721\n",
      "Batch 60 : Average Loss = 11.0973\n",
      "Batch 61 : Average Loss = 11.09726\n",
      "Batch 62 : Average Loss = 11.09742\n",
      "Batch 63 : Average Loss = 11.09747\n",
      "Batch 64 : Average Loss = 11.09755\n",
      "Batch 65 : Average Loss = 11.09763\n",
      "Batch 66 : Average Loss = 11.09755\n",
      "Batch 67 : Average Loss = 11.09747\n",
      "Batch 68 : Average Loss = 11.09747\n",
      "Batch 69 : Average Loss = 11.09753\n",
      "Batch 70 : Average Loss = 11.09746\n",
      "Batch 71 : Average Loss = 11.09754\n",
      "Batch 72 : Average Loss = 11.09753\n",
      "Batch 73 : Average Loss = 11.09757\n",
      "Batch 74 : Average Loss = 11.09743\n",
      "Batch 75 : Average Loss = 11.09742\n",
      "Batch 76 : Average Loss = 11.09758\n",
      "Batch 77 : Average Loss = 11.09756\n",
      "Batch 78 : Average Loss = 11.09761\n",
      "Batch 79 : Average Loss = 11.09756\n",
      "Batch 80 : Average Loss = 11.09752\n",
      "Batch 81 : Average Loss = 11.09755\n",
      "Batch 82 : Average Loss = 11.0976\n",
      "Batch 83 : Average Loss = 11.09757\n",
      "Batch 84 : Average Loss = 11.0976\n",
      "Batch 85 : Average Loss = 11.09767\n",
      "Batch 86 : Average Loss = 11.09759\n",
      "Batch 87 : Average Loss = 11.09763\n",
      "Batch 88 : Average Loss = 11.09757\n",
      "Batch 89 : Average Loss = 11.09747\n",
      "Batch 90 : Average Loss = 11.09756\n",
      "Batch 91 : Average Loss = 11.09759\n",
      "Batch 92 : Average Loss = 11.09756\n",
      "Batch 93 : Average Loss = 11.09756\n",
      "Batch 94 : Average Loss = 11.09755\n",
      "Batch 95 : Average Loss = 11.09757\n",
      "Batch 96 : Average Loss = 11.09752\n",
      "Batch 97 : Average Loss = 11.09758\n",
      "Batch 98 : Average Loss = 11.0976\n",
      "Batch 99 : Average Loss = 11.09759\n",
      "Batch 100 : Average Loss = 11.09751\n",
      "Batch 101 : Average Loss = 11.09755\n",
      "Batch 102 : Average Loss = 11.09759\n",
      "Batch 103 : Average Loss = 11.09762\n",
      "Batch 104 : Average Loss = 11.0976\n",
      "Batch 105 : Average Loss = 11.09763\n",
      "Batch 106 : Average Loss = 11.09768\n",
      "Batch 107 : Average Loss = 11.09764\n",
      "Batch 108 : Average Loss = 11.09768\n",
      "Batch 109 : Average Loss = 11.09775\n",
      "Batch 110 : Average Loss = 11.09765\n",
      "Batch 111 : Average Loss = 11.09769\n",
      "Batch 112 : Average Loss = 11.09765\n",
      "Batch 113 : Average Loss = 11.09767\n",
      "Batch 114 : Average Loss = 11.09772\n",
      "Batch 115 : Average Loss = 11.09773\n",
      "Batch 116 : Average Loss = 11.09776\n",
      "Batch 117 : Average Loss = 11.09773\n",
      "Batch 118 : Average Loss = 11.09767\n",
      "Batch 119 : Average Loss = 11.09761\n",
      "Batch 120 : Average Loss = 11.09762\n",
      "Batch 121 : Average Loss = 11.09754\n",
      "Batch 122 : Average Loss = 11.09754\n",
      "Batch 123 : Average Loss = 11.09762\n",
      "Batch 124 : Average Loss = 11.09756\n",
      "Batch 125 : Average Loss = 11.09754\n",
      "Batch 126 : Average Loss = 11.09759\n",
      "Batch 127 : Average Loss = 11.09757\n",
      "Batch 128 : Average Loss = 11.09761\n",
      "Batch 129 : Average Loss = 11.09759\n",
      "Batch 130 : Average Loss = 11.0976\n",
      "Batch 131 : Average Loss = 11.09767\n",
      "Batch 132 : Average Loss = 11.0977\n",
      "Batch 133 : Average Loss = 11.09769\n",
      "Batch 134 : Average Loss = 11.09769\n",
      "Batch 135 : Average Loss = 11.09771\n",
      "Batch 136 : Average Loss = 11.09773\n",
      "Batch 137 : Average Loss = 11.09779\n",
      "Batch 138 : Average Loss = 11.0977\n",
      "Batch 139 : Average Loss = 11.09769\n",
      "Batch 140 : Average Loss = 11.0977\n",
      "Batch 141 : Average Loss = 11.09767\n",
      "Batch 142 : Average Loss = 11.09767\n",
      "Batch 143 : Average Loss = 11.09761\n",
      "Batch 144 : Average Loss = 11.09763\n",
      "Batch 145 : Average Loss = 11.09763\n",
      "Batch 146 : Average Loss = 11.09763\n",
      "Batch 147 : Average Loss = 11.09762\n",
      "Batch 148 : Average Loss = 11.09767\n",
      "Batch 149 : Average Loss = 11.09775\n",
      "Batch 150 : Average Loss = 11.09778\n",
      "Batch 151 : Average Loss = 11.09775\n",
      "Batch 152 : Average Loss = 11.09773\n",
      "Batch 153 : Average Loss = 11.09776\n",
      "Batch 154 : Average Loss = 11.09771\n",
      "Batch 155 : Average Loss = 11.0977\n",
      "Batch 156 : Average Loss = 11.09772\n",
      "Batch 157 : Average Loss = 11.09771\n",
      "Batch 158 : Average Loss = 11.09775\n",
      "Batch 159 : Average Loss = 11.09778\n",
      "Batch 160 : Average Loss = 11.09772\n",
      "Batch 161 : Average Loss = 11.09762\n",
      "Batch 162 : Average Loss = 11.09764\n",
      "Batch 163 : Average Loss = 11.09772\n",
      "Batch 164 : Average Loss = 11.09781\n",
      "Batch 165 : Average Loss = 11.09778\n",
      "Batch 166 : Average Loss = 11.09781\n",
      "Batch 167 : Average Loss = 11.09781\n",
      "Batch 168 : Average Loss = 11.09779\n",
      "Batch 169 : Average Loss = 11.09786\n",
      "Batch 170 : Average Loss = 11.09785\n",
      "Batch 171 : Average Loss = 11.09789\n",
      "Batch 172 : Average Loss = 11.09787\n",
      "Batch 173 : Average Loss = 11.09788\n",
      "Batch 174 : Average Loss = 11.09789\n",
      "Batch 175 : Average Loss = 11.09785\n",
      "Batch 176 : Average Loss = 11.09785\n",
      "Batch 177 : Average Loss = 11.09779\n",
      "Batch 178 : Average Loss = 11.09779\n",
      "Batch 179 : Average Loss = 11.09774\n",
      "Batch 180 : Average Loss = 11.09774\n",
      "Batch 181 : Average Loss = 11.09773\n",
      "Batch 182 : Average Loss = 11.09775\n",
      "Batch 183 : Average Loss = 11.09775\n",
      "Batch 184 : Average Loss = 11.09779\n",
      "Batch 185 : Average Loss = 11.09784\n",
      "Batch 186 : Average Loss = 11.09784\n",
      "Batch 187 : Average Loss = 11.09786\n",
      "Batch 188 : Average Loss = 11.09788\n",
      "Batch 189 : Average Loss = 11.09791\n",
      "Batch 190 : Average Loss = 11.09795\n",
      "Batch 191 : Average Loss = 11.09792\n",
      "Batch 192 : Average Loss = 11.09792\n",
      "Batch 193 : Average Loss = 11.0979\n",
      "Batch 194 : Average Loss = 11.09793\n",
      "Batch 195 : Average Loss = 11.09791\n",
      "Batch 196 : Average Loss = 11.09792\n",
      "Batch 197 : Average Loss = 11.09793\n",
      "Batch 198 : Average Loss = 11.09791\n",
      "Batch 199 : Average Loss = 11.09789\n",
      "Batch 200 : Average Loss = 11.09789\n",
      "Batch 201 : Average Loss = 11.09791\n",
      "Batch 202 : Average Loss = 11.09794\n",
      "Batch 203 : Average Loss = 11.0979\n",
      "Batch 204 : Average Loss = 11.09792\n",
      "Batch 205 : Average Loss = 11.09794\n",
      "Batch 206 : Average Loss = 11.09795\n",
      "Batch 207 : Average Loss = 11.09794\n",
      "Batch 208 : Average Loss = 11.09796\n",
      "Batch 209 : Average Loss = 11.09797\n",
      "Batch 210 : Average Loss = 11.09797\n",
      "Batch 211 : Average Loss = 11.09797\n",
      "Batch 212 : Average Loss = 11.09795\n",
      "Batch 213 : Average Loss = 11.098\n",
      "Batch 214 : Average Loss = 11.09805\n",
      "Batch 215 : Average Loss = 11.09806\n",
      "Batch 216 : Average Loss = 11.09804\n",
      "Batch 217 : Average Loss = 11.09807\n",
      "Batch 218 : Average Loss = 11.09808\n",
      "Batch 219 : Average Loss = 11.0981\n",
      "Batch 220 : Average Loss = 11.09809\n",
      "Batch 221 : Average Loss = 11.0981\n",
      "Batch 222 : Average Loss = 11.0981\n",
      "Batch 223 : Average Loss = 11.09808\n",
      "Batch 224 : Average Loss = 11.09809\n",
      "Batch 225 : Average Loss = 11.09807\n",
      "Batch 226 : Average Loss = 11.09807\n",
      "Batch 227 : Average Loss = 11.09806\n",
      "Batch 228 : Average Loss = 11.09808\n",
      "Batch 229 : Average Loss = 11.09809\n",
      "Batch 230 : Average Loss = 11.0981\n",
      "Batch 231 : Average Loss = 11.0981\n",
      "Batch 232 : Average Loss = 11.09811\n",
      "Batch 233 : Average Loss = 11.09809\n",
      "Batch 234 : Average Loss = 11.09805\n",
      "Batch 235 : Average Loss = 11.09799\n",
      "Batch 236 : Average Loss = 11.09801\n",
      "Batch 237 : Average Loss = 11.09801\n",
      "Batch 238 : Average Loss = 11.09805\n",
      "Batch 239 : Average Loss = 11.09806\n",
      "Batch 240 : Average Loss = 11.09809\n",
      "Batch 241 : Average Loss = 11.09808\n",
      "Batch 242 : Average Loss = 11.09806\n",
      "Batch 243 : Average Loss = 11.09803\n",
      "Batch 244 : Average Loss = 11.098\n",
      "Batch 245 : Average Loss = 11.09804\n",
      "Batch 246 : Average Loss = 11.09808\n",
      "Batch 247 : Average Loss = 11.09809\n",
      "Batch 248 : Average Loss = 11.09808\n",
      "Batch 249 : Average Loss = 11.09807\n",
      "Batch 250 : Average Loss = 11.09808\n",
      "Batch 251 : Average Loss = 11.0981\n",
      "Batch 252 : Average Loss = 11.09809\n",
      "Batch 253 : Average Loss = 11.0981\n",
      "Batch 254 : Average Loss = 11.09811\n",
      "Batch 255 : Average Loss = 11.09813\n",
      "Batch 256 : Average Loss = 11.09813\n",
      "Batch 257 : Average Loss = 11.09812\n",
      "Batch 258 : Average Loss = 11.09817\n",
      "Batch 259 : Average Loss = 11.09816\n",
      "Batch 260 : Average Loss = 11.09817\n",
      "Batch 261 : Average Loss = 11.09817\n",
      "Batch 262 : Average Loss = 11.0982\n",
      "Batch 263 : Average Loss = 11.09818\n",
      "Batch 264 : Average Loss = 11.09815\n",
      "Batch 265 : Average Loss = 11.09815\n",
      "Batch 266 : Average Loss = 11.09815\n",
      "Batch 267 : Average Loss = 11.09815\n",
      "Batch 268 : Average Loss = 11.09817\n",
      "Batch 269 : Average Loss = 11.09817\n",
      "Batch 270 : Average Loss = 11.09815\n",
      "Batch 271 : Average Loss = 11.09816\n",
      "Batch 272 : Average Loss = 11.09816\n",
      "Batch 273 : Average Loss = 11.09815\n",
      "Batch 274 : Average Loss = 11.09814\n",
      "Batch 275 : Average Loss = 11.09815\n",
      "Batch 276 : Average Loss = 11.09815\n",
      "Batch 277 : Average Loss = 11.09815\n",
      "Batch 278 : Average Loss = 11.09812\n",
      "Batch 279 : Average Loss = 11.09815\n",
      "Batch 280 : Average Loss = 11.09816\n",
      "Batch 281 : Average Loss = 11.09818\n",
      "Epoch 4 : Average Loss = 11.09818\n"
     ]
    }
   ],
   "source": [
    "vocab = dataset.word_to_idx\n",
    "len_vocab = len(vocab)\n",
    "#dataset, vocab = get_data(...)\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(dataset, len_vocab, word_embeddings_hyperparameters['embedding_dim'], padding_idx=dataset.pad_idx)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    loader =  get_loader(dataset, word_embeddings_hyperparameters['batch_size']) # in your script it was outside of the loop, it did not work here, is it because it's custom?\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        context = batch.contexts\n",
    "        #target_word = batch.target_word\n",
    "        centers = batch.centers\n",
    "        #Note: this is not nessecary - the loss function understands.\n",
    "        #targets = torch.zeros(len(centers), len_vocab)\n",
    "        #for j, idx in enumerate(centers): #one hot encoding\n",
    "         #   targets[j][idx] = 1\n",
    "        \n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "        \n",
    "        loss = loss_fn(output, centers)\n",
    "        total_loss += loss.item()\n",
    "        print(f'Batch {i} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')\n",
    "        \n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in vanvas under files/03-l). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "203\n",
      "[[1.         0.13155658]\n",
      " [0.13155658 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "gs_path = 'wordsim_similarity_goldstandard.txt'\n",
    "vocab = dataset\n",
    "embeddings = cbow_model.embeddings(centers)\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "        \n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "            \n",
    "            # get the index for the word - not needed\n",
    "            # word1_idx = vocab.word2idx(word1)     \n",
    "            # word2_idx = vocab.word2idx(word2)\n",
    "\n",
    "            # get the embedding of the word\n",
    "            word1_emb = cbow_model.get_embedding(word1) # we do not need word indices\n",
    "            word2_emb = cbow_model.get_embedding(word2) \n",
    "            \n",
    "            cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "            \n",
    "            model_sims.append(cosine_similarity)\n",
    "    \n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "data, model = read_wordsim(gs_path, vocab, embeddings)\n",
    "\n",
    "# checking if they are of the same length\n",
    "print(len(data))\n",
    "print(len(model))\n",
    "\n",
    "pearson_correlation = np.corrcoef(np.array(data).astype(float), np.array(model).astype(float))\n",
    "            \n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's coefficient spans between -1 and 1, with -1 being a strong negative association, 1 being strong positive association, and 0 being no association. The association here is around 0.13. According to [https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php] it is in the lower range of small positive association. This means that there is some level of similarity between the gold standard judgements and the word embedding similarities. Since the embedding models are supposed to reflect similarities between words, this is not terribly bad, but naturally the higher the performance, the better. The difference may be due to the kind of data we created our embeddings on or the low number of epochs, but also due to human judgements (which I assume the gold standard to be) taking into account features other than the embeddings do.  \n",
    "\n",
    "What is worth pointing out is that when we ran the model on a smaller collection (5k sentences), we got correlation scores between 0.2 and 0.3, so higher. It is really interesting that the smaller model got better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 best performing pairs of words are:\n",
      "\tplane, car: 0.19662246108055115\n",
      "\tfurnace, stove: 0.20392639935016632\n",
      "\tlobster, wine: 0.20648574829101562\n",
      "\tstreet, block: 0.21518373489379883\n",
      "\ttiger, tiger: 1.0\n",
      "\tArafat, Jackson: 1.0\n",
      "\tJapanese, American: 1.0\n",
      "\tHarvard, Yale: 1.0\n",
      "\tMexico, Brazil: 1.0\n",
      "\n",
      "The bottom 10 worst performing pairs of words are:\n",
      "\twood, forest: -0.24051402509212494\n",
      "\tstudent, professor: -0.21034862101078033\n",
      "\tsituation, conclusion: -0.20367661118507385\n",
      "\tking, queen: -0.19686846435070038\n",
      "\tprecedent, collection: -0.16645506024360657\n",
      "\tatmosphere, landscape: -0.16415929794311523\n",
      "\tphysics, chemistry: -0.16200031340122223\n",
      "\tproblem, airport: -0.16017237305641174\n",
      "\tasylum, madhouse: -0.15234695374965668\n"
     ]
    }
   ],
   "source": [
    "lens = len(data)\n",
    "sim_dict = {}\n",
    "word_pairs = []\n",
    "sorted_list = []\n",
    "\n",
    "with open(gs_path) as f:\n",
    "    for line in f:\n",
    "        word1, word2, score = line.split()\n",
    "        word_pairs.append((word1, word2))\n",
    "\n",
    "for i in range(0, lens):\n",
    "    sim_dict[word_pairs[i]] = model[i].tolist()\n",
    "    \n",
    "sorted_dict = dict(sorted(sim_dict.items(), key=lambda item: item[1]))\n",
    "for k,v in sorted_dict.items():\n",
    "    sorted_list.append((k,v))\n",
    "\n",
    "bottom10 = sorted_list[:9]\n",
    "top10 = sorted_list[-9:]\n",
    "\n",
    "print(f'The top 10 best performing pairs of words are:')\n",
    "for element in top10:\n",
    "    words, score = element\n",
    "    word1, word2 = words\n",
    "    print(f'\\t{word1}, {word2}: {score}')\n",
    "\n",
    "print()\n",
    "print(f'The bottom 10 worst performing pairs of words are:')\n",
    "for element in bottom10:\n",
    "    words, score = element\n",
    "    word1, word2 = words\n",
    "    print(f'\\t{word1}, {word2}: {score}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the best performing pairs, there is quite some semantic similarity in the pairs, although they are not synonymous, except for the tiger-tiger pair. Japanese and American are both nationalities (or nation-related adjectives). Harvard and Yale are both solid American universities. Marathon and sprint are types of races. Mexico and Brazil are both countries. From the pairs that do not have a 1:1 similarity, street and block are both city planning elements, lobster and wine go well together during a fancy dinner, a furnace is like a big stove, and both a plane and a car are vehicles.\n",
    "\n",
    "In the bottom pairs there are some surprising ones, like wood and forest, king and queen, physics and chemisty, asylum and madhouse. The rest seem less related indeed. I would guess that the contexts in which these words appeared in the training data made them distinctive enough not to be classified together. \n",
    "\n",
    "It is worth noting that re-running the model gives slightly different results. The discussion pertains to the first time we ran and evaluated it. This description pertains to the result we got with a 50k sample:  \n",
    "\n",
    "The top 10 best performing pairs of words are:\n",
    "+ plane, car: 0.19662246108055115\n",
    "+ furnace, stove: 0.20392639935016632\n",
    "+ lobster, wine: 0.20648574829101562\n",
    "+ street, block: 0.21518373489379883\n",
    "+ tiger, tiger: 1.0\n",
    "+ Arafat, Jackson: 1.0\n",
    "+ Japanese, American: 1.0\n",
    "+ Harvard, Yale: 1.0\n",
    "+ Mexico, Brazil: 1.0  \n",
    "\n",
    "The bottom 10 worst performing pairs of words are:\n",
    "+ wood, forest: -0.24051402509212494\n",
    "+ student, professor: -0.21034862101078033\n",
    "+ situation, conclusion: -0.20367661118507385\n",
    "+ king, queen: -0.19686846435070038\n",
    "+ precedent, collection: -0.16645506024360657\n",
    "+ atmosphere, landscape: -0.16415929794311523\n",
    "+ physics, chemistry: -0.16200031340122223\n",
    "+ problem, airport: -0.16017237305641174\n",
    "+ asylum, madhouse: -0.15234695374965668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ More attention could be paid to the kind of sources we use for training data (more data, make sure that many semantic fields are covered by it).\n",
    "+ Having the model go through more epochs could also help (but not too many to avoid overfitting!).\n",
    "+ We did not implement dropout here, and that perhaps could also affect the performance.\n",
    "+ We should curate the things in the dataset more, eliminate more symbols, non-English words, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('wood', 'forest'): -0.24051402509212494, ('student', 'professor'): -0.21034862101078033, ('situation', 'conclusion'): -0.20367661118507385, ('king', 'queen'): -0.19686846435070038, ('precedent', 'collection'): -0.16645506024360657, ('atmosphere', 'landscape'): -0.16415929794311523, ('physics', 'chemistry'): -0.16200031340122223, ('problem', 'airport'): -0.16017237305641174, ('asylum', 'madhouse'): -0.15234695374965668, ('planet', 'moon'): -0.14457717537879944, ('professor', 'doctor'): -0.13931499421596527, ('opera', 'performance'): -0.13614094257354736, ('professor', 'cucumber'): -0.12920096516609192, ('sign', 'recess'): -0.12804320454597473, ('line', 'insurance'): -0.12732526659965515, ('start', 'match'): -0.12089414894580841, ('holy', 'sex'): -0.12024291604757309, ('dollar', 'yen'): -0.11730515956878662, ('morality', 'marriage'): -0.11720168590545654, ('cemetery', 'woodland'): -0.10990757495164871, ('bird', 'cock'): -0.10202678292989731, ('opera', 'industry'): -0.10187534987926483, ('smart', 'stupid'): -0.0995870903134346, ('population', 'development'): -0.09914498776197433, ('bread', 'butter'): -0.09777539223432541, ('forest', 'graveyard'): -0.09685616940259933, ('drink', 'car'): -0.09579229354858398, ('skin', 'eye'): -0.09233087301254272, ('tiger', 'jaguar'): -0.09229976683855057, ('precedent', 'example'): -0.09091365337371826, ('governor', 'interview'): -0.0879325270652771, ('chord', 'smile'): -0.08385259658098221, ('drink', 'mother'): -0.07703128457069397, ('listing', 'proximity'): -0.07484136521816254, ('psychology', 'psychiatry'): -0.07255034893751144, ('street', 'place'): -0.07228216528892517, ('cup', 'entity'): -0.06982260197401047, ('consumer', 'energy'): -0.06827075034379959, ('precedent', 'group'): -0.06729032844305038, ('rooster', 'voyage'): -0.06649664789438248, ('board', 'recommendation'): -0.06548548489809036, ('coast', 'shore'): -0.06317584216594696, ('championship', 'tournament'): -0.06294392794370651, ('dividend', 'payment'): -0.06064422056078911, ('food', 'rooster'): -0.057811956852674484, ('vodka', 'gin'): -0.05732100084424019, ('investigation', 'effort'): -0.05720672756433487, ('possibility', 'girl'): -0.05616672337055206, ('precedent', 'information'): -0.052873801440000534, ('mile', 'kilometer'): -0.0521574430167675, ('Mars', 'water'): -0.05060543119907379, ('magician', 'wizard'): -0.04903414472937584, ('peace', 'plan'): -0.04795480892062187, ('word', 'similarity'): -0.04713186249136925, ('travel', 'activity'): -0.04624320939183235, ('sugar', 'approach'): -0.04457658901810646, ('benchmark', 'index'): -0.0442534014582634, ('chance', 'credibility'): -0.043636467307806015, ('crane', 'implement'): -0.04138637334108353, ('volunteer', 'motto'): -0.040448158979415894, ('start', 'year'): -0.040071599185466766, ('money', 'operation'): -0.040071483701467514, ('music', 'project'): -0.03902127593755722, ('drink', 'eat'): -0.03814202919602394, ('psychology', 'discipline'): -0.03652755543589592, ('media', 'radio'): -0.03588629513978958, ('cup', 'object'): -0.03166523203253746, ('month', 'hotel'): -0.03155384212732315, ('man', 'governor'): -0.02980494312942028, ('dollar', 'buck'): -0.02611539699137211, ('king', 'cabbage'): -0.025737877935171127, ('life', 'death'): -0.02033042535185814, ('seven', 'series'): -0.018330467864871025, ('consumer', 'confidence'): -0.01614741049706936, ('cup', 'substance'): -0.015883585438132286, ('car', 'automobile'): -0.015482080169022083, ('theater', 'history'): -0.015059308148920536, ('rock', 'jazz'): -0.014836655929684639, ('food', 'fruit'): -0.01440015621483326, ('glass', 'magician'): -0.014136800542473793, ('tiger', 'organism'): -0.01356278732419014, ('fuck', 'sex'): -0.012614794075489044, ('murder', 'manslaughter'): -0.008948281407356262, ('announcement', 'effort'): -0.008185199461877346, ('architecture', 'century'): -0.007101692724972963, ('five', 'month'): -0.006608415860682726, ('space', 'chemistry'): -0.0061782062985002995, ('gem', 'jewel'): -0.004261353053152561, ('media', 'gain'): -0.0037861058954149485, ('delay', 'racism'): -0.00360392639413476, ('coast', 'forest'): -0.00358952465467155, ('vodka', 'brandy'): -0.0032910879235714674, ('shower', 'thunderstorm'): -0.0027427328750491142, ('drink', 'ear'): -0.0018656058236956596, ('coast', 'hill'): -0.0011707148514688015, ('king', 'rook'): -0.00018193144933320582, ('ministry', 'culture'): 0.00047698686830699444, ('tiger', 'mammal'): 0.001559193478897214, ('noon', 'string'): 0.0030150304082781076, ('direction', 'combination'): 0.0033450222108513117, ('football', 'basketball'): 0.004402122460305691, ('boy', 'lad'): 0.004566987976431847, ('deployment', 'departure'): 0.005243426188826561, ('president', 'medal'): 0.006545887794345617, ('street', 'avenue'): 0.006872451398521662, ('school', 'center'): 0.008065245114266872, ('stock', 'CD'): 0.009380488656461239, ('morality', 'importance'): 0.009826465509831905, ('glass', 'metal'): 0.01048875693231821, ('energy', 'secretary'): 0.016329143196344376, ('stock', 'live'): 0.016451256349682808, ('football', 'soccer'): 0.018119746819138527, ('money', 'cash'): 0.01930123195052147, ('aluminum', 'metal'): 0.024733658879995346, ('prejudice', 'recognition'): 0.025144776329398155, ('doctor', 'nurse'): 0.025215379893779755, ('precedent', 'antecedent'): 0.025721555575728416, ('money', 'dollar'): 0.0259822029620409, ('bird', 'crane'): 0.02677152119576931, ('report', 'gain'): 0.026934713125228882, ('observation', 'architecture'): 0.029466843232512474, ('type', 'kind'): 0.029830103740096092, ('phone', 'equipment'): 0.03079529106616974, ('jaguar', 'cat'): 0.030836157500743866, ('planet', 'star'): 0.03280584141612053, ('cup', 'artifact'): 0.03406820446252823, ('tiger', 'feline'): 0.03713742643594742, ('precedent', 'cognition'): 0.037296362221241, ('seafood', 'lobster'): 0.03963495418429375, ('life', 'term'): 0.04066339507699013, ('psychology', 'science'): 0.04085378721356392, ('viewer', 'serial'): 0.043524764478206635, ('lobster', 'food'): 0.044579870998859406, ('experience', 'music'): 0.04474521055817604, ('money', 'currency'): 0.04488088935613632, ('tiger', 'cat'): 0.04568377137184143, ('delay', 'news'): 0.04626213014125824, ('lad', 'brother'): 0.04899699240922928, ('doctor', 'personnel'): 0.05408191680908203, ('football', 'tennis'): 0.05417042598128319, ('cup', 'tableware'): 0.0586395263671875, ('shore', 'woodland'): 0.05959885194897652, ('practice', 'institution'): 0.06042424589395523, ('tiger', 'fauna'): 0.060750268399715424, ('stock', 'jaguar'): 0.06172412261366844, ('tiger', 'carnivore'): 0.0623115599155426, ('lad', 'wizard'): 0.06294533610343933, ('car', 'flight'): 0.0639701709151268, ('train', 'car'): 0.06644824892282486, ('monk', 'slave'): 0.06997005641460419, ('cup', 'article'): 0.07009231299161911, ('century', 'nation'): 0.07604502886533737, ('minority', 'peace'): 0.0762585923075676, ('cucumber', 'potato'): 0.07967524975538254, ('bishop', 'rabbi'): 0.08348891139030457, ('jaguar', 'car'): 0.08465997874736786, ('reason', 'hypertension'): 0.08540701121091843, ('Wednesday', 'news'): 0.08630026876926422, ('journey', 'voyage'): 0.08653159439563751, ('focus', 'life'): 0.08945915848016739, ('calculation', 'computation'): 0.09119653701782227, ('peace', 'insurance'): 0.09253443032503128, ('development', 'issue'): 0.09362570196390152, ('century', 'year'): 0.09425517171621323, ('announcement', 'news'): 0.09480273723602295, ('man', 'woman'): 0.09493930637836456, ('marathon', 'sprint'): 0.0965762510895729, ('cell', 'phone'): 0.09821733087301254, ('midday', 'noon'): 0.10036570578813553, ('image', 'surface'): 0.1048634722828865, ('museum', 'theater'): 0.10566256940364838, ('situation', 'isolation'): 0.10586408525705338, ('attempt', 'peace'): 0.10691626369953156, ('media', 'trading'): 0.11021121591329575, ('announcement', 'production'): 0.11956572532653809, ('profit', 'loss'): 0.12104376405477524, ('smart', 'student'): 0.12433583289384842, ('hospital', 'infrastructure'): 0.12439549714326859, ('street', 'children'): 0.12673181295394897, ('stock', 'egg'): 0.13005667924880981, ('seafood', 'food'): 0.1301494836807251, ('profit', 'warning'): 0.13177815079689026, ('production', 'hike'): 0.13460715115070343, ('liquid', 'water'): 0.13645786046981812, ('computer', 'news'): 0.14029091596603394, ('tiger', 'animal'): 0.14878487586975098, ('stock', 'phone'): 0.15071356296539307, ('peace', 'atmosphere'): 0.15097969770431519, ('stock', 'life'): 0.15387125313282013, ('planet', 'sun'): 0.16638801991939545, ('cup', 'food'): 0.17658889293670654, ('television', 'radio'): 0.1769132912158966, ('monk', 'oracle'): 0.1821870505809784, ('journal', 'association'): 0.18396642804145813, ('plane', 'car'): 0.19662246108055115, ('furnace', 'stove'): 0.20392639935016632, ('lobster', 'wine'): 0.20648574829101562, ('street', 'block'): 0.21518373489379883, ('tiger', 'tiger'): 1.0, ('Arafat', 'Jackson'): 1.0, ('Japanese', 'American'): 1.0, ('Harvard', 'Yale'): 1.0, ('Mexico', 'Brazil'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy, sad: -0.1403135359287262\n",
      "happy, glad: 0.11902550607919693\n",
      "good, bad: -0.0582507848739624\n",
      "good, great: -0.0650825947523117\n",
      "useful, useless: -0.10068221390247345\n",
      "useful, handy: -0.07618600130081177\n",
      "smart, stupid: -0.0995870903134346\n",
      "smart, brilliant: -0.22326377034187317\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [('happy','sad'),('happy','glad'),('good','bad'),('good','great'),('useful','useless'),('useful','handy'),\n",
    "              ('smart','stupid'),('smart','brilliant')]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    word1, word2 = pair\n",
    "    word1_emb = cbow_model.get_embedding(word1)  \n",
    "    word2_emb = cbow_model.get_embedding(word2) \n",
    "            \n",
    "    cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "\n",
    "    print(f'{word1}, {word2}: {cosine_similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the whole similarities dictionary, we can notice that there are pairs that are rated as very similar, but that would have different polarity when it comes to sentiment analysis (one is positive, one is negative), e.g. smart and stupid (1.0), profit and loss (0.05 - okay, this is not much but it's positive at least). I have also constructed some word pairs where the first (positive word) gets paired with a negative word, and later with a positive word close in meaning.  \n",
    "+ happy vs. sad vs. glad: the scores are mediocre, with the good words pair having a higher score (the only one).\n",
    "+ good vs. bad vs. great: the scores are low, and the pair with both positive words seems even more dissimilar.\n",
    "+ useful vs. useless vs. handy: the scores are low, the pair of negative words has a slightly better score.\n",
    "+ smart vs. stupid vs. brilliant: the scores are low, and the pair of both positive words has a lower score.  \n",
    "\n",
    "It seems like the model is not good at recognizing positive or negative sentiment, so in this way it would be bad.\n",
    "\n",
    "If we were trying to determine the semantic field of the comment or text, our model would be better, as in numerous cases it rates words from the same semantic field pretty high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-cropus.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './wiki-corpus.10000.txt'\n",
    "raw_texts = corpus_reader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anarchist historian George Woodcock reports that \" The annual Congress of the International had not taken place in 1870 owing to the outbreak of the Paris Commune , and in 1871 the General Council called only a special conference in London .',\n",
       " 'A bomb was thrown by an unknown party near the conclusion of the rally , killing an officer .',\n",
       " 'In the ensuing panic , police opened fire on the crowd and each other .',\n",
       " 'Josiah Warren is widely regarded as the first American anarchist , and the four-page weekly paper he edited during 1833 , The Peaceful Revolutionist , was the first anarchist periodical published .',\n",
       " 'Weak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism .',\n",
       " \"The word autism first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler 's terminology autistic psychopaths in a lecture in German about child psychology .\",\n",
       " 'Most land areas are in an albedo range of 0.1 to 0.4 .',\n",
       " 'Now expanded to nine , these include the Poarch Band of Creek Indians , MOWA Band of Choctaw Indians , Star Clan of Muscogee Creeks , Echota Cherokee Tribe of Alabama , Cherokees of Northeast Alabama , Cherokees of Southeast Alabama , Ma-Chis Lower Creek Indian Tribe , Piqua Sept of Ohio Shawnee Tribe , and United Cherokee Ani-Yun-Wiya Nation .',\n",
       " 'Of those who indicated a religious preference , 59 % said they possessed a \" full understanding \" of their faith and needed no further learning .',\n",
       " 'Once he realized that his distraction was endangering his life , he refocused and killed her .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row in our raw_texts contain one sentence.\n",
    "raw_texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom torch Dataset\n",
    "class LMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sentences,\n",
    "                 tokenizer=None,\n",
    "                 lower=True,\n",
    "                 min_freq=0,  # it is not implemented for this Dataset\n",
    "                 unk_label='<unk>',\n",
    "                 pad_label='<pad>',\n",
    "                 start_label='<start>',\n",
    "                 end_label='<end>'):\n",
    "        \n",
    "        \"\"\"\n",
    "        raw_texts: list of texts\n",
    "        \"\"\"\n",
    "        \n",
    "        self.window_size= 4\n",
    "        self.min_freq = min_freq\n",
    "        self.unk_label, self.unk_idx = unk_label, 0\n",
    "        self.pad_label, self.pad_idx = pad_label, 1\n",
    "        self.start_label, self.start_idx = start_label, 2\n",
    "        self.end_label, self.end_idx = end_label, 3\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = lambda x: x.replace('.',' ').replace(',', ' ').replace('\"', ' ').split() #simple punctuation and whitespace\n",
    "        self.lower = lower\n",
    "        \n",
    "        # Load Data\n",
    "        # we first tokenize the sentences and then create a unique words vocab BEFORE adding start and end labels, since otherwise\n",
    "        # we were getting the nasty error that took us so long to figure out how to fix (and what it was in general)\n",
    "        # this way our vocab is all the unique words in the sentences excluding these special tokens.\n",
    "        self.sentences = [self.tokenize(s) for s in sentences]\n",
    "        self.vocab = self.unique_words()\n",
    "        self.sentences = [[self.start_label]+s+[self.end_label] for s in self.sentences]\n",
    "        \n",
    "        # Vocabulary\n",
    "        self.word_to_idx = dict()\n",
    "        self.word_to_idx[self.unk_label] = self.unk_idx\n",
    "        self.word_to_idx[self.pad_label] = self.pad_idx\n",
    "        self.word_to_idx[self.start_label] = self.start_idx\n",
    "        self.word_to_idx[self.end_label] = self.end_idx\n",
    "        self.word_to_idx.update({word:idx+max(self.word_to_idx.values())+1 for idx, word in enumerate(self.vocab)})\n",
    "\n",
    "        self.idx_to_word = {v:k for k,v in self.word_to_idx.items()}\n",
    "        \n",
    "    def unique_words(self):\n",
    "        all_words = []\n",
    "        for s in self.sentences:\n",
    "            all_words += s\n",
    "        return np.unique(all_words)\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        if self.lower:\n",
    "            string = string.lower()\n",
    "        return self.tokenizer(string)\n",
    "\n",
    "    def idx2word(self, idx_or_list):\n",
    "        try:\n",
    "            len(idx_or_list)\n",
    "            return [self.idx_to_word.get(idx, self.unk_label) for idx in idx_or_list]\n",
    "        except:\n",
    "            return self.idx_to_word.get(idx_or_list, self.unk_label)\n",
    "    \n",
    "    def word2idx(self, word_or_list):\n",
    "        \n",
    "        if isinstance(word_or_list, str):\n",
    "            return self.word_to_idx.get(word_or_list, self.unk_idx)\n",
    "        else:\n",
    "            return [self.word_to_idx.get(w, self.unk_idx) for w in word_or_list]\n",
    "        \n",
    "    def get_shuffled_data(self, seed=None): \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        rng = np.arange(len(self))\n",
    "        np.random.shuffle(rng)\n",
    "        return self.sentences[rng]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<start>', 'a', 'bomb', 'was', 'thrown', 'by', 'an', 'unknown', 'party', '<end>'], ['<start>', 'coherence', 'theory', 'hypothesizes', 'that', 'a', 'limited', '<end>'], ['<start>', 'a', 'bomb', 'was', 'thrown', 'by', 'an', 'unknown', 'party', '<end>'], ['<start>', 'coherence', 'theory', 'hypothesizes', 'that', 'a', 'limited', '<end>']]\n",
      "{'<unk>': 0, '<pad>': 1, '<start>': 2, '<end>': 3, 'a': 4, 'an': 5, 'bomb': 6, 'by': 7, 'coherence': 8, 'hypothesizes': 9, 'limited': 10, 'party': 11, 'that': 12, 'theory': 13, 'thrown': 14, 'unknown': 15, 'was': 16}\n"
     ]
    }
   ],
   "source": [
    "test_sentences = ['A bomb was thrown by an unknown party.', 'coherence theory, hypothesizes that a limited.', 'A bomb was thrown by an unknown party.', 'coherence theory, hypothesizes that a limited.']\n",
    "test_dataset = LMDataset(test_sentences)\n",
    "print(test_dataset.sentences)\n",
    "print(test_dataset.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<end>' '<start>' 'a' 'an' 'bomb' 'by' 'coherence' 'hypothesizes'\n",
      " 'limited' 'party' 'that' 'theory' 'thrown' 'unknown' 'was']\n",
      "['a' 'an' 'bomb' 'by' 'coherence' 'hypothesizes' 'limited' 'party' 'that'\n",
      " 'theory' 'thrown' 'unknown' 'was']\n"
     ]
    }
   ],
   "source": [
    "# comparing the vocabulary with and without added tokens\n",
    "print(test_dataset.unique_words())\n",
    "print(test_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using pytorch DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "class Collate():\n",
    "    def __init__(self, word_to_idx, pad_idx, batch_first=True):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "        self.word_to_idx = word_to_idx\n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.tensor([self.word_to_idx[w] for w in s], device=device) for s in batch]\n",
    "        batch = pad_sequence(batch, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "        return batch\n",
    "\n",
    "def get_loader(dataset, batch_size=16, shuffle=True):\n",
    "    return DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=Collate(dataset.word_to_idx, dataset.pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  6, 16, 14,  7,  5, 15, 11,  3],\n",
      "        [ 2,  8, 13,  9, 12,  4, 10,  3,  1,  1]], device='cuda:0')\n",
      "tensor([[ 2,  4,  6, 16, 14,  7,  5, 15, 11,  3],\n",
      "        [ 2,  8, 13,  9, 12,  4, 10,  3,  1,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loader = get_loader(test_dataset, batch_size=2, shuffle=False)\n",
    "for i in test_loader:\n",
    "    print(i)\n",
    "    \n",
    "# pad_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs':3,\n",
    "                      'batch_size':8,\n",
    "                      'learning_rate':0.0001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, dataset, vocab_size, embedding_dim, output_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=output_dim, num_layers=1)\n",
    "        self.predict_word = nn.Linear(output_dim, vocab_size)\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_representation, *_ = self.LSTM(embedded_seq)\n",
    "        predicted_words = self.predict_word(timestep_representation)\n",
    "        \n",
    "        return predicted_words\n",
    "    \n",
    "    # This was useful for evaluatin the previous model so we implemented it here too, even though it turns out not to have\n",
    "    # been useful at all.\n",
    "    def get_embedding(self, word):\n",
    "        idx_word = self.dataset.word2idx(word)\n",
    "        word_embedding = self.embeddings(torch.tensor(idx_word))\n",
    "        \n",
    "        return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 : Average Loss = 9.93662\n",
      "Batch 100 : Average Loss = 8.18665\n",
      "Batch 200 : Average Loss = 7.44131\n",
      "Batch 300 : Average Loss = 7.15346\n",
      "Batch 400 : Average Loss = 6.95966\n",
      "Batch 500 : Average Loss = 6.86735\n",
      "Batch 600 : Average Loss = 6.80722\n",
      "Epoch 0 : Average Loss = 6.79817\n",
      "Batch 0 : Average Loss = 5.95479\n",
      "Batch 100 : Average Loss = 6.46978\n",
      "Batch 200 : Average Loss = 6.4737\n",
      "Batch 300 : Average Loss = 6.46158\n",
      "Batch 400 : Average Loss = 6.45408\n",
      "Batch 500 : Average Loss = 6.4607\n",
      "Batch 600 : Average Loss = 6.4484\n",
      "Epoch 1 : Average Loss = 6.45905\n",
      "Batch 0 : Average Loss = 5.88237\n",
      "Batch 100 : Average Loss = 6.34776\n",
      "Batch 200 : Average Loss = 6.33271\n",
      "Batch 300 : Average Loss = 6.42052\n",
      "Batch 400 : Average Loss = 6.41153\n",
      "Batch 500 : Average Loss = 6.42455\n",
      "Batch 600 : Average Loss = 6.42659\n",
      "Epoch 2 : Average Loss = 6.42386\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "LM_data = LMDataset(raw_texts) \n",
    "vocab = LM_data.word_to_idx\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(LM_data,\n",
    "                       len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    dataset = get_loader(LM_data, batch_size=lm_hyperparameters[\"batch_size\"], shuffle=True)\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        \n",
    "        # we remove the <end> token, as per instructions\n",
    "        input_sentence = sentence[:, 0:-1]\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        try:\n",
    "            output = lm_model(input_sentence)\n",
    "        except:\n",
    "            print(\"Something went wrong with this batch\")\n",
    "            continue\n",
    "        \n",
    "        # we remove the <start> token, as per instructions\n",
    "        gold_data = sentence[:, 1:]\n",
    "        \n",
    "        # the output and sentence variables are reshaped\n",
    "        loss = loss_fn(output.view(lm_hyperparameters['batch_size'], len(vocab), -1), gold_data)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for every 100 batches, and later for the epoch too\n",
    "        print_every=100\n",
    "        if i%print_every==0:\n",
    "            print(f'Batch {i} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')\n",
    "            \n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    \n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = tokenize(good_s)\n",
    "            tok_bad_s = tokenize(bad_s)\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([vocab.word2idx(x) for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([vocab.word2idx(x) for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            # we use torch.no_grad() so we do not alter the model by running it on these sentences\n",
    "            with torch.no_grad():\n",
    "                good_s = model(enc_good_s)\n",
    "                bad_s = model(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(good_s, dim=2)\n",
    "            bs_probs = F.softmax(bad_s, dim=2)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "\n",
    "def tokenize(string):\n",
    "    # we need to add the start token so that we can get the probability for the first word of the sentence, but we do not need \n",
    "    # any end token.\n",
    "    new_string = [\"<start>\"] + string.replace('.',' ').replace(',', ' ').replace('\"', ' ').lower().split()\n",
    "    return new_string\n",
    "\n",
    "def find_token_probs(model_probs, encoded_sentence):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    # we iterate over the sentence WITHOUT the start token so that we get the token number (the row of the matrix) and the NEXT\n",
    "    # word relative to the model_probs (i.e. the 0th row in model_probs includes the probabilities of the words that follow\n",
    "    # <start>, so within that we want to look up the entry for the next word in the sentence, and not for <start>. This is perhaps\n",
    "    # hard to explain but it is analogous to what we did when training the model with the start and end tokens and offsetting the\n",
    "    # predictions and the gold standard).\n",
    "    for token_nr, gold_token in enumerate(encoded_sentence[0, 1:]):\n",
    "\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = model_probs[0, token_nr, gold_token]\n",
    "        probs.append(float(prob))\n",
    "        \n",
    "    sentence_prob = np.cumprod(probs)[-1]\n",
    "    \n",
    "    return sentence_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:\n",
      "0.662\n"
     ]
    }
   ],
   "source": [
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, LM_data, lm_model)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agreed that the stupidest model we can get is one that gets 50% accuracy, as we have a 50% chance of choosing the \"right\" answer just by choosing randomly. Anything above that will be an improvement relative to that, so any model that has more than 50% accuracy will be \"good\".  \n",
    "\n",
    "For a really good model we thought we could compare its performance against some state-of-the-art models (contemporary and past ones). However that performance will be hard to match, and we are not sure where to draw a realistic benchmark for a decent model, that is not just barely better than random guessing.\n",
    "\n",
    "When we ran our model on 5000 sentences, we had a 78.8% accuracy, which is way more than we expected. We have tried to re-run the model with the 50000 sentences dataset, but that was causing the notebook to freeze when done locally on a CPU, and caused GPU memory issues on the server. We tested some other sets (10k, 15k) and depending on the size of the embeddings they sometimes also caused those issues. Running them with smaller embeddings or fewer epochs yielded really unsatisfactory results, which is why we are submitting this one where we re-ran the model on 5k and got 66.2% accuracy. It seems like a significant part of how good the model is here is up to us being lucky with the random embeddings at the start. Please do not hesistate to let us know if you want us to try to re-run it with any other data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to pay more attention to or implement the following:  \n",
    "+ dropout - as mentioned for the other model, we did not implement that, which means that our models may be prone to overfitting, learning its traning data rather than learning something from it. This sounds like a very reasonable thing to use; we did not, however, use it, since we prioritized getting the models to work and it was not a required element of the assignment. It may be worth seeing what difference it makes for a VG project, perhaps.\n",
    "+ rerunning with different hyperparameters - as we have heard in the lectures, models tend to be tested for what the best hyperparameters for them are: perhaps our model will learn better with a different batch size, embedding size, or learning rate; perhaps we should have it go through a few more epochs? It is a lot of variables and they all can improve (or hurt) the performance of the model.\n",
    "+ min_freq - we did not, in the end, use a minimum frequency (although we acknowledge that Adam did fix our version of it in the CBOW model). Eliminating very rare words could perhaps help our model.\n",
    "+ curated data - once again, we are not really sure what our model is learning from. As per one of the readings for the following week, it is important to be aware of that! Perhaps better training data would yield us a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are unsure if we understood properly what is meant by this question.\n",
    "For just the type of evaluation that we just implemented:\n",
    "+ We could normalize the probability over the length of the sentence and establish some baseline for \"grammatical\" vs. \"ungrammatical\" sentences and then calculate precision, recall, F1 score.\n",
    "\n",
    "If it is about evaluating the whole system, not just this evaluation:\n",
    "+ We could use an evaluation set (split the 50k sentences into training and testing data).\n",
    "+ We could use other pairs of sentences from the same github repository and test how the model performs with other types of \"incorrectness\".\n",
    "+ We could take sentences and mask words in them, and then compare the model predictions to the standard (inspired by MLM).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):11371155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 31113119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
