{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks - VG\n",
    "Based on the Python Notebook by Adam Ek, expanded upon by Maria Irena Szawerna for the VT2022 Computational Semantics course.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description, references\n",
    "In this notebook I will, on the basis of Lab 5, explore how SNLI-trained models perform on FRACAS and whether or not fine-tuning them will improve the performance. A more detailed summary can be found at the end of the notebook. Throughout the notebook I will remark on what was sourced from the original Lab 5 notebook or somewhere else; I did contribute to all the parts of the basic Lab 5 though, so it is not strictly only my classmates' work. The references for the lab can be found at the end of the notebook, and I did not use anything besides that except for the FRACAS dataset itself, which is linked later in this cell, as well as a guide on how to access XML files, linked in the appropriate code block.  \n",
    "\n",
    "##### Research question and thesis statement  \n",
    "+ Does an SNLI-trained BiLSTM model perform well when evaluated on the FRACAS dataset? Does fine-tuning it on FRACAS influence the performance?\n",
    "+ My prediction is that fine-tuning a model trained on SNLI on FRACAS will improve its performance on FRACAS-related tasks. However, the FRACAS dataset is much smaller, so the change may not be drastically big.\n",
    "\n",
    "##### Description of the datasets' structure and my changes to the FRACAS dataset\n",
    "*The dataset could not be downloaded in the simplified version, the link did not work; instead, I used [this](https://github.com/sdobnik/computational-semantics/blob/master/assignments/05-natural-language-inference/simple_snli_1.0.zip)*\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "I will also work with the FRACAS dataset from [here](https://nlp.stanford.edu/~wcmac/downloads/fracas.xml). This one accepts the following relations:\n",
    "* yes - equivalent to entailment\n",
    "* no - equivalent to contradiction\n",
    "* unknown - equivalent to neutral (neither entailment nor contradiction can be concluded)\n",
    "* undef - the example is too tricky for an expert to resolve  \n",
    "\n",
    "I will have to transform this dataset into a format that will be easy to work with, replacing their names of judgements with mine. In addition, some of their examples have two or more premises for one hypothesis. According to the dataset decription, one-premise problems constitute around 55% of the dataset, that is 192 problems. While this does slim the dataset down, I cannot do it any other way without combining the multiple premises into one sentence, as the model I am working with only compares two sentences (the hypothesis and the premise), and so the multi-premise problems will have to be excluded. In the end fewer than 192 problems are used, as some of those do not have an answer (undef), and so I excluded them.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as in Lab 5\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI data and datasets/loaders  \n",
    "----\n",
    "**THIS WHOLE SECTION IS THE SAME AS IN LAB 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('simple_snli_1.0_train.csv', header=None, sep='\\t')\n",
    "train_data.columns = ['premise', 'hypothesis', 'relation']\n",
    "test_data = pd.read_csv('simple_snli_1.0_test.csv', header=None, sep='\\t')\n",
    "test_data.columns = ['premise', 'hypothesis', 'relation']\n",
    "dev_data = pd.read_csv('simple_snli_1.0_dev.csv', header=None, sep='\\t')\n",
    "dev_data.columns = ['premise', 'hypothesis', 'relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implement a Dataset to keep track of vocab, word2idx, idx2word\n",
    "# Dataset can also be used in DataLoader which gives batch loading, etc, for free.\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, unk_label='<unk>', pad_label='<pad>'):\n",
    "        \n",
    "        self.unk_idx, self.unk_label = 0, unk_label\n",
    "        self.pad_idx, self.pad_label = 1, pad_label\n",
    "\n",
    "        self.data = data.copy()\n",
    "        self.data['premise'] = self.data['premise'].apply(self.tokenize)\n",
    "        self.data['hypothesis'] = self.data['hypothesis'].apply(self.tokenize)\n",
    "\n",
    "\n",
    "        self.vocab = self.__unique_words()\n",
    "        \n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word2idx[self.unk_label] = self.unk_idx\n",
    "        self.word2idx[self.pad_label] = self.pad_idx\n",
    "        self.word2idx.update({word:idx+max(self.word2idx.values())+1 for idx, word in enumerate(self.vocab)})\n",
    "\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "\n",
    "        self.labels = list(np.unique(self.data['relation']))\n",
    "\n",
    "    def __unique_words(self):\n",
    "        all_words = []\n",
    "        for s in self.data['premise']:\n",
    "            all_words += s\n",
    "        for s in self.data['hypothesis']:\n",
    "            all_words += s\n",
    "        return np.unique(all_words)\n",
    "        \n",
    "    def tokenize(self, string):\n",
    "        if isinstance(string, str): \n",
    "            # The tokenizer was given as a whitespace tokenizer\n",
    "            return string.lower().split()\n",
    "        else:  # for NaN\n",
    "            return \"<unk>\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #x = self.data.iloc[0] #for test\n",
    "        x = self.data.iloc[idx]\n",
    "        out = (x['premise'], x['hypothesis'], x['relation'])\n",
    "        return out\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "relation_to_idx = {k:v for v,k in enumerate(sorted(np.unique(train_data['relation'])))}\n",
    "idx_relation = {v:k for k,v in relation_to_idx.items()}\n",
    "\n",
    "class Collate():\n",
    "    def __init__(self, word_to_idx, pad_idx=1, unk_idx=0, relation_to_idx=relation_to_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.unk_idx = unk_idx\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.relation_to_idx = relation_to_idx\n",
    "    def __call__(self, batch):\n",
    "        batch = np.transpose(batch)\n",
    "        \n",
    "        premises = np.transpose(batch[0])\n",
    "        premises = [torch.tensor([self.word_to_idx.get(w, self.unk_idx) for w in s], device=device) for s in premises]\n",
    "        premises = pad_sequence(premises, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        hypothesis = np.transpose(batch[1]) #batch first\n",
    "        hypothesis = [torch.tensor([self.word_to_idx.get(w, self.unk_idx) for w in s], device=device) for s in hypothesis]\n",
    "        hypothesis = pad_sequence(hypothesis, batch_first=True, padding_value=self.pad_idx)\n",
    "        \n",
    "        relations = [self.relation_to_idx[rel] for rel in batch[2]]\n",
    "\n",
    "        return premises, hypothesis, relations\n",
    "\n",
    "\n",
    "def dataloader(dataset, word2idx, pad_idx, unk_idx, batch_size=32, shuffle=True): # Need word2idx etc to match between train and test. Id probably do this is another wya in hindsight.\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=Collate(word2idx, pad_idx, unk_idx) )\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRACAS data\n",
    "----\n",
    "**THIS WHOLE SECTION IS ENTIRELY NEW, WITH THE parseXML() FUNCTION ADAPTED FROM UNDER THE LINK IN THE COMMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(xmlfile):\n",
    "    # adapted from https://www.geeksforgeeks.org/xml-parsing-python/\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    problems = []\n",
    "    counter = 0\n",
    "  \n",
    "    for item in root.findall('./problem'):\n",
    "        counter += 1\n",
    "        \n",
    "        items = {}\n",
    "\n",
    "        for child in item:\n",
    "            if child.tag not in items:\n",
    "                items[child.tag] = child.text.encode('utf8')\n",
    "            else:  # if it is in items, meaning it's a problem with multiple premises\n",
    "                items['multi'] = 'yes'\n",
    "            \n",
    "            # I estimated that from the category 5. Adjectives and onwards, those are semantic examples\n",
    "            if counter <= 196:\n",
    "                items['category'] = 'syntactic'\n",
    "            else:\n",
    "                items['category'] = 'semantic'\n",
    "        \n",
    "        if 'multi' not in items:  # if it is a 1-premise problem\n",
    "            problems.append(items)\n",
    "        else:\n",
    "            continue\n",
    "      \n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_problem(problem):\n",
    "    problem['p'] = problem['p'].decode(\"utf-8\").strip()\n",
    "    problem['h'] = problem['h'].decode(\"utf-8\").strip()\n",
    "    del problem['q']\n",
    "\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_1_premise_problems(xmlfile):\n",
    "    \n",
    "    problems = parseXML(xmlfile)\n",
    "    \n",
    "    # removing unnecessary \"features\"\n",
    "    for problem in problems:\n",
    "        if 'note' in problem:\n",
    "            del problem['note']\n",
    "        if 'why' in problem:\n",
    "            del problem['why']\n",
    "\n",
    "    # picking the ones that are not undef, changing the relation name\n",
    "    selected_problems = []\n",
    "    for problem in problems:\n",
    "        if problem['a'] == b' Yes ':\n",
    "            problem['a'] = 'entailment'\n",
    "            problem = decode_problem(problem)\n",
    "            selected_problems.append(problem)\n",
    "        elif problem['a'] == b' No ':\n",
    "            problem['a'] = 'contradiction'\n",
    "            problem = decode_problem(problem)\n",
    "            selected_problems.append(problem)\n",
    "        elif problem['a'] == b\" Don't know \":\n",
    "            problem['a'] = 'neutral'\n",
    "            problem = decode_problem(problem)\n",
    "            selected_problems.append(problem)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return selected_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = select_1_premise_problems('Natlog Problems.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fracas_frame = pd.DataFrame(problems)\n",
    "# I need to rename the columns so that it works with our datasets and loaders from the section above.\n",
    "full_fracas_frame.columns = ['premise', 'category', 'hypothesis', 'relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>category</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Italian became the world's greatest tenor.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There was an Italian who became the world's gr...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The really ambitious tenors are Italian.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are really ambitious tenors who are Ital...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No really great tenors are modest.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are really great tenors who are modest.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some great tenors are Swedish.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are great tenors who are Swedish.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Many great tenors are German.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are great tenors who are German.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>It is true that ITEL won the contract in 1992.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL won the contract in 1992.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>It is false that ITEL won the contract in 1992.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL won the contract in 1992.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Smith saw Jones sign the contract.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Jones signed the contract.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Smith saw Jones sign the contract and his secr...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith saw Jones sign the contract.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Smith saw Jones sign the contract or cross out...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith either saw Jones sign the contract or sa...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               premise   category  \\\n",
       "0        An Italian became the world's greatest tenor.  syntactic   \n",
       "1             The really ambitious tenors are Italian.  syntactic   \n",
       "2                   No really great tenors are modest.  syntactic   \n",
       "3                       Some great tenors are Swedish.  syntactic   \n",
       "4                        Many great tenors are German.  syntactic   \n",
       "..                                                 ...        ...   \n",
       "160     It is true that ITEL won the contract in 1992.   semantic   \n",
       "161    It is false that ITEL won the contract in 1992.   semantic   \n",
       "162                 Smith saw Jones sign the contract.   semantic   \n",
       "163  Smith saw Jones sign the contract and his secr...   semantic   \n",
       "164  Smith saw Jones sign the contract or cross out...   semantic   \n",
       "\n",
       "                                            hypothesis       relation  \n",
       "0    There was an Italian who became the world's gr...     entailment  \n",
       "1    There are really ambitious tenors who are Ital...     entailment  \n",
       "2        There are really great tenors who are modest.  contradiction  \n",
       "3              There are great tenors who are Swedish.     entailment  \n",
       "4               There are great tenors who are German.     entailment  \n",
       "..                                                 ...            ...  \n",
       "160                     ITEL won the contract in 1992.     entailment  \n",
       "161                     ITEL won the contract in 1992.  contradiction  \n",
       "162                         Jones signed the contract.     entailment  \n",
       "163                 Smith saw Jones sign the contract.     entailment  \n",
       "164  Smith either saw Jones sign the contract or sa...     entailment  \n",
       "\n",
       "[165 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_fracas_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Usually data is divided into train, test, dev in a 8:1:1 ratio. Since I will not have a train set here (I train on the SNLI\n",
    "data and only finetune on FRACAS dev), that leaves us with test and dev at 1:1.\n",
    "\n",
    "since I want it shuffled, but I want to keep the results reproducible for re-running the notebook, I will keep a fixed seed. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'p': 'A few committee members are from Scandinavia.', 'category': 'syntactic', 'h': 'At least a few female committee members are from Scandinavia.', 'a': 'neutral'}, {'p': 'Smith wrote a report for two hours.', 'category': 'semantic', 'h': 'Smith wrote a report.', 'a': 'neutral'}, {'p': 'Smith wrote a novel in 1991.', 'category': 'semantic', 'h': 'Smith wrote it in 1992.', 'a': 'contradiction'}, {'p': 'Smith and Jones left the meeting.', 'category': 'semantic', 'h': 'Smith left the meeting.', 'a': 'entailment'}, {'p': 'No delegate finished the report on time.', 'category': 'syntactic', 'h': 'Some Scandinavian delegate finished the report on time.', 'a': 'contradiction'}, {'p': 'Some Irish delegates finished the survey on time.', 'category': 'syntactic', 'h': 'Some delegates finished the survey on time.', 'a': 'entailment'}, {'p': 'Several delegates got the results published in major national newspapers.', 'category': 'syntactic', 'h': 'Several delegates got the results published.', 'a': 'entailment'}, {'p': \"Bill suggested to Frank's boss that they should go to the meeting together, and Carl to Alan's wife.\", 'category': 'syntactic', 'h': \"If it was suggested that Bill and Frank's boss should go together, it was suggested that Carl and Alan should go together.\", 'a': 'neutral'}, {'p': 'Mary used her workstation.', 'category': 'syntactic', 'h': 'Mary is female.', 'a': 'entailment'}, {'p': 'Smith ran his own business for two years.', 'category': 'semantic', 'h': 'Smith ran his own business.', 'a': 'entailment'}, {'p': 'John has a genuine diamond.', 'category': 'semantic', 'h': 'John has a diamond.', 'a': 'entailment'}, {'p': \"An Italian became the world's greatest tenor.\", 'category': 'syntactic', 'h': \"There was an Italian who became the world's greatest tenor.\", 'a': 'entailment'}, {'p': 'Smith wrote a report in two hours.', 'category': 'semantic', 'h': 'Smith spent more than two hours writing the report.', 'a': 'contradiction'}, {'p': 'ITEL won more orders than APCOM lost.', 'category': 'semantic', 'h': 'APCOM lost some orders.', 'a': 'neutral'}, {'p': 'John said Bill had hurt himself.', 'category': 'syntactic', 'h': 'Someone said John had been hurt.', 'a': 'neutral'}, {'p': 'Several great tenors are British.', 'category': 'syntactic', 'h': 'There are great tenors who are British.', 'a': 'entailment'}, {'p': 'The PC-6082 is as fast as the ITEL-XZ.', 'category': 'semantic', 'h': 'The PC-6082 is fast.', 'a': 'neutral'}, {'p': 'Smith discovered new species for two years.', 'category': 'semantic', 'h': 'Smith discovered new species.', 'a': 'entailment'}, {'p': 'Most Europeans who are resident in Europe can travel freely within Europe.', 'category': 'syntactic', 'h': 'Most Europeans can travel freely within Europe.', 'a': 'neutral'}, {'p': 'John is a former successful university student.', 'category': 'semantic', 'h': 'John is a university student.', 'a': 'neutral'}, {'p': \"Bill suggested to Frank's boss that they should go to the meeting together, and Carl to Alan's wife.\", 'category': 'syntactic', 'h': \"If it was suggested that Bill and Frank's boss should go together, it was suggested that Carl and Alan's wife should go together.\", 'a': 'entailment'}, {'p': 'At least three commissioners spend time at home.', 'category': 'syntactic', 'h': 'At least three male commissioners spend time at home.', 'a': 'neutral'}, {'p': 'John went to Paris by car, and Bill by train to Berlin.', 'category': 'syntactic', 'h': 'Bill went to Berlin by train.', 'a': 'entailment'}, {'p': 'Many great tenors are German.', 'category': 'syntactic', 'h': 'There are great tenors who are German.', 'a': 'entailment'}, {'p': 'ITEL was winning the contract from APCOM in 1993.', 'category': 'semantic', 'h': 'ITEL won a contract in 1993.', 'a': 'neutral'}, {'p': 'Just one accountant attended the meeting.', 'category': 'syntactic', 'h': 'Some accountants attended the meeting.', 'a': 'entailment'}, {'p': 'Some delegates finished the survey.', 'category': 'syntactic', 'h': 'Some delegates finished the survey on time.', 'a': 'neutral'}, {'p': 'John wrote a report, and Bill said Peter did too.', 'category': 'syntactic', 'h': 'Bill said Peter wrote a report.', 'a': 'entailment'}, {'p': 'Both commissioners used to be leading businessmen.', 'category': 'syntactic', 'h': 'Both commissioners used to be businessmen.', 'a': 'entailment'}, {'p': 'Smith lived in Birmingham for two years.', 'category': 'semantic', 'h': 'Smith lived in Birmingham for exactly a year.', 'a': 'contradiction'}, {'p': 'At most ten female commissioners spend time at home.', 'category': 'syntactic', 'h': 'At most ten commissioners spend time at home.', 'a': 'neutral'}, {'p': 'ITEL managed to win the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'entailment'}, {'p': 'All the people who were at the meeting voted for a new chairman.', 'category': 'syntactic', 'h': 'Everyone at the meeting voted for a new chairman.', 'a': 'entailment'}, {'p': \"Bill suggested to Frank's boss that they should go to the meeting together, and Carl to Alan's wife.\", 'category': 'syntactic', 'h': 'If it was suggested that Bill and Frank should go together, it was suggested that Carl and Alan should go together.', 'a': 'entailment'}, {'p': 'In two years Smith owned a chain of businesses.', 'category': 'semantic', 'h': 'Smith owned a chain of business for more than two years.', 'a': 'neutral'}, {'p': 'Smith discovered a new species in two hours.', 'category': 'semantic', 'h': 'Smith spent two hours discovering the new species.', 'a': 'contradiction'}, {'p': 'At least three female commissioners spend time at home.', 'category': 'syntactic', 'h': 'At least three commissioners spend time at home.', 'a': 'entailment'}, {'p': 'Smith saw Jones sign the contract and his secretary make a copy.', 'category': 'semantic', 'h': 'Smith saw Jones sign the contract.', 'a': 'entailment'}, {'p': 'Smith saw Jones sign the contract or cross out the crucial clause.', 'category': 'semantic', 'h': 'Smith either saw Jones sign the contract or saw Jones cross out the crucial clause.', 'a': 'entailment'}, {'p': 'Many delegates obtained interesting results from the survey.', 'category': 'syntactic', 'h': 'Many British delegates obtained interesting results from the survey.', 'a': 'neutral'}, {'p': 'At most ten commissioners spend time at home.', 'category': 'syntactic', 'h': 'At most ten female commissioners spend time at home.', 'a': 'entailment'}, {'p': 'Just one accountant attended the meeting.', 'category': 'syntactic', 'h': 'No accountants attended the meeting.', 'a': 'contradiction'}, {'p': 'Fido is not a large animal.', 'category': 'semantic', 'h': 'Fido is a small animal.', 'a': 'neutral'}, {'p': 'Smith and Jones left the meeting.', 'category': 'semantic', 'h': 'Jones left the meeting.', 'a': 'entailment'}, {'p': 'Exactly two lawyers and three accountants signed the contract.', 'category': 'syntactic', 'h': 'Six accountants signed the contract.', 'a': 'contradiction'}, {'p': \"Bill suggested to Frank's boss that they should go to the meeting together, and Carl to Alan's wife.\", 'category': 'syntactic', 'h': \"If it was suggested that Bill, Frank and Frank's boss should go together, it was suggested that Carl, Alan and Alan's wife should go together.\", 'a': 'entailment'}, {'p': 'The PC-6082 is faster than the ITEL-XZ.', 'category': 'semantic', 'h': 'The PC-6082 is fast.', 'a': 'neutral'}, {'p': 'ITEL was building MTALK in 1993.', 'category': 'semantic', 'h': 'ITEL finished MTALK in 1993.', 'a': 'neutral'}, {'p': 'Neither commissioner spends a lot of time at home.', 'category': 'syntactic', 'h': 'Neither commissioner spends time at home.', 'a': 'neutral'}, {'p': 'Most Europeans can travel freely within Europe.', 'category': 'syntactic', 'h': 'Most Europeans who are resident outside Europe can travel freely within Europe.', 'a': 'neutral'}, {'p': 'Smith was running his own business in two years.', 'category': 'semantic', 'h': 'Smith spent more than two years running his own business.', 'a': 'neutral'}, {'p': 'Smith, Jones and several lawyers signed the contract.', 'category': 'syntactic', 'h': 'Jones signed the contract.', 'a': 'entailment'}, {'p': 'The really ambitious tenors are Italian.', 'category': 'syntactic', 'h': 'There are really ambitious tenors who are Italian.', 'a': 'entailment'}, {'p': 'The chairman read out the items on the agenda.', 'category': 'syntactic', 'h': 'The chairman read out every item on the agenda.', 'a': 'entailment'}, {'p': 'Many delegates obtained results from the survey.', 'category': 'syntactic', 'h': 'Many delegates obtained interesting results from the survey.', 'a': 'neutral'}, {'p': 'The PC-6082 is faster than the ITEL-ZX and the ITEL-ZY.', 'category': 'semantic', 'h': 'The PC-6082 is faster than the ITEL-ZX.', 'a': 'entailment'}, {'p': 'John is a former successful university student.', 'category': 'semantic', 'h': 'John is successful.', 'a': 'neutral'}, {'p': 'A company director awarded himself a large payrise.', 'category': 'syntactic', 'h': 'A company director has awarded and been awarded a payrise.', 'a': 'entailment'}, {'p': 'Smith left the meeting before he lost his temper.', 'category': 'semantic', 'h': 'Smith lost his temper.', 'a': 'neutral'}, {'p': 'ITEL won more orders than APCOM.', 'category': 'semantic', 'h': 'APCOM won some orders.', 'a': 'neutral'}, {'p': 'Smith discovered a new species in 1991.', 'category': 'semantic', 'h': 'Smith discovered a new species in 1992.', 'a': 'neutral'}, {'p': 'Smith discovered a new species in two hours.', 'category': 'semantic', 'h': 'Smith discovered a new species.', 'a': 'entailment'}, {'p': 'Smith wrote a report in two hours.', 'category': 'semantic', 'h': 'Smith wrote a report in one hour.', 'a': 'neutral'}, {'p': 'Kim is a clever politician.', 'category': 'semantic', 'h': 'Kim is clever.', 'a': 'neutral'}, {'p': 'John said that Mary wrote a report, and that Bill did too.', 'category': 'syntactic', 'h': 'Bill said Mary wrote a report.', 'a': 'neutral'}, {'p': 'Dumbo is a large animal.', 'category': 'semantic', 'h': 'Dumbo is a small animal.', 'a': 'contradiction'}, {'p': \"Clients at the demonstration were impressed by the system's performance.\", 'category': 'syntactic', 'h': \"Most clients at the demonstration were impressed by the system's performance.\", 'a': 'entailment'}, {'p': 'John is a cleverer politician than Bill.', 'category': 'semantic', 'h': 'John is cleverer than Bill.', 'a': 'neutral'}, {'p': 'John wants to know how many men work part time, and which.', 'category': 'syntactic', 'h': 'John wants to know which men work part time.', 'a': 'entailment'}, {'p': 'ITEL won more orders than APCOM did.', 'category': 'semantic', 'h': 'APCOM won some orders.', 'a': 'neutral'}, {'p': \"Last week I already knew that when, in a month's time, Smith would discover that she had been duped she would be furious.\", 'category': 'semantic', 'h': 'It will be the case that in a few weeks Smith will discover that she has been duped; and she will be furious.', 'a': 'entailment'}, {'p': 'Several delegates got the results published in major national newspapers.', 'category': 'syntactic', 'h': 'Several Portuguese delegates got the results published in major national newspapers.', 'a': 'neutral'}, {'p': 'Mickey is a small animal.', 'category': 'semantic', 'h': 'Mickey is a large animal.', 'a': 'contradiction'}, {'p': 'Smith lived in Birmingham in 1991.', 'category': 'semantic', 'h': 'Smith lived in Birmingham in 1992.', 'a': 'neutral'}, {'p': 'Mary used her workstation.', 'category': 'syntactic', 'h': 'Mary has a workstation.', 'a': 'entailment'}, {'p': 'Smith wrote a report in two hours.', 'category': 'semantic', 'h': 'Smith wrote a report.', 'a': 'entailment'}, {'p': 'Smith ran his own business for two years.', 'category': 'semantic', 'h': 'Smith ran his own business for a year.', 'a': 'entailment'}, {'p': 'In March 1993 APCOM founded ITEL.', 'category': 'semantic', 'h': 'ITEL existed in 1992.', 'a': 'contradiction'}, {'p': 'When Jones got his job at the CIA, he knew that he would never be allowed to write his memoirs.', 'category': 'semantic', 'h': 'It is the case that Jones is not and will never be allowed to write his memoirs.', 'a': 'entailment'}, {'p': 'Either Smith, Jones or Anderson signed the contract.', 'category': 'syntactic', 'h': 'If Smith and Anderson did not sign the contract, Jones signed the contract.', 'a': 'entailment'}, {'p': 'In 1994 ITEL sent a progress report every month.', 'category': 'semantic', 'h': 'ITEL sent a progress report in July 1994.', 'a': 'entailment'}, {'p': 'Fido is not a small animal.', 'category': 'semantic', 'h': 'Fido is a large animal.', 'a': 'neutral'}, {'p': 'Smith was running his own business in two years.', 'category': 'semantic', 'h': 'Smith spent two years running his own business.', 'a': 'neutral'}, {'p': 'Smith claimed he had costed his proposal and so did Jones.', 'category': 'syntactic', 'h': \"Jones claimed Smith had costed Jones' proposal.\", 'a': 'neutral'}, {'p': 'Neither commissioner spends time at home.', 'category': 'syntactic', 'h': 'One of the commissioners spends a lot of time at home.', 'a': 'contradiction'}, {'p': 'Several Portuguese delegates got the results published in major national newspapers.', 'category': 'syntactic', 'h': 'Several delegates got the results published in major national newspapers.', 'a': 'entailment'}, {'p': 'Many delegates obtained interesting results from the survey.', 'category': 'syntactic', 'h': 'Many delegates obtained results from the survey.', 'a': 'entailment'}, {'p': 'At least three commissioners spend a lot of time at home.', 'category': 'syntactic', 'h': 'At least three commissioners spend time at home.', 'a': 'entailment'}, {'p': 'At most ten commissioners spend a lot of time at home.', 'category': 'syntactic', 'h': 'At most ten commissioners spend time at home.', 'a': 'neutral'}, {'p': 'Smith believed that ITEL had won the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'neutral'}, {'p': 'Smith, Jones and Anderson signed the contract.', 'category': 'syntactic', 'h': 'Jones signed the contract.', 'a': 'entailment'}, {'p': 'John went to Paris by car, and Bill to Berlin.', 'category': 'syntactic', 'h': 'Bill went to Berlin by car.', 'a': 'entailment'}, {'p': 'John needed to buy a car, and Bill did.', 'category': 'syntactic', 'h': 'Bill bought a car.', 'a': 'neutral'}, {'p': 'Smith wrote a novel in 1991.', 'category': 'semantic', 'h': 'Smith wrote a novel in 1992.', 'a': 'neutral'}, {'p': 'It is true that ITEL won the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'entailment'}, {'p': 'ITEL won more orders than the APCOM contract.', 'category': 'semantic', 'h': 'ITEL won more than one order.', 'a': 'entailment'}, {'p': 'No really great tenors are modest.', 'category': 'syntactic', 'h': 'There are really great tenors who are modest.', 'a': 'contradiction'}, {'p': 'Software faults were blamed for the system failure.', 'category': 'syntactic', 'h': 'The system failure was blamed on one or more software faults.', 'a': 'entailment'}, {'p': 'ITEL owned APCOM from 1988 to 1992.', 'category': 'semantic', 'h': 'ITEL owned APCOM in 1990.', 'a': 'entailment'}, {'p': 'Smith lived in Birmingham for two years.', 'category': 'semantic', 'h': 'Smith lived in Birmingham for a year.', 'a': 'entailment'}, {'p': 'The people who were at the meeting voted for a new chairman.', 'category': 'syntactic', 'h': 'Everyone at the meeting voted for a new chairman.', 'a': 'neutral'}, {'p': 'Most great tenors are Italian.', 'category': 'syntactic', 'h': 'There are great tenors who are Italian.', 'a': 'entailment'}, {'p': 'Several delegates got the results published.', 'category': 'syntactic', 'h': 'Several delegates got the results published in major national newspapers.', 'a': 'neutral'}, {'p': 'Some delegates finished the survey on time.', 'category': 'syntactic', 'h': 'Some Irish delegates finished the survey on time.', 'a': 'neutral'}, {'p': 'ITEL has a factory in Birmingham.', 'category': 'semantic', 'h': 'ITEL currently has a factory in Birmingham.', 'a': 'entailment'}, {'p': 'Smith lived in Birmingham for two years.', 'category': 'semantic', 'h': 'Smith lived in Birmingham.', 'a': 'entailment'}, {'p': 'Smith wrote a report in two hours.', 'category': 'semantic', 'h': 'Smith spent two hours writing the report.', 'a': 'neutral'}, {'p': 'At least three tenors will take part in the concert.', 'category': 'syntactic', 'h': 'There are tenors who will take part in the concert.', 'a': 'entailment'}, {'p': 'The Ancient Greeks were noted philosophers.', 'category': 'syntactic', 'h': 'Every Ancient Greek was a noted philosopher.', 'a': 'neutral'}, {'p': 'Just one accountant attended the meeting.', 'category': 'syntactic', 'h': 'No accountant attended the meeting.', 'a': 'contradiction'}, {'p': \"Bill suggested to Frank's boss that they should go to the meeting together, and Carl to Alan's wife.\", 'category': 'syntactic', 'h': \"If it was suggested that Bill and Frank should go together, it was suggested that Carl and Alan's wife should go together.\", 'a': 'neutral'}, {'p': 'Just one accountant attended the meeting.', 'category': 'syntactic', 'h': 'Some accountant attended the meeting.', 'a': 'entailment'}, {'p': 'Smith knew that ITEL had won the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'entailment'}, {'p': 'Just one accountant attended the meeting.', 'category': 'syntactic', 'h': 'Some accountant attended the meeting.', 'a': 'entailment'}, {'p': 'The PC-6082 is as fast as the ITEL-XZ.', 'category': 'semantic', 'h': 'The PC-6082 is faster than the ITEL-XZ.', 'a': 'neutral'}, {'p': 'The inhabitants of Cambridge voted for a Labour MP.', 'category': 'syntactic', 'h': 'Every inhabitant of Cambridge voted for a Labour MP.', 'a': 'neutral'}, {'p': 'The Ancient Greeks were all noted philosophers.', 'category': 'syntactic', 'h': 'Every Ancient Greek was a noted philosopher.', 'a': 'entailment'}, {'p': 'John went to Paris by car, and Bill by train.', 'category': 'syntactic', 'h': 'Bill went to Paris by train.', 'a': 'entailment'}, {'p': 'ITEL won more orders than APCOM lost.', 'category': 'semantic', 'h': 'ITEL won some orders.', 'a': 'entailment'}, {'p': 'Smith saw Jones sign the contract.', 'category': 'semantic', 'h': 'Jones signed the contract.', 'a': 'entailment'}, {'p': 'Kim is a clever person.', 'category': 'semantic', 'h': 'Kim is clever.', 'a': 'entailment'}, {'p': 'Many British delegates obtained interesting results from the survey.', 'category': 'syntactic', 'h': 'Many delegates obtained interesting results from the survey.', 'a': 'neutral'}, {'p': 'John is going to Paris by car, and the students by train.', 'category': 'syntactic', 'h': 'The students are going to Paris by train.', 'a': 'entailment'}, {'p': 'ITEL won more orders than APCOM did.', 'category': 'semantic', 'h': 'ITEL won some orders.', 'a': 'entailment'}, {'p': 'No delegate finished the report on time.', 'category': 'syntactic', 'h': 'No delegate finished the report.', 'a': 'neutral'}, {'p': 'Both commissioners used to be businessmen.', 'category': 'syntactic', 'h': 'Both commissioners used to be leading businessmen.', 'a': 'neutral'}, {'p': 'John said Bill had hurt himself.', 'category': 'syntactic', 'h': 'John said Bill had been hurt.', 'a': 'entailment'}, {'p': 'ITEL tried to win the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'neutral'}, {'p': 'Few female committee members are from southern Europe.', 'category': 'syntactic', 'h': 'Few committee members are from southern Europe.', 'a': 'neutral'}, {'p': 'An Irishman won a Nobel prize.', 'category': 'syntactic', 'h': 'An Irishman won the Nobel prize for literature.', 'a': 'neutral'}, {'p': 'The PC-6082 is as fast as the ITEL-XZ.', 'category': 'semantic', 'h': 'The PC-6082 is slower than the ITEL-XZ.', 'a': 'contradiction'}, {'p': 'In two years Smith owned a chain of businesses.', 'category': 'semantic', 'h': 'Smith owned a chain of business for two years.', 'a': 'neutral'}, {'p': 'No delegate finished the report.', 'category': 'syntactic', 'h': 'Some delegate finished the report on time.', 'a': 'contradiction'}, {'p': 'Smith wrote his first novel in 1991.', 'category': 'semantic', 'h': 'Smith wrote his first novel in 1992.', 'a': 'contradiction'}, {'p': 'Few committee members are from southern Europe.', 'category': 'syntactic', 'h': 'Few female committee members are from southern Europe.', 'a': 'entailment'}, {'p': 'Exactly two lawyers and three accountants signed the contract.', 'category': 'syntactic', 'h': 'Six lawyers signed the contract.', 'a': 'contradiction'}, {'p': 'Every mammal is an animal.', 'category': 'semantic', 'h': 'Every four-legged mammal is a four-legged animal.', 'a': 'entailment'}, {'p': 'ITEL won more orders than APCOM.', 'category': 'semantic', 'h': 'ITEL won some orders.', 'a': 'entailment'}, {'p': 'Smith, Anderson and Jones met.', 'category': 'semantic', 'h': 'There was a group of people that met.', 'a': 'entailment'}, {'p': 'At most ten commissioners spend time at home.', 'category': 'syntactic', 'h': 'At most ten commissioners spend a lot of time at home.', 'a': 'entailment'}, {'p': 'No Scandinavian delegate finished the report on time.', 'category': 'syntactic', 'h': 'Some delegate finished the report on time.', 'a': 'neutral'}, {'p': 'In two years Smith owned a chain of businesses.', 'category': 'semantic', 'h': 'Smith owned a chain of business.', 'a': 'entailment'}, {'p': 'Smith was running a business in 1991.', 'category': 'semantic', 'h': 'Smith was running it in 1992.', 'a': 'neutral'}, {'p': 'ITEL built MTALK in 1993.', 'category': 'semantic', 'h': 'ITEL finished MTALK in 1993.', 'a': 'entailment'}, {'p': 'Dumbo is a four-legged animal.', 'category': 'semantic', 'h': 'Dumbo is four-legged.', 'a': 'entailment'}, {'p': 'Smith wrote a report for two hours.', 'category': 'semantic', 'h': 'Smith wrote a report for an hour.', 'a': 'entailment'}, {'p': 'Either Smith, Jones or Anderson signed the contract.', 'category': 'syntactic', 'h': 'Jones signed the contract.', 'a': 'neutral'}, {'p': 'Smith represents his company and so does Jones.', 'category': 'syntactic', 'h': \"Smith represents Jones' company.\", 'a': 'neutral'}, {'p': 'Smith was running his own business in two years.', 'category': 'semantic', 'h': 'Smith ran his own business.', 'a': 'entailment'}, {'p': 'Every representative or client was at the meeting.', 'category': 'syntactic', 'h': 'Every representative and every client was at the meeting.', 'a': 'entailment'}, {'p': 'No one who starts gambling seriously stops until he is broke.', 'category': 'semantic', 'h': 'Everyone who starts gambling seriously continues until he is broke.', 'a': 'entailment'}, {'p': 'A few female committee members are from Scandinavia.', 'category': 'syntactic', 'h': 'At least a few committee members are from Scandinavia.', 'a': 'entailment'}, {'p': 'ITEL won the contract from APCOM in 1993.', 'category': 'semantic', 'h': 'ITEL won a contract in 1993.', 'a': 'entailment'}, {'p': 'At least three commissioners spend time at home.', 'category': 'syntactic', 'h': 'At least three commissioners spend a lot of time at home.', 'a': 'neutral'}, {'p': 'ITEL won more orders than the APCOM contract.', 'category': 'semantic', 'h': 'ITEL won the APCOM contract.', 'a': 'entailment'}, {'p': 'When Smith arrived in Katmandu she had been travelling for three days.', 'category': 'semantic', 'h': 'Smith had been travelling the day before she arrived in Katmandu.', 'a': 'entailment'}, {'p': 'It is false that ITEL won the contract in 1992.', 'category': 'semantic', 'h': 'ITEL won the contract in 1992.', 'a': 'contradiction'}, {'p': 'An Irishman won the Nobel prize for literature.', 'category': 'syntactic', 'h': 'An Irishman won a Nobel prize.', 'a': 'entailment'}, {'p': 'Mary used her workstation.', 'category': 'syntactic', 'h': \"Mary's workstation was used.\", 'a': 'entailment'}, {'p': 'Some delegates finished the survey on time.', 'category': 'syntactic', 'h': 'Some delegates finished the survey.', 'a': 'entailment'}, {'p': 'Smith discovered a new species in 1991.', 'category': 'semantic', 'h': 'Smith discovered it in 1992.', 'a': 'contradiction'}, {'p': 'John wanted to buy a car, and he did.', 'category': 'syntactic', 'h': 'John bought a car.', 'a': 'entailment'}, {'p': 'The people who were at the meeting all voted for a new chairman.', 'category': 'syntactic', 'h': 'Everyone at the meeting voted for a new chairman.', 'a': 'entailment'}, {'p': 'Some great tenors are Swedish.', 'category': 'syntactic', 'h': 'There are great tenors who are Swedish.', 'a': 'entailment'}, {'p': 'John is a fatter politician than Bill.', 'category': 'semantic', 'h': 'John is fatter than Bill.', 'a': 'entailment'}]\n"
     ]
    }
   ],
   "source": [
    "random.Random(25).shuffle(problems)\n",
    "print(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = math.ceil(len(problems) / 2)\n",
    "\n",
    "test_fracas = problems[:split_point]\n",
    "dev_fracas = problems[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fracas_frame = pd.DataFrame(test_fracas)\n",
    "test_fracas_frame.columns = ['premise', 'category', 'hypothesis', 'relation']\n",
    "dev_fracas_frame = pd.DataFrame(dev_fracas)\n",
    "dev_fracas_frame.columns = ['premise', 'category', 'hypothesis', 'relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>category</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A few committee members are from Scandinavia.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At least a few female committee members are fr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smith wrote a report for two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote a report.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smith wrote a novel in 1991.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote it in 1992.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smith and Jones left the meeting.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith left the meeting.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No delegate finished the report on time.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Some Scandinavian delegate finished the report...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>When Jones got his job at the CIA, he knew tha...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>It is the case that Jones is not and will neve...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Either Smith, Jones or Anderson signed the con...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If Smith and Anderson did not sign the contrac...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>In 1994 ITEL sent a progress report every month.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL sent a progress report in July 1994.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Fido is not a small animal.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Fido is a large animal.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Smith was running his own business in two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith spent two years running his own business.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise   category  \\\n",
       "0       A few committee members are from Scandinavia.  syntactic   \n",
       "1                 Smith wrote a report for two hours.   semantic   \n",
       "2                        Smith wrote a novel in 1991.   semantic   \n",
       "3                   Smith and Jones left the meeting.   semantic   \n",
       "4            No delegate finished the report on time.  syntactic   \n",
       "..                                                ...        ...   \n",
       "78  When Jones got his job at the CIA, he knew tha...   semantic   \n",
       "79  Either Smith, Jones or Anderson signed the con...  syntactic   \n",
       "80   In 1994 ITEL sent a progress report every month.   semantic   \n",
       "81                        Fido is not a small animal.   semantic   \n",
       "82   Smith was running his own business in two years.   semantic   \n",
       "\n",
       "                                           hypothesis       relation  \n",
       "0   At least a few female committee members are fr...        neutral  \n",
       "1                               Smith wrote a report.        neutral  \n",
       "2                             Smith wrote it in 1992.  contradiction  \n",
       "3                             Smith left the meeting.     entailment  \n",
       "4   Some Scandinavian delegate finished the report...  contradiction  \n",
       "..                                                ...            ...  \n",
       "78  It is the case that Jones is not and will neve...     entailment  \n",
       "79  If Smith and Anderson did not sign the contrac...     entailment  \n",
       "80          ITEL sent a progress report in July 1994.     entailment  \n",
       "81                            Fido is a large animal.        neutral  \n",
       "82    Smith spent two years running his own business.        neutral  \n",
       "\n",
       "[83 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fracas_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>category</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith claimed he had costed his proposal and s...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Jones claimed Smith had costed Jones' proposal.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neither commissioner spends time at home.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>One of the commissioners spends a lot of time ...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Several Portuguese delegates got the results p...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Several delegates got the results published in...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Many delegates obtained interesting results fr...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Many delegates obtained results from the survey.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At least three commissioners spend a lot of ti...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At least three commissioners spend time at home.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Smith discovered a new species in 1991.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith discovered it in 1992.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>John wanted to buy a car, and he did.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>John bought a car.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>The people who were at the meeting all voted f...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Everyone at the meeting voted for a new chairman.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Some great tenors are Swedish.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are great tenors who are Swedish.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>John is a fatter politician than Bill.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>John is fatter than Bill.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise   category  \\\n",
       "0   Smith claimed he had costed his proposal and s...  syntactic   \n",
       "1           Neither commissioner spends time at home.  syntactic   \n",
       "2   Several Portuguese delegates got the results p...  syntactic   \n",
       "3   Many delegates obtained interesting results fr...  syntactic   \n",
       "4   At least three commissioners spend a lot of ti...  syntactic   \n",
       "..                                                ...        ...   \n",
       "77            Smith discovered a new species in 1991.   semantic   \n",
       "78              John wanted to buy a car, and he did.  syntactic   \n",
       "79  The people who were at the meeting all voted f...  syntactic   \n",
       "80                     Some great tenors are Swedish.  syntactic   \n",
       "81             John is a fatter politician than Bill.   semantic   \n",
       "\n",
       "                                           hypothesis       relation  \n",
       "0     Jones claimed Smith had costed Jones' proposal.        neutral  \n",
       "1   One of the commissioners spends a lot of time ...  contradiction  \n",
       "2   Several delegates got the results published in...     entailment  \n",
       "3    Many delegates obtained results from the survey.     entailment  \n",
       "4    At least three commissioners spend time at home.     entailment  \n",
       "..                                                ...            ...  \n",
       "77                       Smith discovered it in 1992.  contradiction  \n",
       "78                                 John bought a car.     entailment  \n",
       "79  Everyone at the meeting voted for a new chairman.     entailment  \n",
       "80            There are great tenors who are Swedish.     entailment  \n",
       "81                          John is fatter than Bill.     entailment  \n",
       "\n",
       "[82 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_fracas_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will want to see the performance on syntactic vs. semantic examples, so I will divide the test set into those two\n",
    "syn_test_fracas = []\n",
    "sem_test_fracas = []\n",
    "\n",
    "for item in test_fracas:\n",
    "    if item[\"category\"] == 'syntactic':\n",
    "        syn_test_fracas.append(item)\n",
    "    else:  # if category == semantic\n",
    "        sem_test_fracas.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_test_fracas_frame = pd.DataFrame(syn_test_fracas)\n",
    "syn_test_fracas_frame.columns = ['premise', 'category', 'hypothesis', 'relation']\n",
    "sem_test_fracas_frame = pd.DataFrame(sem_test_fracas)\n",
    "sem_test_fracas_frame.columns = ['premise', 'category', 'hypothesis', 'relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>category</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A few committee members are from Scandinavia.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At least a few female committee members are fr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No delegate finished the report on time.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Some Scandinavian delegate finished the report...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some Irish delegates finished the survey on time.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Some delegates finished the survey on time.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Several delegates got the results published in...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Several delegates got the results published.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill suggested to Frank's boss that they shoul...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If it was suggested that Bill and Frank's boss...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mary used her workstation.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Mary is female.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Italian became the world's greatest tenor.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There was an Italian who became the world's gr...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>John said Bill had hurt himself.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Someone said John had been hurt.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Several great tenors are British.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are great tenors who are British.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Most Europeans who are resident in Europe can ...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Most Europeans can travel freely within Europe.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bill suggested to Frank's boss that they shoul...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If it was suggested that Bill and Frank's boss...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>At least three commissioners spend time at home.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At least three male commissioners spend time a...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>John went to Paris by car, and Bill by train t...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Bill went to Berlin by train.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Many great tenors are German.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are great tenors who are German.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Just one accountant attended the meeting.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Some accountants attended the meeting.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Some delegates finished the survey.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Some delegates finished the survey on time.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>John wrote a report, and Bill said Peter did too.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Bill said Peter wrote a report.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Both commissioners used to be leading business...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Both commissioners used to be businessmen.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>At most ten female commissioners spend time at...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At most ten commissioners spend time at home.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>All the people who were at the meeting voted f...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Everyone at the meeting voted for a new chairman.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bill suggested to Frank's boss that they shoul...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If it was suggested that Bill and Frank should...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>At least three female commissioners spend time...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At least three commissioners spend time at home.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Many delegates obtained interesting results fr...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Many British delegates obtained interesting re...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>At most ten commissioners spend time at home.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>At most ten female commissioners spend time at...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Just one accountant attended the meeting.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>No accountants attended the meeting.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Exactly two lawyers and three accountants sign...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Six accountants signed the contract.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bill suggested to Frank's boss that they shoul...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If it was suggested that Bill, Frank and Frank...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Neither commissioner spends a lot of time at h...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Neither commissioner spends time at home.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Most Europeans can travel freely within Europe.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Most Europeans who are resident outside Europe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Smith, Jones and several lawyers signed the co...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Jones signed the contract.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The really ambitious tenors are Italian.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>There are really ambitious tenors who are Ital...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The chairman read out the items on the agenda.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>The chairman read out every item on the agenda.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Many delegates obtained results from the survey.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Many delegates obtained interesting results fr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A company director awarded himself a large pay...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>A company director has awarded and been awarde...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>John said that Mary wrote a report, and that B...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Bill said Mary wrote a report.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Clients at the demonstration were impressed by...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Most clients at the demonstration were impress...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>John wants to know how many men work part time...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>John wants to know which men work part time.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Several delegates got the results published in...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Several Portuguese delegates got the results p...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Mary used her workstation.</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Mary has a workstation.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Either Smith, Jones or Anderson signed the con...</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>If Smith and Anderson did not sign the contrac...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise   category  \\\n",
       "0       A few committee members are from Scandinavia.  syntactic   \n",
       "1            No delegate finished the report on time.  syntactic   \n",
       "2   Some Irish delegates finished the survey on time.  syntactic   \n",
       "3   Several delegates got the results published in...  syntactic   \n",
       "4   Bill suggested to Frank's boss that they shoul...  syntactic   \n",
       "5                          Mary used her workstation.  syntactic   \n",
       "6       An Italian became the world's greatest tenor.  syntactic   \n",
       "7                    John said Bill had hurt himself.  syntactic   \n",
       "8                   Several great tenors are British.  syntactic   \n",
       "9   Most Europeans who are resident in Europe can ...  syntactic   \n",
       "10  Bill suggested to Frank's boss that they shoul...  syntactic   \n",
       "11   At least three commissioners spend time at home.  syntactic   \n",
       "12  John went to Paris by car, and Bill by train t...  syntactic   \n",
       "13                      Many great tenors are German.  syntactic   \n",
       "14          Just one accountant attended the meeting.  syntactic   \n",
       "15                Some delegates finished the survey.  syntactic   \n",
       "16  John wrote a report, and Bill said Peter did too.  syntactic   \n",
       "17  Both commissioners used to be leading business...  syntactic   \n",
       "18  At most ten female commissioners spend time at...  syntactic   \n",
       "19  All the people who were at the meeting voted f...  syntactic   \n",
       "20  Bill suggested to Frank's boss that they shoul...  syntactic   \n",
       "21  At least three female commissioners spend time...  syntactic   \n",
       "22  Many delegates obtained interesting results fr...  syntactic   \n",
       "23      At most ten commissioners spend time at home.  syntactic   \n",
       "24          Just one accountant attended the meeting.  syntactic   \n",
       "25  Exactly two lawyers and three accountants sign...  syntactic   \n",
       "26  Bill suggested to Frank's boss that they shoul...  syntactic   \n",
       "27  Neither commissioner spends a lot of time at h...  syntactic   \n",
       "28    Most Europeans can travel freely within Europe.  syntactic   \n",
       "29  Smith, Jones and several lawyers signed the co...  syntactic   \n",
       "30           The really ambitious tenors are Italian.  syntactic   \n",
       "31     The chairman read out the items on the agenda.  syntactic   \n",
       "32   Many delegates obtained results from the survey.  syntactic   \n",
       "33  A company director awarded himself a large pay...  syntactic   \n",
       "34  John said that Mary wrote a report, and that B...  syntactic   \n",
       "35  Clients at the demonstration were impressed by...  syntactic   \n",
       "36  John wants to know how many men work part time...  syntactic   \n",
       "37  Several delegates got the results published in...  syntactic   \n",
       "38                         Mary used her workstation.  syntactic   \n",
       "39  Either Smith, Jones or Anderson signed the con...  syntactic   \n",
       "\n",
       "                                           hypothesis       relation  \n",
       "0   At least a few female committee members are fr...        neutral  \n",
       "1   Some Scandinavian delegate finished the report...  contradiction  \n",
       "2         Some delegates finished the survey on time.     entailment  \n",
       "3        Several delegates got the results published.     entailment  \n",
       "4   If it was suggested that Bill and Frank's boss...        neutral  \n",
       "5                                     Mary is female.     entailment  \n",
       "6   There was an Italian who became the world's gr...     entailment  \n",
       "7                    Someone said John had been hurt.        neutral  \n",
       "8             There are great tenors who are British.     entailment  \n",
       "9     Most Europeans can travel freely within Europe.        neutral  \n",
       "10  If it was suggested that Bill and Frank's boss...     entailment  \n",
       "11  At least three male commissioners spend time a...        neutral  \n",
       "12                      Bill went to Berlin by train.     entailment  \n",
       "13             There are great tenors who are German.     entailment  \n",
       "14             Some accountants attended the meeting.     entailment  \n",
       "15        Some delegates finished the survey on time.        neutral  \n",
       "16                    Bill said Peter wrote a report.     entailment  \n",
       "17         Both commissioners used to be businessmen.     entailment  \n",
       "18      At most ten commissioners spend time at home.        neutral  \n",
       "19  Everyone at the meeting voted for a new chairman.     entailment  \n",
       "20  If it was suggested that Bill and Frank should...     entailment  \n",
       "21   At least three commissioners spend time at home.     entailment  \n",
       "22  Many British delegates obtained interesting re...        neutral  \n",
       "23  At most ten female commissioners spend time at...     entailment  \n",
       "24               No accountants attended the meeting.  contradiction  \n",
       "25               Six accountants signed the contract.  contradiction  \n",
       "26  If it was suggested that Bill, Frank and Frank...     entailment  \n",
       "27          Neither commissioner spends time at home.        neutral  \n",
       "28  Most Europeans who are resident outside Europe...        neutral  \n",
       "29                         Jones signed the contract.     entailment  \n",
       "30  There are really ambitious tenors who are Ital...     entailment  \n",
       "31    The chairman read out every item on the agenda.     entailment  \n",
       "32  Many delegates obtained interesting results fr...        neutral  \n",
       "33  A company director has awarded and been awarde...     entailment  \n",
       "34                     Bill said Mary wrote a report.        neutral  \n",
       "35  Most clients at the demonstration were impress...     entailment  \n",
       "36       John wants to know which men work part time.     entailment  \n",
       "37  Several Portuguese delegates got the results p...        neutral  \n",
       "38                            Mary has a workstation.     entailment  \n",
       "39  If Smith and Anderson did not sign the contrac...     entailment  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_test_fracas_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>category</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smith wrote a report for two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote a report.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smith wrote a novel in 1991.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote it in 1992.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smith and Jones left the meeting.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith left the meeting.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smith ran his own business for two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith ran his own business.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John has a genuine diamond.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>John has a diamond.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Smith wrote a report in two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith spent more than two hours writing the re...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ITEL won more orders than APCOM lost.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>APCOM lost some orders.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The PC-6082 is as fast as the ITEL-XZ.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>The PC-6082 is fast.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Smith discovered new species for two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith discovered new species.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>John is a former successful university student.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>John is a university student.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ITEL was winning the contract from APCOM in 1993.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL won a contract in 1993.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Smith lived in Birmingham for two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith lived in Birmingham for exactly a year.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ITEL managed to win the contract in 1992.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL won the contract in 1992.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>In two years Smith owned a chain of businesses.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith owned a chain of business for more than ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Smith discovered a new species in two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith spent two hours discovering the new spec...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Smith saw Jones sign the contract and his secr...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith saw Jones sign the contract.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Smith saw Jones sign the contract or cross out...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith either saw Jones sign the contract or sa...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fido is not a large animal.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Fido is a small animal.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Smith and Jones left the meeting.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Jones left the meeting.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The PC-6082 is faster than the ITEL-XZ.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>The PC-6082 is fast.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ITEL was building MTALK in 1993.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL finished MTALK in 1993.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Smith was running his own business in two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith spent more than two years running his ow...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The PC-6082 is faster than the ITEL-ZX and the...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>The PC-6082 is faster than the ITEL-ZX.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>John is a former successful university student.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>John is successful.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Smith left the meeting before he lost his temper.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith lost his temper.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ITEL won more orders than APCOM.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>APCOM won some orders.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Smith discovered a new species in 1991.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith discovered a new species in 1992.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Smith discovered a new species in two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith discovered a new species.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Smith wrote a report in two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote a report in one hour.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Kim is a clever politician.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Kim is clever.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Dumbo is a large animal.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Dumbo is a small animal.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>John is a cleverer politician than Bill.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>John is cleverer than Bill.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ITEL won more orders than APCOM did.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>APCOM won some orders.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Last week I already knew that when, in a month...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>It will be the case that in a few weeks Smith ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mickey is a small animal.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Mickey is a large animal.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Smith lived in Birmingham in 1991.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith lived in Birmingham in 1992.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Smith wrote a report in two hours.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith wrote a report.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Smith ran his own business for two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith ran his own business for a year.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>In March 1993 APCOM founded ITEL.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL existed in 1992.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>When Jones got his job at the CIA, he knew tha...</td>\n",
       "      <td>semantic</td>\n",
       "      <td>It is the case that Jones is not and will neve...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>In 1994 ITEL sent a progress report every month.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>ITEL sent a progress report in July 1994.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Fido is not a small animal.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Fido is a large animal.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Smith was running his own business in two years.</td>\n",
       "      <td>semantic</td>\n",
       "      <td>Smith spent two years running his own business.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  category  \\\n",
       "0                 Smith wrote a report for two hours.  semantic   \n",
       "1                        Smith wrote a novel in 1991.  semantic   \n",
       "2                   Smith and Jones left the meeting.  semantic   \n",
       "3           Smith ran his own business for two years.  semantic   \n",
       "4                         John has a genuine diamond.  semantic   \n",
       "5                  Smith wrote a report in two hours.  semantic   \n",
       "6               ITEL won more orders than APCOM lost.  semantic   \n",
       "7              The PC-6082 is as fast as the ITEL-XZ.  semantic   \n",
       "8         Smith discovered new species for two years.  semantic   \n",
       "9     John is a former successful university student.  semantic   \n",
       "10  ITEL was winning the contract from APCOM in 1993.  semantic   \n",
       "11           Smith lived in Birmingham for two years.  semantic   \n",
       "12          ITEL managed to win the contract in 1992.  semantic   \n",
       "13    In two years Smith owned a chain of businesses.  semantic   \n",
       "14       Smith discovered a new species in two hours.  semantic   \n",
       "15  Smith saw Jones sign the contract and his secr...  semantic   \n",
       "16  Smith saw Jones sign the contract or cross out...  semantic   \n",
       "17                        Fido is not a large animal.  semantic   \n",
       "18                  Smith and Jones left the meeting.  semantic   \n",
       "19            The PC-6082 is faster than the ITEL-XZ.  semantic   \n",
       "20                   ITEL was building MTALK in 1993.  semantic   \n",
       "21   Smith was running his own business in two years.  semantic   \n",
       "22  The PC-6082 is faster than the ITEL-ZX and the...  semantic   \n",
       "23    John is a former successful university student.  semantic   \n",
       "24  Smith left the meeting before he lost his temper.  semantic   \n",
       "25                   ITEL won more orders than APCOM.  semantic   \n",
       "26            Smith discovered a new species in 1991.  semantic   \n",
       "27       Smith discovered a new species in two hours.  semantic   \n",
       "28                 Smith wrote a report in two hours.  semantic   \n",
       "29                        Kim is a clever politician.  semantic   \n",
       "30                           Dumbo is a large animal.  semantic   \n",
       "31           John is a cleverer politician than Bill.  semantic   \n",
       "32               ITEL won more orders than APCOM did.  semantic   \n",
       "33  Last week I already knew that when, in a month...  semantic   \n",
       "34                          Mickey is a small animal.  semantic   \n",
       "35                 Smith lived in Birmingham in 1991.  semantic   \n",
       "36                 Smith wrote a report in two hours.  semantic   \n",
       "37          Smith ran his own business for two years.  semantic   \n",
       "38                  In March 1993 APCOM founded ITEL.  semantic   \n",
       "39  When Jones got his job at the CIA, he knew tha...  semantic   \n",
       "40   In 1994 ITEL sent a progress report every month.  semantic   \n",
       "41                        Fido is not a small animal.  semantic   \n",
       "42   Smith was running his own business in two years.  semantic   \n",
       "\n",
       "                                           hypothesis       relation  \n",
       "0                               Smith wrote a report.        neutral  \n",
       "1                             Smith wrote it in 1992.  contradiction  \n",
       "2                             Smith left the meeting.     entailment  \n",
       "3                         Smith ran his own business.     entailment  \n",
       "4                                 John has a diamond.     entailment  \n",
       "5   Smith spent more than two hours writing the re...  contradiction  \n",
       "6                             APCOM lost some orders.        neutral  \n",
       "7                                The PC-6082 is fast.        neutral  \n",
       "8                       Smith discovered new species.     entailment  \n",
       "9                       John is a university student.        neutral  \n",
       "10                       ITEL won a contract in 1993.        neutral  \n",
       "11      Smith lived in Birmingham for exactly a year.  contradiction  \n",
       "12                     ITEL won the contract in 1992.     entailment  \n",
       "13  Smith owned a chain of business for more than ...        neutral  \n",
       "14  Smith spent two hours discovering the new spec...  contradiction  \n",
       "15                 Smith saw Jones sign the contract.     entailment  \n",
       "16  Smith either saw Jones sign the contract or sa...     entailment  \n",
       "17                            Fido is a small animal.        neutral  \n",
       "18                            Jones left the meeting.     entailment  \n",
       "19                               The PC-6082 is fast.        neutral  \n",
       "20                       ITEL finished MTALK in 1993.        neutral  \n",
       "21  Smith spent more than two years running his ow...        neutral  \n",
       "22            The PC-6082 is faster than the ITEL-ZX.     entailment  \n",
       "23                                John is successful.        neutral  \n",
       "24                             Smith lost his temper.        neutral  \n",
       "25                             APCOM won some orders.        neutral  \n",
       "26            Smith discovered a new species in 1992.        neutral  \n",
       "27                    Smith discovered a new species.     entailment  \n",
       "28                  Smith wrote a report in one hour.        neutral  \n",
       "29                                     Kim is clever.        neutral  \n",
       "30                           Dumbo is a small animal.  contradiction  \n",
       "31                        John is cleverer than Bill.        neutral  \n",
       "32                             APCOM won some orders.        neutral  \n",
       "33  It will be the case that in a few weeks Smith ...     entailment  \n",
       "34                          Mickey is a large animal.  contradiction  \n",
       "35                 Smith lived in Birmingham in 1992.        neutral  \n",
       "36                              Smith wrote a report.     entailment  \n",
       "37             Smith ran his own business for a year.     entailment  \n",
       "38                              ITEL existed in 1992.  contradiction  \n",
       "39  It is the case that Jones is not and will neve...     entailment  \n",
       "40          ITEL sent a progress report in July 1994.     entailment  \n",
       "41                            Fido is a large animal.        neutral  \n",
       "42    Smith spent two years running his own business.        neutral  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_test_fracas_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This section is roughly the same as in Lab 5, with the exception of using PyTorch max pooling rather than a custom (and slower) function. I have also changed the embedding size relative to what was submitted in the draft submission of Lab 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "----\n",
    "I will not use my custom max pooling function, as it was rather slow. Instead, I will use the PyTorch one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1323, 0.7823, 0.5127, 0.9199, 0.3370],\n",
      "         [0.3986, 0.7415, 0.1556, 0.1236, 0.9320],\n",
      "         [0.7010, 0.7487, 0.6831, 0.8826, 0.2171]],\n",
      "\n",
      "        [[0.0781, 0.1650, 0.0951, 0.0395, 0.1147],\n",
      "         [0.7370, 0.1587, 0.0340, 0.9292, 0.9115],\n",
      "         [0.0550, 0.2563, 0.2875, 0.2127, 0.1141]],\n",
      "\n",
      "        [[0.0838, 0.2077, 0.7513, 0.6818, 0.7008],\n",
      "         [0.5850, 0.8879, 0.3350, 0.7218, 0.6587],\n",
      "         [0.1865, 0.7171, 0.5937, 0.8862, 0.8820]],\n",
      "\n",
      "        [[0.3829, 0.5779, 0.0744, 0.1269, 0.7876],\n",
      "         [0.6733, 0.6044, 0.9346, 0.4400, 0.7812],\n",
      "         [0.1369, 0.6862, 0.2518, 0.8547, 0.5197]],\n",
      "\n",
      "        [[0.3529, 0.6909, 0.0228, 0.5946, 0.6634],\n",
      "         [0.8360, 0.4068, 0.0725, 0.4669, 0.8832],\n",
      "         [0.3467, 0.7944, 0.9746, 0.6484, 0.5324]],\n",
      "\n",
      "        [[0.1317, 0.1929, 0.7879, 0.9044, 0.9152],\n",
      "         [0.5748, 0.4898, 0.7750, 0.2756, 0.1717],\n",
      "         [0.2656, 0.2103, 0.6207, 0.3750, 0.3937]],\n",
      "\n",
      "        [[0.1467, 0.4918, 0.5485, 0.6889, 0.4375],\n",
      "         [0.2035, 0.1219, 0.5624, 0.2148, 0.7853],\n",
      "         [0.7589, 0.8200, 0.9739, 0.6421, 0.0735]],\n",
      "\n",
      "        [[0.4168, 0.1324, 0.4621, 0.8754, 0.2955],\n",
      "         [0.0045, 0.9071, 0.3971, 0.3895, 0.8779],\n",
      "         [0.4414, 0.1824, 0.7231, 0.8214, 0.0084]]], device='cuda:2',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "dimensions = 5\n",
    "# A tensor for testing\n",
    "test_tensor = torch.rand([batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7010, 0.7823, 0.6831, 0.9199, 0.9320],\n",
       "        [0.7370, 0.2563, 0.2875, 0.9292, 0.9115],\n",
       "        [0.5850, 0.8879, 0.7513, 0.8862, 0.8820],\n",
       "        [0.6733, 0.6862, 0.9346, 0.8547, 0.7876],\n",
       "        [0.8360, 0.7944, 0.9746, 0.6484, 0.8832],\n",
       "        [0.5748, 0.4898, 0.7879, 0.9044, 0.9152],\n",
       "        [0.7589, 0.8200, 0.9739, 0.6889, 0.7853],\n",
       "        [0.4414, 0.9071, 0.7231, 0.8754, 0.8779]], device='cuda:2',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(test_tensor, dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "----\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8366, 0.3686, 0.4485, 0.9285, 0.9173],\n",
      "        [0.9342, 0.9869, 0.9526, 0.9503, 0.8760],\n",
      "        [0.9636, 0.6425, 0.6769, 0.8461, 0.7809],\n",
      "        [0.9657, 0.7991, 0.9479, 0.9716, 0.9304],\n",
      "        [0.6827, 0.8800, 0.5000, 0.6977, 0.4619],\n",
      "        [0.9638, 0.7895, 0.7666, 0.5614, 0.9902],\n",
      "        [0.5916, 0.8364, 0.4759, 0.7160, 0.7296],\n",
      "        [0.4630, 0.7979, 0.8645, 0.9520, 0.4016]], device='cuda:2',\n",
      "       dtype=torch.float64)\n",
      "tensor([[0.3498, 0.7195, 0.8864, 0.4609, 0.7742],\n",
      "        [0.2673, 0.8991, 0.4715, 0.8393, 0.3224],\n",
      "        [0.8582, 0.8807, 0.7806, 0.7027, 0.6609],\n",
      "        [0.5979, 0.3729, 0.7625, 0.8292, 0.8282],\n",
      "        [0.9695, 0.5026, 0.5869, 0.8204, 0.9345],\n",
      "        [0.9071, 0.6866, 0.8673, 0.5703, 0.5868],\n",
      "        [0.7155, 0.8885, 0.4790, 0.9022, 0.6091],\n",
      "        [0.6841, 0.9987, 0.9844, 0.9081, 0.9202]], device='cuda:2',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Test tensors (size of batch, num, dim)\n",
    "t = torch.rand([2*batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "t1, t2 = torch.split(t, batch_size)\n",
    "# Pooled test tensors (size of batch, dim)\n",
    "pt1 = torch.max(t1, dim=1)[0]\n",
    "pt2 = torch.max(t2, dim=1)[0]\n",
    "print(pt1)\n",
    "print(pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    \n",
    "    batches = len(hypothesis)\n",
    "    dims = len(hypothesis[0])\n",
    "    final_dims = 4*dims\n",
    "\n",
    "    new_tensors = []\n",
    "\n",
    "    for i in range(0,batches):\n",
    "        hyp = hypothesis[i]\n",
    "        pre = premise[i]\n",
    "    \n",
    "        summed = torch.cat((pre,hyp))\n",
    "        subtracted = pre - hyp\n",
    "        multiplied = torch.mul(pre, hyp)\n",
    "    \n",
    "        new_tensors.append(torch.cat((summed, subtracted, multiplied)))\n",
    "    \n",
    "    output = torch.stack(new_tensors)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3498,  0.7195,  0.8864,  0.4609,  0.7742,  0.8366,  0.3686,  0.4485,\n",
       "          0.9285,  0.9173, -0.4868,  0.3509,  0.4379, -0.4676, -0.1431,  0.2926,\n",
       "          0.2652,  0.3975,  0.4280,  0.7102],\n",
       "        [ 0.2673,  0.8991,  0.4715,  0.8393,  0.3224,  0.9342,  0.9869,  0.9526,\n",
       "          0.9503,  0.8760, -0.6669, -0.0877, -0.4811, -0.1110, -0.5537,  0.2497,\n",
       "          0.8873,  0.4492,  0.7975,  0.2824],\n",
       "        [ 0.8582,  0.8807,  0.7806,  0.7027,  0.6609,  0.9636,  0.6425,  0.6769,\n",
       "          0.8461,  0.7809, -0.1054,  0.2381,  0.1037, -0.1433, -0.1200,  0.8270,\n",
       "          0.5658,  0.5284,  0.5946,  0.5161],\n",
       "        [ 0.5979,  0.3729,  0.7625,  0.8292,  0.8282,  0.9657,  0.7991,  0.9479,\n",
       "          0.9716,  0.9304, -0.3677, -0.4262, -0.1854, -0.1423, -0.1022,  0.5774,\n",
       "          0.2980,  0.7228,  0.8057,  0.7706],\n",
       "        [ 0.9695,  0.5026,  0.5869,  0.8204,  0.9345,  0.6827,  0.8800,  0.5000,\n",
       "          0.6977,  0.4619,  0.2868, -0.3774,  0.0869,  0.1228,  0.4725,  0.6619,\n",
       "          0.4423,  0.2934,  0.5724,  0.4316],\n",
       "        [ 0.9071,  0.6866,  0.8673,  0.5703,  0.5868,  0.9638,  0.7895,  0.7666,\n",
       "          0.5614,  0.9902, -0.0567, -0.1030,  0.1007,  0.0090, -0.4033,  0.8742,\n",
       "          0.5421,  0.6648,  0.3202,  0.5810],\n",
       "        [ 0.7155,  0.8885,  0.4790,  0.9022,  0.6091,  0.5916,  0.8364,  0.4759,\n",
       "          0.7160,  0.7296,  0.1238,  0.0520,  0.0030,  0.1862, -0.1205,  0.4233,\n",
       "          0.7431,  0.2279,  0.6460,  0.4444],\n",
       "        [ 0.6841,  0.9987,  0.9844,  0.9081,  0.9202,  0.4630,  0.7979,  0.8645,\n",
       "          0.9520,  0.4016,  0.2211,  0.2008,  0.1199, -0.0439,  0.5186,  0.3168,\n",
       "          0.7969,  0.8510,  0.8645,  0.3696]], device='cuda:2',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_premise_and_hypothesis(pt1, pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, word2idx, relation2idx, embedding_dim=64, hidden_size=128, padding_idx=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(word2idx)\n",
    "        self.output_dim = len(relation2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        # your code goes here\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, embedding_dim, padding_idx=padding_idx) #\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size*8, self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = self.embeddings(premise)\n",
    "        h = self.embeddings(hypothesis)\n",
    "        \n",
    "        lstm_p, (hidden, c) = self.LSTM(p)\n",
    "        lstm_h, (hidden, c) = self.LSTM(h)\n",
    "        \n",
    "        p_pooled = torch.max(lstm_p, dim=1)[0]\n",
    "        h_pooled = torch.max(lstm_h, dim=1)[0]\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(h_pooled,p_pooled)\n",
    "        ph_representation = self.dropout(ph_representation)  # is this at the right stage??\n",
    "        \n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**This section re-uses the training loop from Lab 5 and adapts its evaluation loop into a function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InferenceDataset(train_data)\n",
    "test_dataset = InferenceDataset(test_data)\n",
    "# the dev set was used when testing if this works\n",
    "dev_dataset = InferenceDataset(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_fracas_dataset = InferenceDataset(dev_fracas_frame)\n",
    "test_fracas_dataset = InferenceDataset(test_fracas_frame)\n",
    "syn_test_fracas_dataset = InferenceDataset(syn_test_fracas_frame)\n",
    "sem_test_fracas_dataset = InferenceDataset(sem_test_fracas_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloader(train_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "test_loader = dataloader(test_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "dev_loader = dataloader(dev_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "dev_fracas_loader = dataloader(dev_fracas_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "test_fracas_loader = dataloader(test_fracas_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "syn_test_fracas_loader = dataloader(syn_test_fracas_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)\n",
    "sem_test_fracas_loader = dataloader(sem_test_fracas_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model just on SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Loss = 1.40134\n",
      " Batch 200 : Average Loss = 1.10077\n",
      " Batch 400 : Average Loss = 1.07978\n",
      " Batch 600 : Average Loss = 1.06881\n",
      " Batch 800 : Average Loss = 1.05873\n",
      " Batch 1000 : Average Loss = 1.04806\n",
      " Batch 1200 : Average Loss = 1.03642\n",
      " Batch 1400 : Average Loss = 1.03038\n",
      " Batch 1600 : Average Loss = 1.02504\n",
      " Batch 1800 : Average Loss = 1.01949\n",
      " Batch 2000 : Average Loss = 1.01454\n",
      " Batch 2200 : Average Loss = 1.00815\n",
      " Batch 2400 : Average Loss = 1.00342\n",
      " Batch 2600 : Average Loss = 0.99813\n",
      " Batch 2800 : Average Loss = 0.99525\n",
      " Batch 3000 : Average Loss = 0.99171\n",
      " Batch 3200 : Average Loss = 0.98877\n",
      " Batch 3400 : Average Loss = 0.98656\n",
      " Batch 3600 : Average Loss = 0.98324\n",
      " Batch 3800 : Average Loss = 0.98025\n",
      " Batch 4000 : Average Loss = 0.97578\n",
      " Batch 4200 : Average Loss = 0.97138\n",
      " Batch 4400 : Average Loss = 0.96819\n",
      " Batch 4600 : Average Loss = 0.96609\n",
      " Batch 4800 : Average Loss = 0.96293\n",
      " Batch 5000 : Average Loss = 0.961\n",
      " Batch 5200 : Average Loss = 0.95769\n",
      " Batch 5400 : Average Loss = 0.95556\n",
      " Batch 5600 : Average Loss = 0.95358\n",
      " Batch 5800 : Average Loss = 0.95087\n",
      " Batch 6000 : Average Loss = 0.94803\n",
      " Batch 6200 : Average Loss = 0.94518\n",
      " Batch 6400 : Average Loss = 0.942\n",
      " Batch 6600 : Average Loss = 0.93938\n",
      " Batch 6800 : Average Loss = 0.93639\n",
      " Batch 7000 : Average Loss = 0.93477\n",
      " Batch 7200 : Average Loss = 0.93224\n",
      " Batch 7400 : Average Loss = 0.93078\n",
      " Batch 7600 : Average Loss = 0.92935\n",
      " Batch 7800 : Average Loss = 0.92749\n",
      " Batch 8000 : Average Loss = 0.92497\n",
      " Batch 8200 : Average Loss = 0.92376\n",
      " Batch 8400 : Average Loss = 0.92243\n",
      " Batch 8600 : Average Loss = 0.92094\n",
      " Batch 8800 : Average Loss = 0.91948\n",
      " Batch 9000 : Average Loss = 0.91779\n",
      " Batch 9200 : Average Loss = 0.91678\n",
      " Batch 9400 : Average Loss = 0.91487\n",
      " Batch 9600 : Average Loss = 0.91315\n",
      " Batch 9800 : Average Loss = 0.91213\n",
      " Batch 10000 : Average Loss = 0.91092\n",
      " Batch 10200 : Average Loss = 0.90931\n",
      " Batch 10400 : Average Loss = 0.90738\n",
      " Batch 10600 : Average Loss = 0.9056\n",
      " Batch 10800 : Average Loss = 0.90364\n",
      " Batch 11000 : Average Loss = 0.90199\n",
      " Batch 11200 : Average Loss = 0.90054\n",
      " Batch 11400 : Average Loss = 0.89879\n",
      " Batch 11600 : Average Loss = 0.89755\n",
      " Batch 11800 : Average Loss = 0.89655\n",
      " Batch 12000 : Average Loss = 0.89502\n",
      " Batch 12200 : Average Loss = 0.89396\n",
      " Batch 12400 : Average Loss = 0.89258\n",
      " Batch 12600 : Average Loss = 0.89147\n",
      " Batch 12800 : Average Loss = 0.88984\n",
      " Batch 13000 : Average Loss = 0.88937\n",
      " Batch 13200 : Average Loss = 0.88839\n",
      " Batch 13400 : Average Loss = 0.88713\n",
      " Batch 13600 : Average Loss = 0.88625\n",
      " Batch 13800 : Average Loss = 0.88515\n",
      " Batch 14000 : Average Loss = 0.88356\n",
      " Batch 14200 : Average Loss = 0.88293\n",
      " Batch 14400 : Average Loss = 0.88214\n",
      " Batch 14600 : Average Loss = 0.88146\n",
      " Batch 14800 : Average Loss = 0.87978\n",
      " Batch 15000 : Average Loss = 0.87893\n",
      " Batch 15200 : Average Loss = 0.87795\n",
      " Batch 15400 : Average Loss = 0.87705\n",
      " Batch 15600 : Average Loss = 0.87626\n",
      " Batch 15800 : Average Loss = 0.87508\n",
      " Batch 16000 : Average Loss = 0.87412\n",
      " Batch 16200 : Average Loss = 0.87326\n",
      " Batch 16400 : Average Loss = 0.87252\n",
      " Batch 16600 : Average Loss = 0.87167\n",
      " Batch 16800 : Average Loss = 0.87064\n",
      " Batch 17000 : Average Loss = 0.86967\n",
      " Batch 17200 : Average Loss = 0.86882\n",
      " Batch 17400 : Average Loss = 0.86785\n",
      " Batch 17600 : Average Loss = 0.86712\n",
      " Batch 17800 : Average Loss = 0.86626\n",
      " Batch 18000 : Average Loss = 0.86529\n",
      " Batch 18200 : Average Loss = 0.86476\n",
      " Batch 18400 : Average Loss = 0.8646\n",
      " Batch 18600 : Average Loss = 0.86348\n",
      " Batch 18800 : Average Loss = 0.86249\n",
      " Batch 19000 : Average Loss = 0.86196\n",
      " Batch 19200 : Average Loss = 0.86106\n",
      " Batch 19400 : Average Loss = 0.86004\n",
      " Batch 19600 : Average Loss = 0.85909\n",
      " Batch 19800 : Average Loss = 0.85836\n",
      " Batch 20000 : Average Loss = 0.8579\n",
      " Batch 20200 : Average Loss = 0.85733\n",
      " Batch 20400 : Average Loss = 0.85706\n",
      " Batch 20600 : Average Loss = 0.85625\n",
      " Batch 20800 : Average Loss = 0.85532\n",
      " Batch 21000 : Average Loss = 0.85459\n",
      " Batch 21200 : Average Loss = 0.85374\n",
      " Batch 21400 : Average Loss = 0.85317\n",
      " Batch 21600 : Average Loss = 0.85264\n",
      " Batch 21800 : Average Loss = 0.85201\n",
      " Batch 22000 : Average Loss = 0.85159\n",
      " Batch 22200 : Average Loss = 0.85106\n",
      " Batch 22400 : Average Loss = 0.85068\n",
      " Batch 22600 : Average Loss = 0.85022\n",
      " Batch 22800 : Average Loss = 0.84958\n",
      " Batch 23000 : Average Loss = 0.84903\n",
      " Batch 23200 : Average Loss = 0.8483\n",
      " Batch 23400 : Average Loss = 0.84745\n",
      " Batch 23600 : Average Loss = 0.84685\n",
      " Batch 23800 : Average Loss = 0.84625\n",
      " Batch 24000 : Average Loss = 0.8455\n",
      " Batch 24200 : Average Loss = 0.84465\n",
      " Batch 24400 : Average Loss = 0.84407\n",
      " Batch 24600 : Average Loss = 0.84347\n",
      " Batch 24800 : Average Loss = 0.84291\n",
      " Batch 25000 : Average Loss = 0.84215\n",
      " Batch 25200 : Average Loss = 0.84171\n",
      " Batch 25400 : Average Loss = 0.84105\n",
      " Batch 25600 : Average Loss = 0.84089\n",
      " Batch 25800 : Average Loss = 0.84032\n",
      " Batch 26000 : Average Loss = 0.8398\n",
      " Batch 26200 : Average Loss = 0.8396\n",
      " Batch 26400 : Average Loss = 0.83928\n",
      " Batch 26600 : Average Loss = 0.83903\n",
      " Batch 26800 : Average Loss = 0.83862\n",
      " Batch 27000 : Average Loss = 0.83824\n",
      " Batch 27200 : Average Loss = 0.8375\n",
      " Batch 27400 : Average Loss = 0.83675\n",
      " Batch 27600 : Average Loss = 0.83617\n",
      " Batch 27800 : Average Loss = 0.83555\n",
      " Batch 28000 : Average Loss = 0.83477\n",
      " Batch 28200 : Average Loss = 0.8345\n",
      " Batch 28400 : Average Loss = 0.83395\n",
      " Batch 28600 : Average Loss = 0.83358\n",
      " Batch 28800 : Average Loss = 0.83294\n",
      " Batch 29000 : Average Loss = 0.83238\n",
      " Batch 29200 : Average Loss = 0.83172\n",
      " Batch 29400 : Average Loss = 0.83132\n",
      " Batch 29600 : Average Loss = 0.83096\n",
      " Batch 29800 : Average Loss = 0.83048\n",
      " Batch 30000 : Average Loss = 0.83002\n",
      " Batch 30200 : Average Loss = 0.82951\n",
      " Batch 30400 : Average Loss = 0.82905\n",
      " Batch 30600 : Average Loss = 0.82838\n",
      " Batch 30800 : Average Loss = 0.82798\n",
      " Batch 31000 : Average Loss = 0.82757\n",
      " Batch 31200 : Average Loss = 0.82709\n",
      " Batch 31400 : Average Loss = 0.82677\n",
      " Batch 31600 : Average Loss = 0.82662\n",
      " Batch 31800 : Average Loss = 0.82621\n",
      " Batch 32000 : Average Loss = 0.82588\n",
      " Batch 32200 : Average Loss = 0.82549\n",
      " Batch 32400 : Average Loss = 0.8248\n",
      " Batch 32600 : Average Loss = 0.82425\n",
      " Batch 32800 : Average Loss = 0.82384\n",
      " Batch 33000 : Average Loss = 0.82331\n",
      " Batch 33200 : Average Loss = 0.82308\n",
      " Batch 33400 : Average Loss = 0.82274\n",
      " Batch 33600 : Average Loss = 0.8224\n",
      " Batch 33800 : Average Loss = 0.82195\n",
      " Batch 34000 : Average Loss = 0.82154\n",
      " Batch 34200 : Average Loss = 0.8212\n",
      " Batch 34400 : Average Loss = 0.82083\n",
      " Batch 34600 : Average Loss = 0.82043\n",
      " Batch 34800 : Average Loss = 0.81999\n",
      " Batch 35000 : Average Loss = 0.81959\n",
      " Batch 35200 : Average Loss = 0.81941\n",
      " Batch 35400 : Average Loss = 0.81897\n",
      " Batch 35600 : Average Loss = 0.81859\n",
      " Batch 35800 : Average Loss = 0.81812\n",
      " Batch 36000 : Average Loss = 0.81763\n",
      " Batch 36200 : Average Loss = 0.81723\n",
      " Batch 36400 : Average Loss = 0.81693\n",
      " Batch 36600 : Average Loss = 0.81665\n",
      " Batch 36800 : Average Loss = 0.8165\n",
      " Batch 37000 : Average Loss = 0.81621\n",
      " Batch 37200 : Average Loss = 0.81581\n",
      " Batch 37400 : Average Loss = 0.81533\n",
      " Batch 37600 : Average Loss = 0.81498\n",
      " Batch 37800 : Average Loss = 0.81479\n",
      " Batch 38000 : Average Loss = 0.81445\n",
      " Batch 38200 : Average Loss = 0.81415\n",
      " Batch 38400 : Average Loss = 0.8138\n",
      " Batch 38600 : Average Loss = 0.81344\n",
      " Batch 38800 : Average Loss = 0.81288\n",
      " Batch 39000 : Average Loss = 0.81235\n",
      " Batch 39200 : Average Loss = 0.81203\n",
      " Batch 39400 : Average Loss = 0.81177\n",
      " Batch 39600 : Average Loss = 0.81145\n",
      " Batch 39800 : Average Loss = 0.811\n",
      " Batch 40000 : Average Loss = 0.81063\n",
      " Batch 40200 : Average Loss = 0.81024\n",
      " Batch 40400 : Average Loss = 0.80994\n",
      " Batch 40600 : Average Loss = 0.80968\n",
      " Batch 40800 : Average Loss = 0.80943\n",
      " Batch 41000 : Average Loss = 0.8092\n",
      " Batch 41200 : Average Loss = 0.80896\n",
      " Batch 41400 : Average Loss = 0.8086\n",
      " Batch 41600 : Average Loss = 0.80815\n",
      " Batch 41800 : Average Loss = 0.80783\n",
      " Batch 42000 : Average Loss = 0.80744\n",
      " Batch 42200 : Average Loss = 0.80721\n",
      " Batch 42400 : Average Loss = 0.80684\n",
      " Batch 42600 : Average Loss = 0.80644\n",
      " Batch 42800 : Average Loss = 0.80599\n",
      " Batch 43000 : Average Loss = 0.80561\n",
      " Batch 43200 : Average Loss = 0.80518\n",
      " Batch 43400 : Average Loss = 0.80482\n",
      " Batch 43600 : Average Loss = 0.80452\n",
      " Batch 43800 : Average Loss = 0.80414\n",
      " Batch 44000 : Average Loss = 0.80372\n",
      " Batch 44200 : Average Loss = 0.80343\n",
      " Batch 44400 : Average Loss = 0.80327\n",
      " Batch 44600 : Average Loss = 0.80308\n",
      " Batch 44800 : Average Loss = 0.80281\n",
      " Batch 45000 : Average Loss = 0.8026\n",
      " Batch 45200 : Average Loss = 0.80221\n",
      " Batch 45400 : Average Loss = 0.80183\n",
      " Batch 45600 : Average Loss = 0.80159\n",
      " Batch 45800 : Average Loss = 0.80133\n",
      " Batch 46000 : Average Loss = 0.80099\n",
      " Batch 46200 : Average Loss = 0.80058\n",
      " Batch 46400 : Average Loss = 0.80038\n",
      " Batch 46600 : Average Loss = 0.80016\n",
      " Batch 46800 : Average Loss = 0.79993\n",
      " Batch 47000 : Average Loss = 0.79963\n",
      " Batch 47200 : Average Loss = 0.79909\n",
      " Batch 47400 : Average Loss = 0.79897\n",
      " Batch 47600 : Average Loss = 0.79866\n",
      " Batch 47800 : Average Loss = 0.7983\n",
      " Batch 48000 : Average Loss = 0.79788\n",
      " Batch 48200 : Average Loss = 0.79748\n",
      " Batch 48400 : Average Loss = 0.79727\n",
      " Batch 48600 : Average Loss = 0.79692\n",
      " Batch 48800 : Average Loss = 0.79669\n",
      " Batch 49000 : Average Loss = 0.7966\n",
      " Batch 49200 : Average Loss = 0.7964\n",
      " Batch 49400 : Average Loss = 0.79618\n",
      " Batch 49600 : Average Loss = 0.79578\n",
      " Batch 49800 : Average Loss = 0.79546\n",
      " Batch 50000 : Average Loss = 0.79519\n",
      " Batch 50200 : Average Loss = 0.795\n",
      " Batch 50400 : Average Loss = 0.79485\n",
      " Batch 50600 : Average Loss = 0.79458\n",
      " Batch 50800 : Average Loss = 0.79446\n",
      " Batch 51000 : Average Loss = 0.79426\n",
      " Batch 51200 : Average Loss = 0.79393\n",
      " Batch 51400 : Average Loss = 0.79388\n",
      " Batch 51600 : Average Loss = 0.79372\n",
      " Batch 51800 : Average Loss = 0.79332\n",
      " Batch 52000 : Average Loss = 0.79299\n",
      " Batch 52200 : Average Loss = 0.7927\n",
      " Batch 52400 : Average Loss = 0.79272\n",
      " Batch 52600 : Average Loss = 0.79252\n",
      " Batch 52800 : Average Loss = 0.79222\n",
      " Batch 53000 : Average Loss = 0.79192\n",
      " Batch 53200 : Average Loss = 0.79171\n",
      " Batch 53400 : Average Loss = 0.79157\n",
      " Batch 53600 : Average Loss = 0.79143\n",
      " Batch 53800 : Average Loss = 0.7912\n",
      " Batch 54000 : Average Loss = 0.79106\n",
      " Batch 54200 : Average Loss = 0.79092\n",
      " Batch 54400 : Average Loss = 0.79064\n",
      " Batch 54600 : Average Loss = 0.79041\n",
      " Batch 54800 : Average Loss = 0.79019\n",
      " Batch 55000 : Average Loss = 0.7899\n",
      " Batch 55200 : Average Loss = 0.78971\n",
      " Batch 55400 : Average Loss = 0.78939\n",
      " Batch 55600 : Average Loss = 0.78907\n",
      " Batch 55800 : Average Loss = 0.78894\n",
      " Batch 56000 : Average Loss = 0.78857\n",
      " Batch 56200 : Average Loss = 0.78846\n",
      " Batch 56400 : Average Loss = 0.78822\n",
      " Batch 56600 : Average Loss = 0.78784\n",
      " Batch 56800 : Average Loss = 0.7877\n",
      " Batch 57000 : Average Loss = 0.78747\n",
      " Batch 57200 : Average Loss = 0.78711\n",
      " Batch 57400 : Average Loss = 0.78689\n",
      " Batch 57600 : Average Loss = 0.78669\n",
      " Batch 57800 : Average Loss = 0.78647\n",
      " Batch 58000 : Average Loss = 0.78622\n",
      " Batch 58200 : Average Loss = 0.78604\n",
      " Batch 58400 : Average Loss = 0.78585\n",
      " Batch 58600 : Average Loss = 0.78554\n",
      " Batch 58800 : Average Loss = 0.78527\n",
      " Batch 59000 : Average Loss = 0.78502\n",
      " Batch 59200 : Average Loss = 0.78486\n",
      " Batch 59400 : Average Loss = 0.78463\n",
      " Batch 59600 : Average Loss = 0.78447\n",
      " Batch 59800 : Average Loss = 0.78425\n",
      " Batch 60000 : Average Loss = 0.78404\n",
      " Batch 60200 : Average Loss = 0.78389\n",
      " Batch 60400 : Average Loss = 0.7836\n",
      " Batch 60600 : Average Loss = 0.78354\n",
      " Batch 60800 : Average Loss = 0.78342\n",
      " Batch 61000 : Average Loss = 0.78318\n",
      " Batch 61200 : Average Loss = 0.78298\n",
      " Batch 61400 : Average Loss = 0.78267\n",
      " Batch 61600 : Average Loss = 0.78251\n",
      " Batch 61800 : Average Loss = 0.78211\n",
      " Batch 62000 : Average Loss = 0.78185\n",
      " Batch 62200 : Average Loss = 0.78167\n",
      " Batch 62400 : Average Loss = 0.78138\n",
      " Batch 62600 : Average Loss = 0.78116\n",
      " Batch 62800 : Average Loss = 0.78102\n",
      " Batch 63000 : Average Loss = 0.78079\n",
      " Batch 63200 : Average Loss = 0.78054\n",
      " Batch 63400 : Average Loss = 0.78038\n",
      " Batch 63600 : Average Loss = 0.78019\n",
      " Batch 63800 : Average Loss = 0.77994\n",
      " Batch 64000 : Average Loss = 0.77975\n",
      " Batch 64200 : Average Loss = 0.77962\n",
      " Batch 64400 : Average Loss = 0.77956\n",
      " Batch 64600 : Average Loss = 0.77917\n",
      " Batch 64800 : Average Loss = 0.779\n",
      " Batch 65000 : Average Loss = 0.77879\n",
      " Batch 65200 : Average Loss = 0.77877\n",
      " Batch 65400 : Average Loss = 0.77857\n",
      " Batch 65600 : Average Loss = 0.77852\n",
      " Batch 65800 : Average Loss = 0.77826\n",
      " Batch 66000 : Average Loss = 0.77809\n",
      " Batch 66200 : Average Loss = 0.77787\n",
      " Batch 66400 : Average Loss = 0.77768\n",
      " Batch 66600 : Average Loss = 0.77757\n",
      " Batch 66800 : Average Loss = 0.77738\n",
      " Batch 67000 : Average Loss = 0.77725\n",
      " Batch 67200 : Average Loss = 0.77701\n",
      " Batch 67400 : Average Loss = 0.77678\n",
      " Batch 67600 : Average Loss = 0.7765\n",
      " Batch 67800 : Average Loss = 0.77643\n",
      " Batch 68000 : Average Loss = 0.77622\n",
      " Batch 68200 : Average Loss = 0.77598\n",
      " Batch 68400 : Average Loss = 0.77573\n",
      " Batch 68600 : Average Loss = 0.77558\n",
      " Batch 0 : Average Loss = 0.49478\n",
      " Batch 200 : Average Loss = 0.66478\n",
      " Batch 400 : Average Loss = 0.69194\n",
      " Batch 600 : Average Loss = 0.70071\n",
      " Batch 800 : Average Loss = 0.69858\n",
      " Batch 1000 : Average Loss = 0.70204\n",
      " Batch 1200 : Average Loss = 0.69441\n",
      " Batch 1400 : Average Loss = 0.69285\n",
      " Batch 1600 : Average Loss = 0.69206\n",
      " Batch 1800 : Average Loss = 0.69566\n",
      " Batch 2000 : Average Loss = 0.69722\n",
      " Batch 2200 : Average Loss = 0.69827\n",
      " Batch 2400 : Average Loss = 0.69588\n",
      " Batch 2600 : Average Loss = 0.69721\n",
      " Batch 2800 : Average Loss = 0.69923\n",
      " Batch 3000 : Average Loss = 0.70065\n",
      " Batch 3200 : Average Loss = 0.70267\n",
      " Batch 3400 : Average Loss = 0.70527\n",
      " Batch 3600 : Average Loss = 0.70429\n",
      " Batch 3800 : Average Loss = 0.70471\n",
      " Batch 4000 : Average Loss = 0.70255\n",
      " Batch 4200 : Average Loss = 0.70163\n",
      " Batch 4400 : Average Loss = 0.70219\n",
      " Batch 4600 : Average Loss = 0.7024\n",
      " Batch 4800 : Average Loss = 0.7014\n",
      " Batch 5000 : Average Loss = 0.7019\n",
      " Batch 5200 : Average Loss = 0.70093\n",
      " Batch 5400 : Average Loss = 0.70078\n",
      " Batch 5600 : Average Loss = 0.70115\n",
      " Batch 5800 : Average Loss = 0.6996\n",
      " Batch 6000 : Average Loss = 0.69827\n",
      " Batch 6200 : Average Loss = 0.69719\n",
      " Batch 6400 : Average Loss = 0.69627\n",
      " Batch 6600 : Average Loss = 0.69552\n",
      " Batch 6800 : Average Loss = 0.69466\n",
      " Batch 7000 : Average Loss = 0.69491\n",
      " Batch 7200 : Average Loss = 0.6946\n",
      " Batch 7400 : Average Loss = 0.69435\n",
      " Batch 7600 : Average Loss = 0.69428\n",
      " Batch 7800 : Average Loss = 0.69411\n",
      " Batch 8000 : Average Loss = 0.69365\n",
      " Batch 8200 : Average Loss = 0.69388\n",
      " Batch 8400 : Average Loss = 0.69429\n",
      " Batch 8600 : Average Loss = 0.69445\n",
      " Batch 8800 : Average Loss = 0.69442\n",
      " Batch 9000 : Average Loss = 0.69407\n",
      " Batch 9200 : Average Loss = 0.69439\n",
      " Batch 9400 : Average Loss = 0.69379\n",
      " Batch 9600 : Average Loss = 0.69308\n",
      " Batch 9800 : Average Loss = 0.69347\n",
      " Batch 10000 : Average Loss = 0.69351\n",
      " Batch 10200 : Average Loss = 0.69338\n",
      " Batch 10400 : Average Loss = 0.69284\n",
      " Batch 10600 : Average Loss = 0.69212\n",
      " Batch 10800 : Average Loss = 0.6917\n",
      " Batch 11000 : Average Loss = 0.69145\n",
      " Batch 11200 : Average Loss = 0.69102\n",
      " Batch 11400 : Average Loss = 0.69072\n",
      " Batch 11600 : Average Loss = 0.69002\n",
      " Batch 11800 : Average Loss = 0.69007\n",
      " Batch 12000 : Average Loss = 0.68958\n",
      " Batch 12200 : Average Loss = 0.68975\n",
      " Batch 12400 : Average Loss = 0.69013\n",
      " Batch 12600 : Average Loss = 0.69004\n",
      " Batch 12800 : Average Loss = 0.68954\n",
      " Batch 13000 : Average Loss = 0.69012\n",
      " Batch 13200 : Average Loss = 0.6904\n",
      " Batch 13400 : Average Loss = 0.69036\n",
      " Batch 13600 : Average Loss = 0.69031\n",
      " Batch 13800 : Average Loss = 0.68996\n",
      " Batch 14000 : Average Loss = 0.68946\n",
      " Batch 14200 : Average Loss = 0.68987\n",
      " Batch 14400 : Average Loss = 0.68967\n",
      " Batch 14600 : Average Loss = 0.68943\n",
      " Batch 14800 : Average Loss = 0.68864\n",
      " Batch 15000 : Average Loss = 0.68879\n",
      " Batch 15200 : Average Loss = 0.68885\n",
      " Batch 15400 : Average Loss = 0.68877\n",
      " Batch 15600 : Average Loss = 0.68893\n",
      " Batch 15800 : Average Loss = 0.68901\n",
      " Batch 16000 : Average Loss = 0.68893\n",
      " Batch 16200 : Average Loss = 0.68902\n",
      " Batch 16400 : Average Loss = 0.68923\n",
      " Batch 16600 : Average Loss = 0.68908\n",
      " Batch 16800 : Average Loss = 0.689\n",
      " Batch 17000 : Average Loss = 0.6891\n",
      " Batch 17200 : Average Loss = 0.68897\n",
      " Batch 17400 : Average Loss = 0.68888\n",
      " Batch 17600 : Average Loss = 0.68866\n",
      " Batch 17800 : Average Loss = 0.68875\n",
      " Batch 18000 : Average Loss = 0.68859\n",
      " Batch 18200 : Average Loss = 0.68872\n",
      " Batch 18400 : Average Loss = 0.68913\n",
      " Batch 18600 : Average Loss = 0.68874\n",
      " Batch 18800 : Average Loss = 0.68828\n",
      " Batch 19000 : Average Loss = 0.68814\n",
      " Batch 19200 : Average Loss = 0.68793\n",
      " Batch 19400 : Average Loss = 0.68766\n",
      " Batch 19600 : Average Loss = 0.68746\n",
      " Batch 19800 : Average Loss = 0.68738\n",
      " Batch 20000 : Average Loss = 0.68748\n",
      " Batch 20200 : Average Loss = 0.68755\n",
      " Batch 20400 : Average Loss = 0.68791\n",
      " Batch 20600 : Average Loss = 0.68759\n",
      " Batch 20800 : Average Loss = 0.68728\n",
      " Batch 21000 : Average Loss = 0.68713\n",
      " Batch 21200 : Average Loss = 0.68714\n",
      " Batch 21400 : Average Loss = 0.68712\n",
      " Batch 21600 : Average Loss = 0.68723\n",
      " Batch 21800 : Average Loss = 0.68732\n",
      " Batch 22000 : Average Loss = 0.68759\n",
      " Batch 22200 : Average Loss = 0.6875\n",
      " Batch 22400 : Average Loss = 0.68767\n",
      " Batch 22600 : Average Loss = 0.68781\n",
      " Batch 22800 : Average Loss = 0.68773\n",
      " Batch 23000 : Average Loss = 0.68763\n",
      " Batch 23200 : Average Loss = 0.68725\n",
      " Batch 23400 : Average Loss = 0.68697\n",
      " Batch 23600 : Average Loss = 0.68693\n",
      " Batch 23800 : Average Loss = 0.68681\n",
      " Batch 24000 : Average Loss = 0.68669\n",
      " Batch 24200 : Average Loss = 0.6863\n",
      " Batch 24400 : Average Loss = 0.68628\n",
      " Batch 24600 : Average Loss = 0.68615\n",
      " Batch 24800 : Average Loss = 0.68617\n",
      " Batch 25000 : Average Loss = 0.68571\n",
      " Batch 25200 : Average Loss = 0.6857\n",
      " Batch 25400 : Average Loss = 0.68548\n",
      " Batch 25600 : Average Loss = 0.68581\n",
      " Batch 25800 : Average Loss = 0.68572\n",
      " Batch 26000 : Average Loss = 0.68564\n",
      " Batch 26200 : Average Loss = 0.68572\n",
      " Batch 26400 : Average Loss = 0.68575\n",
      " Batch 26600 : Average Loss = 0.68603\n",
      " Batch 26800 : Average Loss = 0.68622\n",
      " Batch 27000 : Average Loss = 0.68616\n",
      " Batch 27200 : Average Loss = 0.6859\n",
      " Batch 27400 : Average Loss = 0.68553\n",
      " Batch 27600 : Average Loss = 0.6854\n",
      " Batch 27800 : Average Loss = 0.68524\n",
      " Batch 28000 : Average Loss = 0.68499\n",
      " Batch 28200 : Average Loss = 0.68511\n",
      " Batch 28400 : Average Loss = 0.68495\n",
      " Batch 28600 : Average Loss = 0.68505\n",
      " Batch 28800 : Average Loss = 0.68488\n",
      " Batch 29000 : Average Loss = 0.68472\n",
      " Batch 29200 : Average Loss = 0.68456\n",
      " Batch 29400 : Average Loss = 0.68448\n",
      " Batch 29600 : Average Loss = 0.68447\n",
      " Batch 29800 : Average Loss = 0.6844\n",
      " Batch 30000 : Average Loss = 0.68449\n",
      " Batch 30200 : Average Loss = 0.68441\n",
      " Batch 30400 : Average Loss = 0.68432\n",
      " Batch 30600 : Average Loss = 0.68407\n",
      " Batch 30800 : Average Loss = 0.68394\n",
      " Batch 31000 : Average Loss = 0.68392\n",
      " Batch 31200 : Average Loss = 0.68383\n",
      " Batch 31400 : Average Loss = 0.68377\n",
      " Batch 31600 : Average Loss = 0.68397\n",
      " Batch 31800 : Average Loss = 0.68395\n",
      " Batch 32000 : Average Loss = 0.68393\n",
      " Batch 32200 : Average Loss = 0.68402\n",
      " Batch 32400 : Average Loss = 0.68367\n",
      " Batch 32600 : Average Loss = 0.68348\n",
      " Batch 32800 : Average Loss = 0.68335\n",
      " Batch 33000 : Average Loss = 0.6832\n",
      " Batch 33200 : Average Loss = 0.68336\n",
      " Batch 33400 : Average Loss = 0.68337\n",
      " Batch 33600 : Average Loss = 0.68349\n",
      " Batch 33800 : Average Loss = 0.68347\n",
      " Batch 34000 : Average Loss = 0.68335\n",
      " Batch 34200 : Average Loss = 0.68326\n",
      " Batch 34400 : Average Loss = 0.68317\n",
      " Batch 34600 : Average Loss = 0.68303\n",
      " Batch 34800 : Average Loss = 0.68286\n",
      " Batch 35000 : Average Loss = 0.68281\n",
      " Batch 35200 : Average Loss = 0.68294\n",
      " Batch 35400 : Average Loss = 0.68277\n",
      " Batch 35600 : Average Loss = 0.68276\n",
      " Batch 35800 : Average Loss = 0.68261\n",
      " Batch 36000 : Average Loss = 0.68248\n",
      " Batch 36200 : Average Loss = 0.68229\n",
      " Batch 36400 : Average Loss = 0.68222\n",
      " Batch 36600 : Average Loss = 0.68226\n",
      " Batch 36800 : Average Loss = 0.68247\n",
      " Batch 37000 : Average Loss = 0.68255\n",
      " Batch 37200 : Average Loss = 0.68254\n",
      " Batch 37400 : Average Loss = 0.68226\n",
      " Batch 37600 : Average Loss = 0.68225\n",
      " Batch 37800 : Average Loss = 0.68242\n",
      " Batch 38000 : Average Loss = 0.68235\n",
      " Batch 38200 : Average Loss = 0.68221\n",
      " Batch 38400 : Average Loss = 0.6822\n",
      " Batch 38600 : Average Loss = 0.68215\n",
      " Batch 38800 : Average Loss = 0.68201\n",
      " Batch 39000 : Average Loss = 0.68182\n",
      " Batch 39200 : Average Loss = 0.68191\n",
      " Batch 39400 : Average Loss = 0.68184\n",
      " Batch 39600 : Average Loss = 0.68189\n",
      " Batch 39800 : Average Loss = 0.68168\n",
      " Batch 40000 : Average Loss = 0.68159\n",
      " Batch 40200 : Average Loss = 0.68151\n",
      " Batch 40400 : Average Loss = 0.68152\n",
      " Batch 40600 : Average Loss = 0.6815\n",
      " Batch 40800 : Average Loss = 0.68154\n",
      " Batch 41000 : Average Loss = 0.68161\n",
      " Batch 41200 : Average Loss = 0.68159\n",
      " Batch 41400 : Average Loss = 0.68151\n",
      " Batch 41600 : Average Loss = 0.68131\n",
      " Batch 41800 : Average Loss = 0.68128\n",
      " Batch 42000 : Average Loss = 0.68107\n",
      " Batch 42200 : Average Loss = 0.68111\n",
      " Batch 42400 : Average Loss = 0.68096\n",
      " Batch 42600 : Average Loss = 0.68088\n",
      " Batch 42800 : Average Loss = 0.68066\n",
      " Batch 43000 : Average Loss = 0.68054\n",
      " Batch 43200 : Average Loss = 0.68037\n",
      " Batch 43400 : Average Loss = 0.68024\n",
      " Batch 43600 : Average Loss = 0.68013\n",
      " Batch 43800 : Average Loss = 0.68004\n",
      " Batch 44000 : Average Loss = 0.67982\n",
      " Batch 44200 : Average Loss = 0.67978\n",
      " Batch 44400 : Average Loss = 0.67988\n",
      " Batch 44600 : Average Loss = 0.68002\n",
      " Batch 44800 : Average Loss = 0.67994\n",
      " Batch 45000 : Average Loss = 0.67998\n",
      " Batch 45200 : Average Loss = 0.67981\n",
      " Batch 45400 : Average Loss = 0.67967\n",
      " Batch 45600 : Average Loss = 0.67965\n",
      " Batch 45800 : Average Loss = 0.67967\n",
      " Batch 46000 : Average Loss = 0.6796\n",
      " Batch 46200 : Average Loss = 0.67954\n",
      " Batch 46400 : Average Loss = 0.67956\n",
      " Batch 46600 : Average Loss = 0.67947\n",
      " Batch 46800 : Average Loss = 0.67945\n",
      " Batch 47000 : Average Loss = 0.67938\n",
      " Batch 47200 : Average Loss = 0.67907\n",
      " Batch 47400 : Average Loss = 0.67917\n",
      " Batch 47600 : Average Loss = 0.67906\n",
      " Batch 47800 : Average Loss = 0.679\n",
      " Batch 48000 : Average Loss = 0.67886\n",
      " Batch 48200 : Average Loss = 0.67861\n",
      " Batch 48400 : Average Loss = 0.67854\n",
      " Batch 48600 : Average Loss = 0.67848\n",
      " Batch 48800 : Average Loss = 0.67841\n",
      " Batch 49000 : Average Loss = 0.67856\n",
      " Batch 49200 : Average Loss = 0.67856\n",
      " Batch 49400 : Average Loss = 0.6786\n",
      " Batch 49600 : Average Loss = 0.67849\n",
      " Batch 49800 : Average Loss = 0.67834\n",
      " Batch 50000 : Average Loss = 0.67832\n",
      " Batch 50200 : Average Loss = 0.67827\n",
      " Batch 50400 : Average Loss = 0.67829\n",
      " Batch 50600 : Average Loss = 0.67827\n",
      " Batch 50800 : Average Loss = 0.67839\n",
      " Batch 51000 : Average Loss = 0.67844\n",
      " Batch 51200 : Average Loss = 0.67833\n",
      " Batch 51400 : Average Loss = 0.67853\n",
      " Batch 51600 : Average Loss = 0.67858\n",
      " Batch 51800 : Average Loss = 0.67832\n",
      " Batch 52000 : Average Loss = 0.67823\n",
      " Batch 52200 : Average Loss = 0.67811\n",
      " Batch 52400 : Average Loss = 0.6783\n",
      " Batch 52600 : Average Loss = 0.67832\n",
      " Batch 52800 : Average Loss = 0.67824\n",
      " Batch 53000 : Average Loss = 0.6781\n",
      " Batch 53200 : Average Loss = 0.67809\n",
      " Batch 53400 : Average Loss = 0.67816\n",
      " Batch 53600 : Average Loss = 0.67825\n",
      " Batch 53800 : Average Loss = 0.67821\n",
      " Batch 54000 : Average Loss = 0.67827\n",
      " Batch 54200 : Average Loss = 0.67831\n",
      " Batch 54400 : Average Loss = 0.67818\n",
      " Batch 54600 : Average Loss = 0.67819\n",
      " Batch 54800 : Average Loss = 0.6782\n",
      " Batch 55000 : Average Loss = 0.67806\n",
      " Batch 55200 : Average Loss = 0.67802\n",
      " Batch 55400 : Average Loss = 0.67786\n",
      " Batch 55600 : Average Loss = 0.67772\n",
      " Batch 55800 : Average Loss = 0.67775\n",
      " Batch 56000 : Average Loss = 0.67759\n",
      " Batch 56200 : Average Loss = 0.67758\n",
      " Batch 56400 : Average Loss = 0.6776\n",
      " Batch 56600 : Average Loss = 0.6774\n",
      " Batch 56800 : Average Loss = 0.67747\n",
      " Batch 57000 : Average Loss = 0.67745\n",
      " Batch 57200 : Average Loss = 0.67732\n",
      " Batch 57400 : Average Loss = 0.67727\n",
      " Batch 57600 : Average Loss = 0.67723\n",
      " Batch 57800 : Average Loss = 0.67721\n",
      " Batch 58000 : Average Loss = 0.6771\n",
      " Batch 58200 : Average Loss = 0.67711\n",
      " Batch 58400 : Average Loss = 0.6771\n",
      " Batch 58600 : Average Loss = 0.67697\n",
      " Batch 58800 : Average Loss = 0.67686\n",
      " Batch 59000 : Average Loss = 0.67684\n",
      " Batch 59200 : Average Loss = 0.67677\n",
      " Batch 59400 : Average Loss = 0.67665\n",
      " Batch 59600 : Average Loss = 0.67665\n",
      " Batch 59800 : Average Loss = 0.67656\n",
      " Batch 60000 : Average Loss = 0.67652\n",
      " Batch 60200 : Average Loss = 0.67651\n",
      " Batch 60400 : Average Loss = 0.67637\n",
      " Batch 60600 : Average Loss = 0.67643\n",
      " Batch 60800 : Average Loss = 0.67646\n",
      " Batch 61000 : Average Loss = 0.67639\n",
      " Batch 61200 : Average Loss = 0.67638\n",
      " Batch 61400 : Average Loss = 0.67625\n",
      " Batch 61600 : Average Loss = 0.67621\n",
      " Batch 61800 : Average Loss = 0.67602\n",
      " Batch 62000 : Average Loss = 0.67589\n",
      " Batch 62200 : Average Loss = 0.67584\n",
      " Batch 62400 : Average Loss = 0.6757\n",
      " Batch 62600 : Average Loss = 0.67567\n",
      " Batch 62800 : Average Loss = 0.67568\n",
      " Batch 63000 : Average Loss = 0.6756\n",
      " Batch 63200 : Average Loss = 0.6755\n",
      " Batch 63400 : Average Loss = 0.6755\n",
      " Batch 63600 : Average Loss = 0.67544\n",
      " Batch 63800 : Average Loss = 0.67536\n",
      " Batch 64000 : Average Loss = 0.67531\n",
      " Batch 64200 : Average Loss = 0.6753\n",
      " Batch 64400 : Average Loss = 0.67534\n",
      " Batch 64600 : Average Loss = 0.67515\n",
      " Batch 64800 : Average Loss = 0.67515\n",
      " Batch 65000 : Average Loss = 0.67505\n",
      " Batch 65200 : Average Loss = 0.67514\n",
      " Batch 65400 : Average Loss = 0.67509\n",
      " Batch 65600 : Average Loss = 0.67518\n",
      " Batch 65800 : Average Loss = 0.67511\n",
      " Batch 66000 : Average Loss = 0.67508\n",
      " Batch 66200 : Average Loss = 0.67502\n",
      " Batch 66400 : Average Loss = 0.67496\n",
      " Batch 66600 : Average Loss = 0.67492\n",
      " Batch 66800 : Average Loss = 0.67486\n",
      " Batch 67000 : Average Loss = 0.67491\n",
      " Batch 67200 : Average Loss = 0.67487\n",
      " Batch 67400 : Average Loss = 0.67479\n",
      " Batch 67600 : Average Loss = 0.67465\n",
      " Batch 67800 : Average Loss = 0.67475\n",
      " Batch 68000 : Average Loss = 0.67471\n",
      " Batch 68200 : Average Loss = 0.67456\n",
      " Batch 68400 : Average Loss = 0.67443\n",
      " Batch 68600 : Average Loss = 0.67442\n",
      " Batch 0 : Average Loss = 0.32789\n",
      " Batch 200 : Average Loss = 0.6311\n",
      " Batch 400 : Average Loss = 0.65359\n",
      " Batch 600 : Average Loss = 0.66001\n",
      " Batch 800 : Average Loss = 0.65709\n",
      " Batch 1000 : Average Loss = 0.66066\n",
      " Batch 1200 : Average Loss = 0.65299\n",
      " Batch 1400 : Average Loss = 0.65167\n",
      " Batch 1600 : Average Loss = 0.65061\n",
      " Batch 1800 : Average Loss = 0.65357\n",
      " Batch 2000 : Average Loss = 0.65392\n",
      " Batch 2200 : Average Loss = 0.65267\n",
      " Batch 2400 : Average Loss = 0.65109\n",
      " Batch 2600 : Average Loss = 0.65171\n",
      " Batch 2800 : Average Loss = 0.65401\n",
      " Batch 3000 : Average Loss = 0.65382\n",
      " Batch 3200 : Average Loss = 0.65537\n",
      " Batch 3400 : Average Loss = 0.65787\n",
      " Batch 3600 : Average Loss = 0.65651\n",
      " Batch 3800 : Average Loss = 0.6573\n",
      " Batch 4000 : Average Loss = 0.65479\n",
      " Batch 4200 : Average Loss = 0.65368\n",
      " Batch 4400 : Average Loss = 0.65379\n",
      " Batch 4600 : Average Loss = 0.65394\n",
      " Batch 4800 : Average Loss = 0.65275\n",
      " Batch 5000 : Average Loss = 0.65222\n",
      " Batch 5200 : Average Loss = 0.65182\n",
      " Batch 5400 : Average Loss = 0.65165\n",
      " Batch 5600 : Average Loss = 0.65183\n",
      " Batch 5800 : Average Loss = 0.64998\n",
      " Batch 6000 : Average Loss = 0.64882\n",
      " Batch 6200 : Average Loss = 0.6476\n",
      " Batch 6400 : Average Loss = 0.64676\n",
      " Batch 6600 : Average Loss = 0.64663\n",
      " Batch 6800 : Average Loss = 0.64575\n",
      " Batch 7000 : Average Loss = 0.64608\n",
      " Batch 7200 : Average Loss = 0.64564\n",
      " Batch 7400 : Average Loss = 0.64518\n",
      " Batch 7600 : Average Loss = 0.64536\n",
      " Batch 7800 : Average Loss = 0.64549\n",
      " Batch 8000 : Average Loss = 0.64553\n",
      " Batch 8200 : Average Loss = 0.64595\n",
      " Batch 8400 : Average Loss = 0.64662\n",
      " Batch 8600 : Average Loss = 0.64675\n",
      " Batch 8800 : Average Loss = 0.64682\n",
      " Batch 9000 : Average Loss = 0.64674\n",
      " Batch 9200 : Average Loss = 0.64699\n",
      " Batch 9400 : Average Loss = 0.64665\n",
      " Batch 9600 : Average Loss = 0.64595\n",
      " Batch 9800 : Average Loss = 0.6463\n",
      " Batch 10000 : Average Loss = 0.64642\n",
      " Batch 10200 : Average Loss = 0.64644\n",
      " Batch 10400 : Average Loss = 0.64625\n",
      " Batch 10600 : Average Loss = 0.64538\n",
      " Batch 10800 : Average Loss = 0.64494\n",
      " Batch 11000 : Average Loss = 0.64467\n",
      " Batch 11200 : Average Loss = 0.64416\n",
      " Batch 11400 : Average Loss = 0.64408\n",
      " Batch 11600 : Average Loss = 0.64369\n",
      " Batch 11800 : Average Loss = 0.64384\n",
      " Batch 12000 : Average Loss = 0.64351\n",
      " Batch 12200 : Average Loss = 0.64386\n",
      " Batch 12400 : Average Loss = 0.64417\n",
      " Batch 12600 : Average Loss = 0.64442\n",
      " Batch 12800 : Average Loss = 0.64404\n",
      " Batch 13000 : Average Loss = 0.64475\n",
      " Batch 13200 : Average Loss = 0.64495\n",
      " Batch 13400 : Average Loss = 0.6449\n",
      " Batch 13600 : Average Loss = 0.64494\n",
      " Batch 13800 : Average Loss = 0.6446\n",
      " Batch 14000 : Average Loss = 0.64413\n",
      " Batch 14200 : Average Loss = 0.64448\n",
      " Batch 14400 : Average Loss = 0.64456\n",
      " Batch 14600 : Average Loss = 0.64422\n",
      " Batch 14800 : Average Loss = 0.64371\n",
      " Batch 15000 : Average Loss = 0.64406\n",
      " Batch 15200 : Average Loss = 0.64417\n",
      " Batch 15400 : Average Loss = 0.64399\n",
      " Batch 15600 : Average Loss = 0.64416\n",
      " Batch 15800 : Average Loss = 0.64425\n",
      " Batch 16000 : Average Loss = 0.64414\n",
      " Batch 16200 : Average Loss = 0.64421\n",
      " Batch 16400 : Average Loss = 0.64437\n",
      " Batch 16600 : Average Loss = 0.6442\n",
      " Batch 16800 : Average Loss = 0.64395\n",
      " Batch 17000 : Average Loss = 0.64393\n",
      " Batch 17200 : Average Loss = 0.64389\n",
      " Batch 17400 : Average Loss = 0.6437\n",
      " Batch 17600 : Average Loss = 0.64368\n",
      " Batch 17800 : Average Loss = 0.64373\n",
      " Batch 18000 : Average Loss = 0.64354\n",
      " Batch 18200 : Average Loss = 0.64375\n",
      " Batch 18400 : Average Loss = 0.64419\n",
      " Batch 18600 : Average Loss = 0.64393\n",
      " Batch 18800 : Average Loss = 0.64341\n",
      " Batch 19000 : Average Loss = 0.64324\n",
      " Batch 19200 : Average Loss = 0.6431\n",
      " Batch 19400 : Average Loss = 0.64291\n",
      " Batch 19600 : Average Loss = 0.64269\n",
      " Batch 19800 : Average Loss = 0.64256\n",
      " Batch 20000 : Average Loss = 0.64293\n",
      " Batch 20200 : Average Loss = 0.6431\n",
      " Batch 20400 : Average Loss = 0.64338\n",
      " Batch 20600 : Average Loss = 0.64292\n",
      " Batch 20800 : Average Loss = 0.64272\n",
      " Batch 21000 : Average Loss = 0.64259\n",
      " Batch 21200 : Average Loss = 0.64261\n",
      " Batch 21400 : Average Loss = 0.64261\n",
      " Batch 21600 : Average Loss = 0.64255\n",
      " Batch 21800 : Average Loss = 0.64248\n",
      " Batch 22000 : Average Loss = 0.64273\n",
      " Batch 22200 : Average Loss = 0.64268\n",
      " Batch 22400 : Average Loss = 0.64285\n",
      " Batch 22600 : Average Loss = 0.64288\n",
      " Batch 22800 : Average Loss = 0.6428\n",
      " Batch 23000 : Average Loss = 0.64281\n",
      " Batch 23200 : Average Loss = 0.64239\n",
      " Batch 23400 : Average Loss = 0.64206\n",
      " Batch 23600 : Average Loss = 0.64197\n",
      " Batch 23800 : Average Loss = 0.64191\n",
      " Batch 24000 : Average Loss = 0.64158\n",
      " Batch 24200 : Average Loss = 0.64141\n",
      " Batch 24400 : Average Loss = 0.64136\n",
      " Batch 24600 : Average Loss = 0.64144\n",
      " Batch 24800 : Average Loss = 0.64149\n",
      " Batch 25000 : Average Loss = 0.64101\n",
      " Batch 25200 : Average Loss = 0.64104\n",
      " Batch 25400 : Average Loss = 0.64081\n",
      " Batch 25600 : Average Loss = 0.64123\n",
      " Batch 25800 : Average Loss = 0.64116\n",
      " Batch 26000 : Average Loss = 0.64128\n",
      " Batch 26200 : Average Loss = 0.6412\n",
      " Batch 26400 : Average Loss = 0.64141\n",
      " Batch 26600 : Average Loss = 0.64191\n",
      " Batch 26800 : Average Loss = 0.64201\n",
      " Batch 27000 : Average Loss = 0.64197\n",
      " Batch 27200 : Average Loss = 0.6418\n",
      " Batch 27400 : Average Loss = 0.64143\n",
      " Batch 27600 : Average Loss = 0.6414\n",
      " Batch 27800 : Average Loss = 0.64114\n",
      " Batch 28000 : Average Loss = 0.64095\n",
      " Batch 28200 : Average Loss = 0.64108\n",
      " Batch 28400 : Average Loss = 0.64096\n",
      " Batch 28600 : Average Loss = 0.64104\n",
      " Batch 28800 : Average Loss = 0.64082\n",
      " Batch 29000 : Average Loss = 0.64074\n",
      " Batch 29200 : Average Loss = 0.64062\n",
      " Batch 29400 : Average Loss = 0.64063\n",
      " Batch 29600 : Average Loss = 0.64063\n",
      " Batch 29800 : Average Loss = 0.64067\n",
      " Batch 30000 : Average Loss = 0.64085\n",
      " Batch 30200 : Average Loss = 0.64082\n",
      " Batch 30400 : Average Loss = 0.64064\n",
      " Batch 30600 : Average Loss = 0.6404\n",
      " Batch 30800 : Average Loss = 0.64029\n",
      " Batch 31000 : Average Loss = 0.64016\n",
      " Batch 31200 : Average Loss = 0.64016\n",
      " Batch 31400 : Average Loss = 0.64008\n",
      " Batch 31600 : Average Loss = 0.6403\n",
      " Batch 31800 : Average Loss = 0.64033\n",
      " Batch 32000 : Average Loss = 0.64044\n",
      " Batch 32200 : Average Loss = 0.64048\n",
      " Batch 32400 : Average Loss = 0.64026\n",
      " Batch 32600 : Average Loss = 0.64012\n",
      " Batch 32800 : Average Loss = 0.64006\n",
      " Batch 33000 : Average Loss = 0.63995\n",
      " Batch 33200 : Average Loss = 0.64007\n",
      " Batch 33400 : Average Loss = 0.64003\n",
      " Batch 33600 : Average Loss = 0.64025\n",
      " Batch 33800 : Average Loss = 0.64028\n",
      " Batch 34000 : Average Loss = 0.64022\n",
      " Batch 34200 : Average Loss = 0.64018\n",
      " Batch 34400 : Average Loss = 0.64012\n",
      " Batch 34600 : Average Loss = 0.64014\n",
      " Batch 34800 : Average Loss = 0.64002\n",
      " Batch 35000 : Average Loss = 0.6399\n",
      " Batch 35200 : Average Loss = 0.64002\n",
      " Batch 35400 : Average Loss = 0.63989\n",
      " Batch 35600 : Average Loss = 0.63994\n",
      " Batch 35800 : Average Loss = 0.63977\n",
      " Batch 36000 : Average Loss = 0.63962\n",
      " Batch 36200 : Average Loss = 0.63948\n",
      " Batch 36400 : Average Loss = 0.63936\n",
      " Batch 36600 : Average Loss = 0.63955\n",
      " Batch 36800 : Average Loss = 0.63977\n",
      " Batch 37000 : Average Loss = 0.63984\n",
      " Batch 37200 : Average Loss = 0.6398\n",
      " Batch 37400 : Average Loss = 0.63952\n",
      " Batch 37600 : Average Loss = 0.63957\n",
      " Batch 37800 : Average Loss = 0.63983\n",
      " Batch 38000 : Average Loss = 0.63982\n",
      " Batch 38200 : Average Loss = 0.63975\n",
      " Batch 38400 : Average Loss = 0.63977\n",
      " Batch 38600 : Average Loss = 0.63977\n",
      " Batch 38800 : Average Loss = 0.63953\n",
      " Batch 39000 : Average Loss = 0.6393\n",
      " Batch 39200 : Average Loss = 0.63946\n",
      " Batch 39400 : Average Loss = 0.6394\n",
      " Batch 39600 : Average Loss = 0.63946\n",
      " Batch 39800 : Average Loss = 0.63926\n",
      " Batch 40000 : Average Loss = 0.63912\n",
      " Batch 40200 : Average Loss = 0.639\n",
      " Batch 40400 : Average Loss = 0.639\n",
      " Batch 40600 : Average Loss = 0.63899\n",
      " Batch 40800 : Average Loss = 0.63907\n",
      " Batch 41000 : Average Loss = 0.63919\n",
      " Batch 41200 : Average Loss = 0.63919\n",
      " Batch 41400 : Average Loss = 0.63915\n",
      " Batch 41600 : Average Loss = 0.63889\n",
      " Batch 41800 : Average Loss = 0.63882\n",
      " Batch 42000 : Average Loss = 0.6387\n",
      " Batch 42200 : Average Loss = 0.63879\n",
      " Batch 42400 : Average Loss = 0.63863\n",
      " Batch 42600 : Average Loss = 0.63859\n",
      " Batch 42800 : Average Loss = 0.63844\n",
      " Batch 43000 : Average Loss = 0.63837\n",
      " Batch 43200 : Average Loss = 0.63822\n",
      " Batch 43400 : Average Loss = 0.63817\n",
      " Batch 43600 : Average Loss = 0.63816\n",
      " Batch 43800 : Average Loss = 0.63809\n",
      " Batch 44000 : Average Loss = 0.63797\n",
      " Batch 44200 : Average Loss = 0.63801\n",
      " Batch 44400 : Average Loss = 0.63822\n",
      " Batch 44600 : Average Loss = 0.63841\n",
      " Batch 44800 : Average Loss = 0.63843\n",
      " Batch 45000 : Average Loss = 0.63845\n",
      " Batch 45200 : Average Loss = 0.63833\n",
      " Batch 45400 : Average Loss = 0.63811\n",
      " Batch 45600 : Average Loss = 0.63817\n",
      " Batch 45800 : Average Loss = 0.63826\n",
      " Batch 46000 : Average Loss = 0.6382\n",
      " Batch 46200 : Average Loss = 0.63813\n",
      " Batch 46400 : Average Loss = 0.63817\n",
      " Batch 46600 : Average Loss = 0.63813\n",
      " Batch 46800 : Average Loss = 0.63819\n",
      " Batch 47000 : Average Loss = 0.63812\n",
      " Batch 47200 : Average Loss = 0.63785\n",
      " Batch 47400 : Average Loss = 0.63793\n",
      " Batch 47600 : Average Loss = 0.63789\n",
      " Batch 47800 : Average Loss = 0.63788\n",
      " Batch 48000 : Average Loss = 0.63773\n",
      " Batch 48200 : Average Loss = 0.63749\n",
      " Batch 48400 : Average Loss = 0.63754\n",
      " Batch 48600 : Average Loss = 0.6375\n",
      " Batch 48800 : Average Loss = 0.63748\n",
      " Batch 49000 : Average Loss = 0.63763\n",
      " Batch 49200 : Average Loss = 0.63761\n",
      " Batch 49400 : Average Loss = 0.63764\n",
      " Batch 49600 : Average Loss = 0.63748\n",
      " Batch 49800 : Average Loss = 0.63743\n",
      " Batch 50000 : Average Loss = 0.63737\n",
      " Batch 50200 : Average Loss = 0.63736\n",
      " Batch 50400 : Average Loss = 0.63743\n",
      " Batch 50600 : Average Loss = 0.63736\n",
      " Batch 50800 : Average Loss = 0.63748\n",
      " Batch 51000 : Average Loss = 0.63751\n",
      " Batch 51200 : Average Loss = 0.63746\n",
      " Batch 51400 : Average Loss = 0.63774\n",
      " Batch 51600 : Average Loss = 0.63784\n",
      " Batch 51800 : Average Loss = 0.63761\n",
      " Batch 52000 : Average Loss = 0.6375\n",
      " Batch 52200 : Average Loss = 0.63739\n",
      " Batch 52400 : Average Loss = 0.6376\n",
      " Batch 52600 : Average Loss = 0.63766\n",
      " Batch 52800 : Average Loss = 0.63761\n",
      " Batch 53000 : Average Loss = 0.63759\n",
      " Batch 53200 : Average Loss = 0.63759\n",
      " Batch 53400 : Average Loss = 0.63765\n",
      " Batch 53600 : Average Loss = 0.63775\n",
      " Batch 53800 : Average Loss = 0.63771\n",
      " Batch 54000 : Average Loss = 0.63775\n",
      " Batch 54200 : Average Loss = 0.63781\n",
      " Batch 54400 : Average Loss = 0.63773\n",
      " Batch 54600 : Average Loss = 0.63776\n",
      " Batch 54800 : Average Loss = 0.63782\n",
      " Batch 55000 : Average Loss = 0.6377\n",
      " Batch 55200 : Average Loss = 0.63771\n",
      " Batch 55400 : Average Loss = 0.63759\n",
      " Batch 55600 : Average Loss = 0.63745\n",
      " Batch 55800 : Average Loss = 0.6375\n",
      " Batch 56000 : Average Loss = 0.63739\n",
      " Batch 56200 : Average Loss = 0.63748\n",
      " Batch 56400 : Average Loss = 0.63757\n",
      " Batch 56600 : Average Loss = 0.6374\n",
      " Batch 56800 : Average Loss = 0.63752\n",
      " Batch 57000 : Average Loss = 0.63748\n",
      " Batch 57200 : Average Loss = 0.63735\n",
      " Batch 57400 : Average Loss = 0.63732\n",
      " Batch 57600 : Average Loss = 0.63731\n",
      " Batch 57800 : Average Loss = 0.63731\n",
      " Batch 58000 : Average Loss = 0.63723\n",
      " Batch 58200 : Average Loss = 0.63721\n",
      " Batch 58400 : Average Loss = 0.63723\n",
      " Batch 58600 : Average Loss = 0.63712\n",
      " Batch 58800 : Average Loss = 0.637\n",
      " Batch 59000 : Average Loss = 0.63698\n",
      " Batch 59200 : Average Loss = 0.63701\n",
      " Batch 59400 : Average Loss = 0.6369\n",
      " Batch 59600 : Average Loss = 0.63692\n",
      " Batch 59800 : Average Loss = 0.63682\n",
      " Batch 60000 : Average Loss = 0.63681\n",
      " Batch 60200 : Average Loss = 0.63688\n",
      " Batch 60400 : Average Loss = 0.63676\n",
      " Batch 60600 : Average Loss = 0.63682\n",
      " Batch 60800 : Average Loss = 0.63686\n",
      " Batch 61000 : Average Loss = 0.63684\n",
      " Batch 61200 : Average Loss = 0.63684\n",
      " Batch 61400 : Average Loss = 0.63675\n",
      " Batch 61600 : Average Loss = 0.63682\n",
      " Batch 61800 : Average Loss = 0.6366\n",
      " Batch 62000 : Average Loss = 0.63651\n",
      " Batch 62200 : Average Loss = 0.63646\n",
      " Batch 62400 : Average Loss = 0.63639\n",
      " Batch 62600 : Average Loss = 0.63639\n",
      " Batch 62800 : Average Loss = 0.63643\n",
      " Batch 63000 : Average Loss = 0.63639\n",
      " Batch 63200 : Average Loss = 0.63629\n",
      " Batch 63400 : Average Loss = 0.63631\n",
      " Batch 63600 : Average Loss = 0.63626\n",
      " Batch 63800 : Average Loss = 0.63617\n",
      " Batch 64000 : Average Loss = 0.63615\n",
      " Batch 64200 : Average Loss = 0.63621\n",
      " Batch 64400 : Average Loss = 0.6363\n",
      " Batch 64600 : Average Loss = 0.63612\n",
      " Batch 64800 : Average Loss = 0.63622\n",
      " Batch 65000 : Average Loss = 0.63615\n",
      " Batch 65200 : Average Loss = 0.63625\n",
      " Batch 65400 : Average Loss = 0.63626\n",
      " Batch 65600 : Average Loss = 0.63636\n",
      " Batch 65800 : Average Loss = 0.63629\n",
      " Batch 66000 : Average Loss = 0.6363\n",
      " Batch 66200 : Average Loss = 0.63627\n",
      " Batch 66400 : Average Loss = 0.63618\n",
      " Batch 66600 : Average Loss = 0.63612\n",
      " Batch 66800 : Average Loss = 0.63609\n",
      " Batch 67000 : Average Loss = 0.63612\n",
      " Batch 67200 : Average Loss = 0.63603\n",
      " Batch 67400 : Average Loss = 0.636\n",
      " Batch 67600 : Average Loss = 0.63584\n",
      " Batch 67800 : Average Loss = 0.63598\n",
      " Batch 68000 : Average Loss = 0.6359\n",
      " Batch 68200 : Average Loss = 0.63582\n",
      " Batch 68400 : Average Loss = 0.63567\n",
      " Batch 68600 : Average Loss = 0.63564\n"
     ]
    }
   ],
   "source": [
    "model = SNLIModel(train_dataset.word2idx, train_dataset.labels).to(device)\n",
    "loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train model\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        prems = batch[0]\n",
    "        hyps = batch[1]\n",
    "        rels = torch.Tensor(batch[2]).long().to(device)\n",
    "\n",
    "        output = model(prems, hyps)\n",
    "        \n",
    "        loss = loss_function(output, rels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i%200==0:\n",
    "            print(f' Batch {i} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')\n",
    "            \n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'inference_VG.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#so that when we load it back in, we can have access to the same word2idx etc.\n",
    "with open(\"train_dataset.pickle\",\"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNLIModel(\n",
       "  (embeddings): Embedding(56258, 64, padding_idx=1)\n",
       "  (LSTM): LSTM(64, 128, bidirectional=True)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"train_dataset.pickle\", 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "model = torch.load('inference_VG.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, name, divider):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct = 0\n",
    "        counter = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            test_output = model(batch[0], batch[1])\n",
    "            # test_output = model(batch[1])\n",
    "            test_output = torch.argmax(test_output, dim=1)\n",
    "            targets = torch.tensor(batch[2], device=device)\n",
    "            correct += torch.sum(test_output == targets)\n",
    "            counter += len(test_output)\n",
    "\n",
    "            test_accu = correct/counter\n",
    "        \n",
    "            if i%divider==0:\n",
    "                print(f' Batch {i} : Average Test Accuracy on the {name} = {round(float(test_accu), 5)}')\n",
    "\n",
    "    print(f'Total Test Accuracy on the {name} = {round(float(test_accu), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the SNLI test set = 0.375\n",
      " Batch 200 : Average Test Accuracy on the SNLI test set = 0.7245\n",
      " Batch 400 : Average Test Accuracy on the SNLI test set = 0.73036\n",
      " Batch 600 : Average Test Accuracy on the SNLI test set = 0.72983\n",
      " Batch 800 : Average Test Accuracy on the SNLI test set = 0.73002\n",
      " Batch 1000 : Average Test Accuracy on the SNLI test set = 0.7294\n",
      " Batch 1200 : Average Test Accuracy on the SNLI test set = 0.73012\n",
      "Total Test Accuracy on the SNLI test set = 0.7313\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader, 'SNLI test set', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the FRACAS test set = 0.75\n",
      " Batch 2 : Average Test Accuracy on the FRACAS test set = 0.54167\n",
      " Batch 4 : Average Test Accuracy on the FRACAS test set = 0.55\n",
      " Batch 6 : Average Test Accuracy on the FRACAS test set = 0.57143\n",
      " Batch 8 : Average Test Accuracy on the FRACAS test set = 0.51389\n",
      " Batch 10 : Average Test Accuracy on the FRACAS test set = 0.49398\n",
      "Total Test Accuracy on the FRACAS test set = 0.49398\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_fracas_loader, 'FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the syntactic FRACAS test set = 0.625\n",
      " Batch 2 : Average Test Accuracy on the syntactic FRACAS test set = 0.58333\n",
      " Batch 4 : Average Test Accuracy on the syntactic FRACAS test set = 0.525\n",
      "Total Test Accuracy on the syntactic FRACAS test set = 0.525\n"
     ]
    }
   ],
   "source": [
    "test_model(model, syn_test_fracas_loader, 'syntactic FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the semantic FRACAS test set = 0.5\n",
      " Batch 2 : Average Test Accuracy on the semantic FRACAS test set = 0.54167\n",
      " Batch 4 : Average Test Accuracy on the semantic FRACAS test set = 0.425\n",
      "Total Test Accuracy on the semantic FRACAS test set = 0.4186\n"
     ]
    }
   ],
   "source": [
    "test_model(model, sem_test_fracas_loader, 'semantic FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model on FRACAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Loss = 0.86952\n",
      " Batch 2 : Average Loss = 0.89718\n",
      " Batch 4 : Average Loss = 0.78495\n",
      " Batch 6 : Average Loss = 0.84265\n",
      " Batch 8 : Average Loss = 0.85689\n",
      " Batch 10 : Average Loss = 0.8796\n",
      " Batch 0 : Average Loss = 1.05906\n",
      " Batch 2 : Average Loss = 0.99514\n",
      " Batch 4 : Average Loss = 0.80413\n",
      " Batch 6 : Average Loss = 0.86354\n",
      " Batch 8 : Average Loss = 0.885\n",
      " Batch 10 : Average Loss = 0.93186\n",
      " Batch 0 : Average Loss = 1.13973\n",
      " Batch 2 : Average Loss = 0.99478\n",
      " Batch 4 : Average Loss = 0.83041\n",
      " Batch 6 : Average Loss = 0.9379\n",
      " Batch 8 : Average Loss = 0.92207\n",
      " Batch 10 : Average Loss = 0.89088\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train model\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dev_fracas_loader):\n",
    "        prems = batch[0]\n",
    "        hyps = batch[1]\n",
    "        rels = torch.Tensor(batch[2]).long().to(device)\n",
    "\n",
    "        output = model(prems, hyps)\n",
    "        \n",
    "        loss = loss_function(output, rels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i%2==0:\n",
    "            print(f' Batch {i} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')\n",
    "            \n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'inference_VG_finetuned.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNLIModel(\n",
       "  (embeddings): Embedding(56258, 64, padding_idx=1)\n",
       "  (LSTM): LSTM(64, 128, bidirectional=True)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = torch.load('inference_VG_finetuned.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the SNLI test set = 0.375\n",
      " Batch 200 : Average Test Accuracy on the SNLI test set = 0.72264\n",
      " Batch 400 : Average Test Accuracy on the SNLI test set = 0.73005\n",
      " Batch 600 : Average Test Accuracy on the SNLI test set = 0.72421\n",
      " Batch 800 : Average Test Accuracy on the SNLI test set = 0.72425\n",
      " Batch 1000 : Average Test Accuracy on the SNLI test set = 0.72203\n",
      " Batch 1200 : Average Test Accuracy on the SNLI test set = 0.72304\n",
      "Total Test Accuracy on the SNLI test set = 0.7239\n"
     ]
    }
   ],
   "source": [
    "test_model(finetuned_model, test_loader, 'SNLI test set', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the FRACAS test set = 0.75\n",
      " Batch 2 : Average Test Accuracy on the FRACAS test set = 0.66667\n",
      " Batch 4 : Average Test Accuracy on the FRACAS test set = 0.6\n",
      " Batch 6 : Average Test Accuracy on the FRACAS test set = 0.60714\n",
      " Batch 8 : Average Test Accuracy on the FRACAS test set = 0.54167\n",
      " Batch 10 : Average Test Accuracy on the FRACAS test set = 0.53012\n",
      "Total Test Accuracy on the FRACAS test set = 0.53012\n"
     ]
    }
   ],
   "source": [
    "test_model(finetuned_model, test_fracas_loader, 'FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the syntactic FRACAS test set = 0.75\n",
      " Batch 2 : Average Test Accuracy on the syntactic FRACAS test set = 0.66667\n",
      " Batch 4 : Average Test Accuracy on the syntactic FRACAS test set = 0.575\n",
      "Total Test Accuracy on the syntactic FRACAS test set = 0.575\n"
     ]
    }
   ],
   "source": [
    "test_model(finetuned_model, syn_test_fracas_loader, 'syntactic FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy on the semantic FRACAS test set = 0.25\n",
      " Batch 2 : Average Test Accuracy on the semantic FRACAS test set = 0.5\n",
      " Batch 4 : Average Test Accuracy on the semantic FRACAS test set = 0.425\n",
      "Total Test Accuracy on the semantic FRACAS test set = 0.44186\n"
     ]
    }
   ],
   "source": [
    "test_model(finetuned_model, sem_test_fracas_loader, 'semantic FRACAS test set', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary, evaluation, and discussion\n",
    "#### Summary\n",
    "The goal of this VG project was to verify and compare how well, in terms of accuracy, a BiLSTM model trained on the SNLI dataset performs on an SNLI test set and on the FRACAS test set (optionally split into semantic- and syntactic-based examples). The second step was to fine-tune the same model on a section of the FRACAS dataset, and perform the same evaluation, so that the results could be compared.  \n",
    "\n",
    "The first step in this assignment was transforming the data from the FRACAS dataset into a format that would be similar to what I had in the SNLI dataset. This meant having to decide how to translate the relation labels as well as what to do with the examples that had more than one premise. All of that has been described in section 1 of the assignment.  \n",
    "The second step was to create the model, for which I used the same architecture as in Assignment 5, with the exception of not using the custom max pooling function, as that one was really slow in comparison with the PyTorch one. I did make sure that the PyTorch one was giving me the output I wanted.  \n",
    "The final step was training the model on SNLI and evaluating it, and then fine-tuning it on FRACAS and re-evaluating again. Both of the models have been saved (along with the train_dataset, whose word2idx is used to encode the words in loaders). The results of the evaluation can be seen below:  \n",
    "\n",
    "| MODEL | SNLI | FRACAS (all) | FRACAS (syntactic) | FRACAS (semantic) |\n",
    "|-----|-----|-------|-----------|---------|\n",
    "| base | 0.7313 | 0.49398 | 0.525 | 0.4186 |\n",
    "| finetuned | 0.7239 | 0.53012 | 0.575 | 0.44186 |  \n",
    "\n",
    "#### Evaluation \n",
    "It can be noticed that the base model (just trained on SNLI) performed better on the SNLI test set, but worse on all of the FRACAS ones. The fine-tuned model (first trained on SNLI, then trained more on FRACAS) performed worse on the SNLI test set, but better on the FRACAS ones. The difference in the peformance on SNLI is not major, and the one on FRACAS is bigger. However, these results are not always consistent when re-running the notebook, and generally oscillate around these values. It seems like fine-tuning on FRACAS has a rather marginal effect.  \n",
    "\n",
    "#### Discussion\n",
    "However, it is worth noting that the FRACAS dataset is significantly smaller than the SNLI train set. SNLI train has 550k examples, while our FRACAS dev - barely more than 80 (that is not 80k examples, that is 80 examples). This massive disproportion likely makes it so that the FRACAS examples do not contribute nearly as much as the SNLI ones, and perhaps balancing them out better would have been a better idea - this is difficult to do though, given that FRACAS itself is a small dataset. It is interesting to see though that it does have an effect, and on some runs of the notebook at least they do suggest that training a BiLSTM model on multiple kinds of inference datasets could improve its performance in some tasks without sacrificing too much of the performance elsewhere.  \n",
    "\n",
    "Overall, given that we had 4 classes (the '-', 'contradiction', 'entailment', 'neutral' from SNLI), and given that '-' is a very rare one, the \"stupid\" baseline for this model would have been 33%. It performs better than that in all the categories, so I am fairly confident when I say it has learned something that lets it perform NLI - but whether it is something about the meaning or the structure of the data itself, I cannot say for certain.  \n",
    "\n",
    "It is interesting to note that unlike what I thought would happen, the semantics-based examples from FRACAS are the ones the model is performing the worst on. This may be a clue to it actually not learning to classify the examples based on meaning but on sentence structure. I expected it to perform better on semantics-based examples, and not the syntactic ones. It might also be due to the semantics examples containing a lot of specialized vocabulary or just vocabulary that does not exist in the model's lexicon (so a lot of UNKNOWN tokens).\n",
    "\n",
    "#### Possible future work\n",
    "It would be really interesting to see the results with a more balanced SNLI-to-FRACAS ratio, seeing what the tradeoff is between gaining proficiency in FRACAS tasks and losing it in SNLI ones. It could also be interesting to test the model on other NLI datasets, or just the embeddings on other tasks. Overall I would also be interested in how NLI-based tasks could be used in training bigger, BERT-like models as one of the pre-training tasks, and what kind of meaning they would contribute - as clearly it also depends on the NLI dataset used.\n",
    "\n",
    "#### Final remarks\n",
    "When annotating the notebook for which parts I have done myself and which I have adapted, I felt like I have not done all that much, but I can promise that I did put quite some work into this little project, and hopefully it is sufficient. We have re-used similar elements in all of our neural network labs (e.g. the custom datasets and dataloaders which were initially done by Isac Boström from my group), and this project (and lab 5) is no exception in some of its sections. I found it particularly fun to have to think how to retrieve the FRACAS data and process it so that it would work with the model and the evaluation, and therein lies the most work I put into it. I also had to fix some minor issues with the model/training loop since relative to the draft submission of lab 5 there were some indexing errors when training on the training set. And, naturally, the discussion of the results is entirely mine, as is anything in the code that differs from lab 5 (unless stated otherwise). I hope that this is sufficient for the VG project, as discussed in e-mails before, and in case it is not, or you have any questions, issues, or comments, do not hesitate to contact me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
