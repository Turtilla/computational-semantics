{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik\n",
    "\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "---\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for phrases affect both the model and the learned information about similarities.  \n",
    "\n",
    "Note that this lab uses a code from `dist_erk.py`, which contains functions that highly resemble those shown during the lecture. In the end, you can use either of the functions (from the lecture / from the file) to solve the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from that code.\n",
    "from dist_erk import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "**Important**: All necessary files which are used in this notebook are available on mlt-gpu, check `/srv/data/computational-semantics-assignment-02`.\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus (`wikipedia.txt`. This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).\n",
    "\n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below.  \n",
    "<!-- <It may already exist in `/opt/mlt/courses/cl2015/a5`.> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia.txt',\n",
       " 'ktw_wikipediaktw.npy',\n",
       " 'ppmi_wikipediaktw.npy',\n",
       " 'raw_wikipediaktw.npy',\n",
       " 'sub_glov.npy',\n",
       " 'svd50_wikipedia10k.npy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dir = '/srv/data/computational-semantics-assignment-02'\n",
    "os.listdir(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls. Functions are imported from `dist_erk.py` earlier, and they largely resemble functions shown during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia.txt\n",
      "create count matrices\n",
      "reading file wikipedia.txt\n",
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# which words to use as targets and context words?\n",
    "# we need to count the words and keep only the N most frequent ones\n",
    "# which function would you use here with which variable?\n",
    "ktw = do_word_count('/srv/data/computational-semantics-assignment-02/', numdims)\n",
    "\n",
    "wi = make_word_index(ktw)\n",
    "# words_in_order = ... # sorted words\n",
    "\n",
    "# create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# which functions with which arguments would you use here?\n",
    "print('create count matrices')\n",
    "space_1k = make_space('/srv/data/computational-semantics-assignment-02/', wi, numdims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(space_1k, numdims, svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions\n",
      "space_1k 1000 1000\n",
      "ppmispace_1k 1000 1000\n",
      "svdspace_1k 1000 5\n"
     ]
    }
   ],
   "source": [
    "print('dimensions')\n",
    "print('space_1k', len(space_1k.keys()), len(space_1k[next(iter(space_1k))]))\n",
    "print('ppmispace_1k', len(ppmispace_1k.keys()), len(ppmispace_1k[next(iter(ppmispace_1k))]))\n",
    "print('svdspace_1k', len(svdspace_1k.keys()), len(svdspace_1k[next(iter(svdspace_1k))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105  567  962  631  443  185  311  189  131   28   93  169\n",
      "   81  125  151  408  194   90   79   29  217  184   62   15   31   70\n",
      "   10    1   41   21    1   31   37    1   30    5   25    7    3   20\n",
      "   11    1   32   36    2    5   66    4    0   46    8   18   28    0\n",
      "   20    7    8   16   10   40    0  175   10    2    7   19    1  174\n",
      "   11    3    1    6    0    0    0   10    9   11    7   24    4    4\n",
      "   14   23   58    7    0   10    2    3   10    6   18    6   13    3\n",
      "   22    0    3    5    3    7   14    3   40   20   19   15    6    8\n",
      "   24    4    5    1   19    0    3    1    0   14    0   14   53    7\n",
      "    7   11    6    5    5    4   12    6   53    1    1  433    4    0\n",
      "    5    7    7   12    1    1    3    4   17    8   16    1    2   31\n",
      "    1   12   14    1   44    6   14    9   38    7    2    6    8    1\n",
      "   10    6   10    1    9    7    9    4    3   10    0   11    3    2\n",
      "    0    2   11   37    2    0    2    1    5    9   10   16   88    6\n",
      "    0   21    1    1    0    2   47    3   27    7    0    2   13    1\n",
      "    2    0    5   31    0    1    0    3   10    0    1    0    3    3\n",
      "   17    1    1   16    3    7    4    7   15    4    0    0    2    5\n",
      "    0    2    0    5    0    9    0    0    8    0   10    0    0    0\n",
      "    2    0    1    3    1    3   15    1    9    0   19   14    0    0\n",
      "    3    2   18    3    1    3    2   19    5    2    4    1   10    6\n",
      "    0    3    3    6    4    2   25    4    6    3    1   25   10   15\n",
      "    3   10   15    1   10    1    8    1   13    1    2    9    9    1\n",
      "    4    1   25    0    4    6    5    5   36    0    2    2    2    0\n",
      "    0    2    3    3    0    1    4    6    5    0   50    2    5    2\n",
      "   14    6    2    2    4    1    9    4    5    3    1    0   12    3\n",
      "    3    2    2    0    0    1    4    7   12    5    0    2    1    2\n",
      "    3    4    7    3    5    0   29    7    1    1    0    3    3    3\n",
      "   10    0   14    2    0    2    4    6    0    5    0    0    1    1\n",
      "    4    1    1    0    0    0    0    3   20    0    0    2    1    5\n",
      "    3    8    3    5    1    2   66    1    2   19    2    1    3    3\n",
      "   21    5    4    2    2    0    4    3    5    0    7    1    6    1\n",
      "    3    3    1    0    3    0    2    0   89    2    3    1    1   14\n",
      "    0    2    1    9    2    3    2    4    2    0   25    0    0   23\n",
      "    0    6    2    1    3    0    2    5    0    4    4    3    0    4\n",
      "   58    3    1    6    2    4    3    3   11    1    1    1   10    0\n",
      "    7    3    1    6    1   18    1    0    4    2    0    8    5    2\n",
      "    0    0    0    0    5    1    2    1    1    3    1    2    1    1\n",
      "    0    6    1    4    1    3   20    1    0    5    2    5    2    1\n",
      "    0    0    0    2    6    1    1    0    1    1    1    0    0    3\n",
      "    3    0    0    6    6   74    3    0   13    5    2    2    1    5\n",
      "    3    3    1    7    4    0    0    2    3    0    4    0    4    1\n",
      "    0    2    5    2    1   14    2    0    0   19    0    1    2    1\n",
      "    0    3    2    0    0    3    1    3    3    2    7   18    7    6\n",
      "    6    0    1    9    1   10    2    0    2    0    2    4    0    0\n",
      "    1    2    0    1    0    2    0    0    0    2    1    2    2    0\n",
      "    3    2    2    0    0    1    2    3    1    1    1    2    0    0\n",
      "    3    0    7    2   39    0   14    0    1    1    0    1    5    3\n",
      "   11    0    3    0    1    1    0    0    1    9    2    1    0   11\n",
      "    1    3    7    0    0    0   32    1    0    0    0    1    1    3\n",
      "    0    9    0    2    0    1    3    2    6    0    3    0    0    2\n",
      "    3    0    1    0    1    4    0    0    1    1    0    0    5   21\n",
      "    2    1    1    3    0    1    7    1    3    4    0    5    3    0\n",
      "    7    2    0    4    2    0    2    1    4    4    0    0    0    5\n",
      "    3    2    2    0    4    0   23    2    2    2    4    0    1    0\n",
      "    4    0    3    5    3    0    8    0    1   16    1    2    2    7\n",
      "    0    0    1   11    1    0    4    0    1    0    1    2    1    5\n",
      "    0   97    0    2    0    3    0    8    1   14    4    9    2    3\n",
      "    1    1    0    3    4    0    5    1    5    2    0    0    0    2\n",
      "    1    2    1    1    1    1   12    0    2    5    1    0    0   13\n",
      "    2    0    0    0    2    2    0    0    3    1    1    1    1    0\n",
      "    1    2    1    0    0    0   10    0    1    0    1    1    1    1\n",
      "    0    1    0    0    3    2    5    0    0    2    1    0   23    0\n",
      "    0    4    0    1    0    0    0    1    1    2    1    0    1    0\n",
      "    0    4    1    0    1    1    5    1    1    0    1    0    0    0\n",
      "    1    0    0    2    2    3    0    1    0    4    3    3    1    4\n",
      "    0    0    0    6    1    2    1    0    5    3    0    0    1    2\n",
      "    0    5    0    0    2    1    1    4   15    0    0    1    1    3\n",
      "    1    0    1    4    1    1    2    8    1    3    0    0    0    0\n",
      "    1    3    2    1    0    1    0    2    0    0    0    0    1    1\n",
      "    0    1    3    7    0    0   42    4    0    1    2    3    1    0\n",
      "    1    3    2    0    0    4    0    0    0    4    2    0    0    8\n",
      "    2    0    1   15    0    0]\n"
     ]
    }
   ],
   "source": [
    "# now, to test the space, you can print vector representation for some words\n",
    "print('house:', space_1k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. It took 40 minutes on a laptop. All matrices are available on mlt-gpu: `ktw_wikipediaktw.npy`, `raw_wikipediaktw.npy`, `ppmi_wikipediaktw.npy`, `svd50_wikipedia10k.npy`. Make sure they are in your path, because they will be loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia.txt',\n",
       " 'ktw_wikipediaktw.npy',\n",
       " 'ppmi_wikipediaktw.npy',\n",
       " 'raw_wikipediaktw.npy',\n",
       " 'sub_glov.npy',\n",
       " 'svd50_wikipedia10k.npy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(corpus_dir + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "print('Please wait...')\n",
    "ktw_10k       = np.load('/srv/data/computational-semantics-assignment-02/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load('/srv/data/computational-semantics-assignment-02/raw_wikipediaktw.npy', allow_pickle=True).all()\n",
    "ppmispace_10k = np.load('/srv/data/computational-semantics-assignment-02/ppmi_wikipediaktw.npy', allow_pickle=True).all()\n",
    "svdspace_10k  = np.load('/srv/data/computational-semantics-assignment-02/svd50_wikipedia10k.npy', allow_pickle=True).all()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# testing semantic space\n",
    "print('house:', space_10k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` (a copy is included with this notebook) contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The score range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture (the one from [2]). For more information, please read the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 12\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # it will check if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "        \n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate Pearson's correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_similarities  = [cosine(w1,w2,space_10k) for w1, w2 in word_pairs]\n",
    "ppmi_similarities = [cosine(w1,w2,ppmispace_10k) for w1, w2 in word_pairs]\n",
    "svd_similarities  = [cosine(w1,w2,svdspace_10k) for w1, w2 in word_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original semantic similarity vs. the calculated raw similarity:\n",
      "\n",
      "    rho     = 0.1522\n",
      "    p-value = 0.0000\n",
      "Original semantic similarity vs. the calculated ppmi similarity:\n",
      "\n",
      "    rho     = 0.4547\n",
      "    p-value = 0.0000\n",
      "Original semantic similarity vs. the calculated svd similarity:\n",
      "\n",
      "    rho     = 0.4232\n",
      "    p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for k,v in {\"raw\": raw_similarities, \"ppmi\": ppmi_similarities, \"svd\": svd_similarities}.items():\n",
    "    print(f'Original semantic similarity vs. the calculated {k} similarity:')\n",
    "    rho, pval = stats.spearmanr(semantic_similarity, v)\n",
    "    print(\"\"\"\n",
    "    rho     = {:.4f}\n",
    "    p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "The scores of the ppmi model correlate best with the original semantic similarity scores, while the raw scores perform by far the worst, with the svd similarities not being far behind ppmi.  \n",
    "In my opinion it is not very unexpected, and I will discuss these one by one:\n",
    "+ raw similarity relies on the raw vectors, where no extra calculation was done; this is a kind of a baseline, and the other versions could perform better or worse than this one - if they are worse that means we are not getting a good meaning representation, and if they perform better, that means that the transformation that we implemented is good at extracting something in the vectors that points to meaning.\n",
    "+ ppmi similarity relies on vectors that store information about how relevant the co-occurence of the two words is; this means that it stores whether two words form some collocation or common phrase, or whether they occured together by chance. This may be good for determining similarity, as it will give us similar values for the word co-occuring in thematically similar surroundings (e.g. \"cold\" and \"chilly\" will often occur in contexts describing winter weather, and less commonly with other contexts). This is not exactly what PPMI tests/saves, but I think this is how it helps improve similarity scores here.\n",
    "+ the SVD similarity relies on vectors the size of which was reduced via singular value decomposition. This sort of dimensionality reduction essentially forces the computer to come up with a smaller number of categories that will preserve the differences between the different vectors; while we don't understand what those categories really are, the computer is apparently good at noticing stuff that is indicative of a word's \"meaning\" as represented by the vector.  \n",
    "\n",
    "Thus, it makes sense that the models which process the raw data in some way that is supposed to highlight/reveal relevant values in the vectors work better than the model with just the raw data.  \n",
    "\n",
    "Perhaps an SVD model based on a PPMI model (and not the raw one, like in our case) would be even better, but it would take too long a time to make one like that (since you said it took 40min to calculate the 10k versions for you) for this version of the assignment - perhaps it could be a part of a VG attempt based on this assignment though?  \n",
    "***\n",
    "All space's similarity scores falls within the 5% p-value threshold. However, raw_similarities clearly have the weakest correlation with the \"true\" semantic and visual similarities.\n",
    "\n",
    "PMI on the other hand has the strongest correlation, and svd comes out slightly short handed. This is not too surprising as dimensionality reduction reduces available information in the data. SVD does however increase generalization and is less prone to overfit. As the models(spaces) are fit on a completely seperate data set from the test data, it is hard to argue that PMI wins due to overfitting. Thus I conclude that PMI is the best model.\n",
    "\n",
    "It would be interesting to see SVD results for different dimensions.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original visual similarity vs. the calculated raw similarity:\n",
      "\n",
      "    rho     = 0.1212\n",
      "    p-value = 0.0007\n",
      "Original visual similarity vs. the calculated ppmi similarity:\n",
      "\n",
      "    rho     = 0.3838\n",
      "    p-value = 0.0000\n",
      "Original visual similarity vs. the calculated svd similarity:\n",
      "\n",
      "    rho     = 0.3097\n",
      "    p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for k,v in {\"raw\": raw_similarities, \"ppmi\": ppmi_similarities, \"svd\": svd_similarities}.items():\n",
    "    print(f'Original visual similarity vs. the calculated {k} similarity:')\n",
    "    rho, pval = stats.spearmanr(visual_similarity, v)\n",
    "    print(\"\"\"\n",
    "    rho     = {:.4f}\n",
    "    p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:** Similarly to the semantic similarity, here the faithfulness of our calculated similarities follows the PPMI > SVD > raw pattern. Overall though the scores are lower than when we compared semantic similarity to our calculated ones. If we look at the stored values for visual_similarity and semantic_similarity, we can see that they do not always correlate (e.g. a pair of words can have a high visual similarity and a low semantic similarity or the other way around). This could be because of the fact that when people were asked to evaluate the similarity of two words or two images, they took different features of these concepts into account. When asked to evaluate the semantic similarity they would think of not only the visuals of the two, but whether they share a hypernym, whether they share any features or relations, whether they are conceptually similar (e.g. ``('chicken', 'sparrow'), 3.0, 4.5)`` shows a high semantic similarity because both are birds, both have feathers and beaks and wings, both eat grain and peck and make bird noises, and could be preyed upon by a bird of prey). When comparing visual similarity the respondents likely focused only on the visual features of the two images, so whatever similarity is not represented like that escaped their judgement; I would expect these similarity scores to tend to be lower than the semantic ones (for the same example as above, the chicken and the sparrow have a lower visual similarity because although they are both birds - so they have beaks, wings, feathers - they have a different silhouette, color, proportions).\n",
    "***\n",
    "The same thought process as in the last question/answer can be applied here as well, since the results are quite similar. This is quite expected as we knew already that the \"true\" visual and semantic similarities were correlated by rho = 0.71.\n",
    "\n",
    "The correlation is lower for all models. This is not surprising as the models are fit on a text/semantic based dataset (corpora).\n",
    "\n",
    "Interestingly raw_similarities have a very similar result for semantic and visual.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions.\n",
    "\n",
    "For example, we can perform ``king - man`` and add the resulting vector to ``woman`` and we hope to get the vector for ``queen``. Also, what would be the result of ``stockholm - sweden + denmark``? Why? **[3 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:** operations of this kind on vectors are element-wise. Since we are looking at SVD vectors, we can assume that each element in a vector reveals something about its meaning (e.g. one element indicates age, one indicates gender, one indicates power, etc., with different values of these elements representing something on that spectrum). If we looked at raw vectors, it would just indicate how often our word occured together with a different one, but processing the vector with something like SVD means that we force our computer to generalize over that and create its own features that words share or differ in.  \n",
    "If we calculate ``king - man`` we will 'neutralize' all of the elements they have in common: if both king and queen are equally high on the gender scale, that will be gone in the resulting vector. However, differences pertaining to what sets these two vectors will be preserved: say that being a member of royalty is encoded as 0.3 in the vector, and not being that as the same element having the value 1. As a result, we will get a vector with 0.7 in that place, which is our difference between the two. \n",
    "Adding that vector to one that encodes feminine gender qualities (we removed all of that by subtracting man) we will get a vector representing an entity that is female and royal, ergo, a queen.  \n",
    "As for ``stockholm - sweden`` we would get a vector preserving whatever features make something a big city and/or a capital. If we add that to ``Denmark``, we should get a vector that describes a big city, the capital of Denmark - Copenhagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some helpful code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heavy', 0.8469616731144479),\n",
       " ('light', 0.785085353730493),\n",
       " ('tank', 0.7784553102891761),\n",
       " ('armour', 0.772217339953805),\n",
       " ('ammunition', 0.7583790659839708),\n",
       " ('fuel', 0.7581116998969929),\n",
       " ('armor', 0.751785180419986),\n",
       " ('explosive', 0.7285789337751698),\n",
       " ('air', 0.7276732934378709),\n",
       " ('heavier', 0.713826306449596)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "find_similar_to(light + (heavy - long), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:** as far as I understand, we should be getting \"short\" pretty high up here - which we are not. I would guess that our models simply do not have that many words or that \"detailed\" vectors. Another option is that the way that the vectors are constructed, or the information that they store is not informative about the semantic differences between these words (vide the example below).\n",
    "***\n",
    "light - (heavy - long) = (light - heavy) + long ) = **outcome** => light - heavy =**outcome** - long. So I expect the outcome to be whatever **outcome** is to long as light is to heavy - which would be short. This is however not one of the word top 10 similar words. Why this is the case is hard to speculate, but in short the model is simply not perfect and a result of the data we have. With infinite data I would expect short to come out with a high similarity (but not nessecarily 1.0).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: Google for `word analogies examples`. You can also construct analogies that are less lexical but more grammatical, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) from [3]. Does the resulting vector similarity confirm your expectations? But remember you can only do this if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = normalize(svdspace_10k['old'])\n",
    "older = normalize(svdspace_10k['older'])\n",
    "low = normalize(svdspace_10k['low'])\n",
    "lower = normalize(svdspace_10k['lower'])\n",
    "cat = normalize(svdspace_10k['cat'])\n",
    "lion = normalize(svdspace_10k['lion'])\n",
    "dog = normalize(svdspace_10k['dog'])\n",
    "wolf = normalize(svdspace_10k['wolf'])\n",
    "play = normalize(svdspace_10k['play'])\n",
    "plays = normalize(svdspace_10k['plays'])\n",
    "find = normalize(svdspace_10k['find'])\n",
    "finds = normalize(svdspace_10k['finds'])\n",
    "known = normalize(svdspace_10k['known'])\n",
    "unknown = normalize(svdspace_10k['unknown'])\n",
    "likely = normalize(svdspace_10k['likely'])\n",
    "unlikely = normalize(svdspace_10k['unlikely'])\n",
    "son = normalize(svdspace_10k['son'])\n",
    "daughter = normalize(svdspace_10k['daughter'])\n",
    "father = normalize(svdspace_10k['father'])\n",
    "mother = normalize(svdspace_10k['mother'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for words similar to low the way older is similar to old. The truest answer is lower.\n",
      "[('low', 0.6894237524984329), ('significantly', 0.6873546542936928), ('slower', 0.6718338963835531), ('faster', 0.6670393994739058), ('cooling', 0.6653603825537823), ('considerably', 0.6647776319091976), ('compression', 0.6628822141661048), ('decreased', 0.6594799562570409), ('higher', 0.6459016827789651), ('dramatically', 0.6321896717731696)]\n",
      "\n",
      "Looking for words similar to dog the way lion is similar to cat. The truest answer is wolf.\n",
      "[('lion', 0.9218163588717467), ('bull', 0.8453881572346528), ('dog', 0.8383677114992866), ('tiger', 0.8273453236965037), ('horse', 0.8264845000165553), ('devil', 0.8167105218171784), ('flower', 0.8102502053077751), ('bird', 0.8031785158794243), ('cow', 0.802565430879903), ('orange', 0.7935392173577519)]\n",
      "\n",
      "Looking for words similar to find the way plays is similar to play. The truest answer is finds.\n",
      "[('finds', 0.8104500821160944), ('explains', 0.7985960633876179), ('presents', 0.7965091826537933), ('mentions', 0.7863015836779274), ('inspired', 0.7769290657244649), ('suggests', 0.7623709202040903), ('contributed', 0.7567250807277677), ('plays', 0.7561317649717021), ('attributed', 0.755577419520653), ('appears', 0.7550710533234977)]\n",
      "\n",
      "Looking for words similar to likely the way unknown is similar to known. The truest answer is unlikely.\n",
      "[('likely', 0.8162002813659075), ('failure', 0.7638104328208204), ('sufficient', 0.752660758685138), ('possible', 0.7476533659241239), ('might', 0.7411342574304359), ('failing', 0.737094232805107), ('impossible', 0.7363870413379375), ('fails', 0.7300126447175319), ('supposed', 0.7292346125322156), ('difficult', 0.7269559693285305)]\n",
      "\n",
      "Looking for words similar to father the way daughter is similar to son. The truest answer is mother.\n",
      "[('wife', 0.9683310362690005), ('mother', 0.9507422230138359), ('father', 0.9506299006394313), ('daughter', 0.9181279193767793), ('sister', 0.8981916032683985), ('husband', 0.8789262639367043), ('grandmother', 0.8739455137653891), ('lady', 0.8518851833112602), ('brother', 0.8489032002714013), ('friend', 0.8488170016242245)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    (('old','older'),('low','lower')), (('cat','lion'),('dog','wolf')), (('play','plays'),('find','finds')), \n",
    "    (('known','unknown'),('likely','unlikely')), (('son','daughter'),('father','mother'))\n",
    "]\n",
    "\n",
    "for pair in pairs:\n",
    "    pair_1, pair_2 = pair\n",
    "    w1, w2 = pair_1\n",
    "    w3, w4 = pair_2\n",
    "    if w1 in svdspace_10k and w2 in svdspace_10k and w3 in svdspace_10k and w4 in svdspace_10k:\n",
    "        word1 = normalize(svdspace_10k[w1])\n",
    "        word2 = normalize(svdspace_10k[w2]) \n",
    "        word3 = normalize(svdspace_10k[w3])\n",
    "        word4 = normalize(svdspace_10k[w4])\n",
    "    else:\n",
    "        continue\n",
    "    print(f'Looking for words similar to {w3} the way {w2} is similar to {w1}. The truest answer is {w4}.')\n",
    "    similar_words = find_similar_to(word3 + (word2 - word1), svdspace_10k)[:10]\n",
    "    print(similar_words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:** For some examples we do get \"good\" results, for some we do not:\n",
    "+ In the first one the word that we are looking for is not there (it was when using the wrong formula!); we do get ``low`` itself, and we get some words in the comparative degree though.\n",
    "+ In the second one the target word is not found, but we do find other words for animals. Clearly the wildness may be preserved (we get ``lion`` and ``tiger``), but felinity or caninity is not. Another possibility is that the aspects of similarity that we can see in this analogy are not well preserved, as the two live in different circumstances etc.\n",
    "+ In the third one we get the correct answer as the most probable one, so it is a success.\n",
    "+ We do not get the correct answer here, but some meaning of likelihood is preserved with words like might, would, etc.\n",
    "+ Unlike when testing it with the old formula, we now get quite good results in this example. The word we are looking for is the second most likely one, only bested by ``wife``, which is a word that can be very close in meaning.\n",
    "\n",
    "Overall there are some elements of meaning that are preserved, but only two examples actually find the correct word. This may be due to many of these pairs of pairs differing in morphology and not meaning per se, which is not that well stored in our vectors, apparently. It may also be due to our model not being trained on a sufficiently big corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to look at how different semantic composition models, introduced in [2] correlate with human judgements. The file with the dataset `mitchell_lapata_acl08.txt` is included with this notebook (we also used it in the class).\n",
    "\n",
    "---\n",
    "\n",
    "Explanation of the task from Discord channel:\n",
    "\n",
    "**What are we trying to achieve?**  \n",
    "We want to create models, which can automatically capture differences between meaning of different phrases. These models should also be as good as we are (humans) in this task. Check example (1) from Mitchell/Lapata paper, it has two sentences which share the same words, but their meaning is completely different. We as humans can clearly see this difference, but how could a machine capture it?\n",
    "\n",
    "It is intuitive that in order to get a meaning of a phrase, we might combine meaning of individual words in this phrase somehow, but how? First, we represent each word with the frequency vector based on the semantic space that we built before (e.g, frequency space, ppmi space, svd space). In other words, each word's meaning is represented by the number of times other words occur in the context, defined by window size. Now, we have a vector for `discussion` and a vector for `thrive`.\n",
    "\n",
    "_How do we combine these vectors to get a single vector for the phrase `discussion thrive`?_\n",
    "\n",
    "Such methods of combining meaning vectors into a single item are called _semantic composition methods_ (literally, because we compose semantic meaning of the phrase from its individuals). During the lecture, we tried different semantic composition methods: additive, multiplicative, combined.\n",
    "\n",
    "Let's say we multiplied the vectors (went with the multiplicative method) and now we have one vector for our phrase.\n",
    "\n",
    "Remember, we want to have a model that captures differences between the phrases; it means that if `discussion thrive` is our reference phrase, we need to have a different phrase (high or low similarity phrase) to compare it against the reference one. How do we get this other phrase? Well, this other phrase can be either very similar to the reference or not similar at all, right? Let's say we decided to go with the second option and made/constructed a phrase `discussion digress`, which we know is very dissimilar to the reference phrase. We label this pair of phrases as having a low similarity, e.g., `low` in `hilo`. We can also create a different phrase (e.g., `discussion develop`) and use it as a high similarity phrase when paired with our reference phrase, right? This then would be labeled as `high` in `hilo`. This is what `hilo` in the dataset stands for: known information about how similar the reference phrase and the landmark phrase are.\n",
    "\n",
    "Now, our main task is to automatically learn the similarities/differences between our reference and our landmark, right? We take the first pair: `discussion thrive` vs. `discussion digress`. We have a vector representation for each of these phrases.\n",
    "\n",
    "**How do we compare two vectors?**\n",
    "\n",
    "We use cosine similarity to calculate a single score that would tell us about the similarity between these two vectors. The bigger the cosine, the more similar two vectors are. Cosine ranges from 0 to 1 (0 is very low similarity, 1 is very high similarity). Let's say, we get a cosine of 0.89, it means that according to the multiplicative model (remember, we decided to use multiplicative semantic composition method), these phrases are very similar (cosine is quite high). But wait a second, we know that these two phrases should be of low similarity, right? Because this is what the value in `hilo` tells us about this pair - `discussion thrive` and `discussion digress` are not similar to each other. Clearly, our multiplicative method fails to capture it.\n",
    "\n",
    "**What can we do to improve our model?**\n",
    "\n",
    "We can try a different composition method to get a phrase vector from phrase's words: let's replace multiplication with addition. Or we can also use combined method. Let's say we used combined method and run cosine again; this time it tells that the cosine score is 0.45. Ok, this seems to be quite low, and it also agrees with our knowledge that these phrases are indeed not similar.\n",
    "\n",
    "---\n",
    "\n",
    "In other words, we need to evaluate different composition models (additive, multiplicative, combined) and analyse how well they perform. **How do we analyse their performance?** Because we have the ground-truth for comparison (`hilo` values), we know whether our phrases are actually similar or not. We want our cosine score to reflect this knowledge: if the score is high, but the groun-truth hilo is low, then we have a problem in the model - it did not learn things well, we need to replace the composition function.\n",
    "\n",
    "---\n",
    "\n",
    "_Long story short_: `hilo` is something that we compare our cosine to. `hilo` contains correct answers about similarity between phrases, and cosine should agree with this. If reference-landmark pair are `high` in `hilo`, then cosine should be high enough to reflect that. If cosine is not high in this case, then we look at our model and change the composition function. We need to find the function, which give us cosines that are super close to the ground-truth known `hilo` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute do we compute correlation between our model's predictions and the ground-truth, something similar to the results from [2] (image above):\n",
    "\n",
    "---\n",
    "\n",
    "In `High` and `Low` columns we have mean cosine values.\n",
    "These are calculated by averaging cosine scores for all pairs of phrases per model.\n",
    "Rows introduce different models: `add` is additive, `multiply` is multiplicative, etc. (these are all described in the paper). `NonComp` is a baseline model, the most \"stupid\" one, it should be the worst. `UpperBound` is how humans performed in this task (they were asked to rate similarity between pairs of phrases, 1 is the lowest, 7 is the highest). Why these numbers are in a different scale, not from 0 to 1 like cosine, but from 1 to 7? Because they are not normalised, and authors explicitly said that they are interested in relative differences.\n",
    "\n",
    "We need models which are closer to human ratings. `Add` model has 0.59 mean for `High` and 0.59 for `Low`, so it did not learn to differentiate between high similarity pairs and low similarity ones. This is a bad model then, we need a better one. `WeightAdd` seems to be doing better, the difference between `High` and `Low` is now 0.01, but it's also quite bad - the difference is not that obvious. The best models are `Multiply` and `Combined`, because their mean cosines for `High` and `Low` are quite different from each other. We can see that these two models gave higher cosines for high similarity pairs (0.42 and 0.38), while giving lower cosines for low similarity pairs (0.28 and 0.28). And this is a good result - it shows that these two composition functions are so far the best in (i) giving high cosine go highly similar pairs and low cosine to very dissimilar pairs, and (ii) keeping the distance (range) between high and low cosines quite large.\n",
    "\n",
    "**However, we can't say how far/close they are when compared to human performance (UpperBound) since human scores are not normalised.**\n",
    "\n",
    "Still, we want to choose a model which is the closest to humans. This is why we want to run **the correlation test.**\n",
    "\n",
    "How do we perform the correlation test? We need to see how well *each model* correlates with human judgements. So for each model, we would have a vector of cosine values this model gives for each pair that we have. For example, let's say we have three pairs of phrases and our cosine values from additive model are the following ones: `[0.89, 0.40, 0.70]`. Now we need to get a vector of the same size, but for human scores. What do we have for human scores? We have multiple participants, which means that a single phrase can be evaluated by multiple participants. Let's say, the first phrase has scores from two participants (`6, 7`), so what we would do is that we would average it to have a single number (`(6 + 7) / 2 = 6.5`). With this, we can get a mean vector of human scores per item: `[6.5, 3, 6]`.\n",
    "\n",
    "Now, we have two vectors and we can run Spearman correlation on these vectors. This is what exactly what the third column in the Results table is showing (the correlation value). There is also a p-value (denoted right below the table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your ultimate task in this part of the assignment?**\n",
    "\n",
    "(i) Process the dataset and extract `reference - landmark` pairs; you can use the code from the lecture as something to start with. Try to keep information about human rating (`input`) and high/low similarity (`hilo`), because you will need it for correlation tests. Also, you might want to keep the information about participant id (will be useful for getting average numbers for correlation tests). Which format you should use to keep all this data? It's up to you, but a dictionary-like format could be a good choice.\n",
    "\n",
    "(ii) Build models of semantic phrase composition: in the lecture we introduced simple additive, simple multiplicative and combined models (details are in [2]). Your task is to take a single pair of phrases, and compute the composition of its vectors using each of these functions. Thus, you will have (at least) three compositional models that take each `noun - verb` phrase from the pair (these phrases can be either references or landmarks) and output a single vector, representing the meaning of this phrase. As your semantic space, you can use pretrained spaces (standard space, ppmi or svd) introduced above. It is up to you which space you use, but for someone who runs your code, it should be pretty straightforward to switch between them.\n",
    "\n",
    "(iii) calculate Spearman correlation between each model's predictions and human judgements; you should have something similar to the scores that are shown in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts process behind calculating the correlation:**\n",
    "\n",
    "Let's look at the example pair: reference `child chatter` and high-level similarity landmark (as the last word in the row indicates) `child gabble`. Let's say we have 3 humans evaluating the similarity between these two phrases and we combine their scores into a single vector: `[5, 6, 5]`. We need to average them to get our human vector for correlation: `[5.3]`.\n",
    "\n",
    "Our A model's output:  \n",
    "`cosine(p1, p2) = 0.88`, where p1 is the result of addition of word vectors in the reference phrase `child gabble`, and p2 is the result of addition of word vectors in the high-level similarity phrase `child chatter`.  \n",
    "\n",
    "Therefore, we have human rating vector `[5.3]` and model A output `[0.88]`. Next is to compute correlation between these two vectors. This should give you a correlation value and p-value for the model of choice and human ratings.\n",
    "\n",
    "Of course, your human rating vectors will be longer (e.g., [6, 7, 3, 4, 5]). Each of your models (A, B, C) will produce a single vector of cosine similarity between these same pairs (e.g., [0.89, 0.98, 0.23, 0.65, 0.55]). The goal is to compare each model's cosine similarity vectors with human rating vectors and identify the model which outputs the best result in terms of being the closest to the way human rate similarity between the phrases.\n",
    "\n",
    "---\n",
    "\n",
    "**The minimum to do in this task**: compute correlations for at least _ONE_ model and human ratings. However, this should not be hard to run it for any other model as well. For examples on how to interpret the results, look at Section 5 Results of the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset = svdspace_10k  # change here to change the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "participant20 stray thought roam 7 low\n",
      "participant20 stray discussion digress 6 high\n",
      "participant20 stray eye roam 7 high\n",
      "participant20 stray child digress 1 low\n",
      "participant20 throb body pulse 5 high\n",
      "participant20 throb head shudder 2 low\n",
      "participant20 throb voice shudder 3 low\n",
      "participant20 throb vein pulse 6 high\n",
      "participant20 chatter machine click 4 high\n"
     ]
    }
   ],
   "source": [
    "#from the lecture notebook:\n",
    "with open('./mitchell_lapata_acl08.txt', 'r') as f:\n",
    "    phrase_dataset = f.read().splitlines()\n",
    "\n",
    "for line in phrase_dataset[:10]:\n",
    "    print(line)\n",
    "    \n",
    "# get all unique words\n",
    "words = []\n",
    "for line in phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in words:\n",
    "        words.append(verb)\n",
    "    if noun not in words:\n",
    "        words.append(noun)\n",
    "    if landmark not in words:\n",
    "        words.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray\n",
      "roam\n",
      "digress\n",
      "throb\n",
      "pulse\n",
      "shudder\n",
      "vein\n",
      "chatter\n",
      "gabble\n",
      "tooth\n",
      "rebound\n",
      "ricochet\n",
      "optimism\n",
      "flicker\n",
      "waver\n",
      "flick\n",
      "subside\n",
      "lessen\n",
      "symptom\n",
      "slump\n",
      "slouch\n",
      "stoop\n",
      "erupt\n",
      "burst\n",
      "temper\n",
      "flare\n",
      "recoil\n",
      "flinch\n",
      "prosper\n",
      "fluctuate\n",
      "falter\n",
      "cigarette\n",
      "reel\n",
      "whirl\n",
      "stagger\n",
      "glow\n",
      "cigar\n"
     ]
    }
   ],
   "source": [
    "# simply check if all words that we have in our task dataset can be found in the reference corpus (the result should return nothing)\n",
    "to_remove = []\n",
    "for w in words:\n",
    "    if w not in our_dataset:\n",
    "        print(w)\n",
    "        to_remove.append(w)\n",
    "# if something is not found, makes sense to ignore phrases with such non-present words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the task dataset (we might call it phrase dataset from now on)\n",
    "# we are removing all phrases which contain non-found words\n",
    "# this would probably remove other words as well (those, which are paired with the non-found words)\n",
    "\n",
    "cleaned_phrase_dataset = []\n",
    "for line in phrase_dataset:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb in to_remove or noun in to_remove or landmark in to_remove:\n",
    "        continue\n",
    "    cleaned_phrase_dataset.append(line)\n",
    "\n",
    "target_words = []\n",
    "for line in cleaned_phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in target_words:\n",
    "        target_words.append(verb)\n",
    "    if noun not in target_words:\n",
    "        target_words.append(noun)\n",
    "    if landmark not in target_words:\n",
    "        target_words.append(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on the code is our own:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('butler', 'bow'), ('butler', 'submit')): {'participant_count': 34, 'input_sum': 100, 'hilo': 'low'}, (('company', 'bow'), ('company', 'submit')): {'participant_count': 34, 'input_sum': 147, 'hilo': 'high'}, (('sale', 'boom'), ('sale', 'thunder')): {'participant_count': 34, 'input_sum': 95, 'hilo': 'low'}, (('gun', 'boom'), ('gun', 'thunder')): {'participant_count': 34, 'input_sum': 191, 'hilo': 'high'}, (('head', 'bow'), ('head', 'submit')): {'participant_count': 26, 'input_sum': 85, 'hilo': 'low'}, (('government', 'bow'), ('government', 'submit')): {'participant_count': 26, 'input_sum': 140, 'hilo': 'high'}, (('noise', 'boom'), ('noise', 'thunder')): {'participant_count': 26, 'input_sum': 159, 'hilo': 'high'}, (('export', 'boom'), ('export', 'thunder')): {'participant_count': 26, 'input_sum': 72, 'hilo': 'low'}}\n"
     ]
    }
   ],
   "source": [
    "phrase_dictionary = {}\n",
    "for string in cleaned_phrase_dataset[1:]:\n",
    "    participant, verb, noun, landmark_verb, human_input, hilo = string.split(' ')\n",
    "    pair_1 = (noun, verb)\n",
    "    pair_2 = (noun, landmark_verb)\n",
    "    pairs = (pair_1, pair_2)\n",
    "    \n",
    "    if pairs not in phrase_dictionary:\n",
    "        phrase_dictionary[pairs] = {'participant_count': 1, 'input_sum': int(human_input), 'hilo': hilo}\n",
    "    else:\n",
    "        phrase_dictionary[pairs]['participant_count'] += 1\n",
    "        phrase_dictionary[pairs]['input_sum'] += int(human_input)\n",
    "        \n",
    "print(phrase_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define functions that represent the compositional models, with the weights based on what was found to be best in [2], \n",
    "# if applicable. We also discarded the Kintsch representation as we believe it needed some other variables (as far as we \n",
    "# understood it). \n",
    "\n",
    "def repr_multiply(word_tuple):\n",
    "    w1, w2 = word_tuple\n",
    "    noun = our_dataset[w1]\n",
    "    verb = our_dataset[w2]\n",
    " \n",
    "    representation = noun * verb\n",
    "    \n",
    "    return representation\n",
    "\n",
    "def repr_add(word_tuple):\n",
    "    w1, w2 = word_tuple\n",
    "    noun = our_dataset[w1]\n",
    "    verb = our_dataset[w2]\n",
    " \n",
    "    representation = noun + verb\n",
    "    \n",
    "    return representation\n",
    "\n",
    "def repr_weight_add(word_tuple):\n",
    "    w1, w2 = word_tuple\n",
    "    noun = our_dataset[w1]\n",
    "    verb = our_dataset[w2]\n",
    " \n",
    "    representation = (0.2 * noun) + (0.8 * verb)\n",
    "    \n",
    "    return representation\n",
    "\n",
    "def repr_combined(word_tuple):\n",
    "    w1, w2 = word_tuple\n",
    "    noun = our_dataset[w1]\n",
    "    verb = our_dataset[w2]\n",
    " \n",
    "    representation = (0.0 * noun) + (0.95 * verb) + (0.05 * noun * verb)\n",
    "    \n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('butler', 'bow'), ('butler', 'submit')): {'average_input': 2.9411764705882355, 'addition': 0.8366040642896164, 'multiplication': 0.8751666899417615, 'weighted_addition': 0.5624267039698349, 'combined': 0.24661793239357221, 'hilo': 'low'}, (('company', 'bow'), ('company', 'submit')): {'average_input': 4.323529411764706, 'addition': 0.9304615493700634, 'multiplication': 0.9273976875291448, 'weighted_addition': 0.6734756630177691, 'combined': 0.132451617886431, 'hilo': 'high'}, (('sale', 'boom'), ('sale', 'thunder')): {'average_input': 2.7941176470588234, 'addition': 0.8776046136128655, 'multiplication': 0.975175543949629, 'weighted_addition': 0.6847917067605415, 'combined': 0.33894277055275446, 'hilo': 'low'}, (('gun', 'boom'), ('gun', 'thunder')): {'average_input': 5.617647058823529, 'addition': 0.9229430503489647, 'multiplication': 0.9740963478239069, 'weighted_addition': 0.7268572088527075, 'combined': 0.3356509207630427, 'hilo': 'high'}, (('head', 'bow'), ('head', 'submit')): {'average_input': 3.269230769230769, 'addition': 0.8747165074822579, 'multiplication': 0.9398637212790923, 'weighted_addition': 0.6163648369816668, 'combined': 0.22380798301971083, 'hilo': 'low'}, (('government', 'bow'), ('government', 'submit')): {'average_input': 5.384615384615385, 'addition': 0.931689728059706, 'multiplication': 0.8671235911741174, 'weighted_addition': 0.6779138438287566, 'combined': 0.13071763918235635, 'hilo': 'high'}, (('noise', 'boom'), ('noise', 'thunder')): {'average_input': 6.115384615384615, 'addition': 0.9219236351707659, 'multiplication': 0.9713075958327421, 'weighted_addition': 0.7335650252383998, 'combined': 0.4065914590057804, 'hilo': 'high'}, (('export', 'boom'), ('export', 'thunder')): {'average_input': 2.769230769230769, 'addition': 0.9174691846974397, 'multiplication': 0.9361078822218509, 'weighted_addition': 0.7159891091461872, 'combined': 0.36278825807688875, 'hilo': 'low'}}\n"
     ]
    }
   ],
   "source": [
    "# here we construct a dictionary to store the values of similarity between representations for different compositional models\n",
    "# and the average human evaluation\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "model_pred_dict = {}\n",
    "\n",
    "for k,v in phrase_dictionary.items():\n",
    "    word_tuple_1, word_tuple_2 = k\n",
    "    participant_count = v['participant_count']\n",
    "    input_sum = v['input_sum']\n",
    "    hilo = v['hilo']\n",
    "    \n",
    "    average_input = input_sum / participant_count\n",
    "    \n",
    "    # addition\n",
    "    repr_add_1 = repr_add(word_tuple_1)\n",
    "    repr_add_2 = repr_add(word_tuple_2)\n",
    "    cosine_sim_add = 1 - spatial.distance.cosine(repr_add_1, repr_add_2)\n",
    "    \n",
    "    # multiplication\n",
    "    repr_mult_1 = repr_multiply(word_tuple_1)\n",
    "    repr_mult_2 = repr_multiply(word_tuple_2)\n",
    "    cosine_sim_mult = 1 - spatial.distance.cosine(repr_mult_1, repr_mult_2)\n",
    "    \n",
    "    # weighted addition\n",
    "    repr_wadd_1 = repr_weight_add(word_tuple_1)\n",
    "    repr_wadd_2 = repr_weight_add(word_tuple_2)\n",
    "    cosine_sim_wadd = 1 - spatial.distance.cosine(repr_wadd_1, repr_wadd_2)\n",
    "    \n",
    "    # combined\n",
    "    repr_comb_1 = repr_combined(word_tuple_1)\n",
    "    repr_comb_2 = repr_combined(word_tuple_2)\n",
    "    cosine_sim_comb = 1 - spatial.distance.cosine(repr_comb_1, repr_comb_2)\n",
    "    \n",
    "    model_pred_dict[k] = {\n",
    "        'average_input': average_input, 'addition': cosine_sim_add, 'multiplication': cosine_sim_mult, \n",
    "        'weighted_addition': cosine_sim_wadd, 'combined': cosine_sim_comb, 'hilo': hilo\n",
    "                         }\n",
    "    \n",
    "print(model_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average': [4.323529411764706, 5.617647058823529, 5.384615384615385, 6.115384615384615], 'addition': [0.9304615493700634, 0.9229430503489647, 0.931689728059706, 0.9219236351707659], 'multiplication': [0.9273976875291448, 0.9740963478239069, 0.8671235911741174, 0.9713075958327421], 'weighted_addition': [0.6734756630177691, 0.7268572088527075, 0.6779138438287566, 0.7335650252383998], 'combined': [0.132451617886431, 0.3356509207630427, 0.13071763918235635, 0.4065914590057804]}\n",
      "{'average': [2.9411764705882355, 2.7941176470588234, 3.269230769230769, 2.769230769230769], 'addition': [0.8366040642896164, 0.8776046136128655, 0.8747165074822579, 0.9174691846974397], 'multiplication': [0.8751666899417615, 0.975175543949629, 0.9398637212790923, 0.9361078822218509], 'weighted_addition': [0.5624267039698349, 0.6847917067605415, 0.6163648369816668, 0.7159891091461872], 'combined': [0.24661793239357221, 0.33894277055275446, 0.22380798301971083, 0.36278825807688875]}\n",
      "{'average': [2.9411764705882355, 4.323529411764706, 2.7941176470588234, 5.617647058823529, 3.269230769230769, 5.384615384615385, 6.115384615384615, 2.769230769230769], 'addition': [0.8366040642896164, 0.9304615493700634, 0.8776046136128655, 0.9229430503489647, 0.8747165074822579, 0.931689728059706, 0.9219236351707659, 0.9174691846974397], 'multiplication': [0.8751666899417615, 0.9273976875291448, 0.975175543949629, 0.9740963478239069, 0.9398637212790923, 0.8671235911741174, 0.9713075958327421, 0.9361078822218509], 'weighted_addition': [0.5624267039698349, 0.6734756630177691, 0.6847917067605415, 0.7268572088527075, 0.6163648369816668, 0.6779138438287566, 0.7335650252383998, 0.7159891091461872], 'combined': [0.24661793239357221, 0.132451617886431, 0.33894277055275446, 0.3356509207630427, 0.22380798301971083, 0.13071763918235635, 0.4065914590057804, 0.36278825807688875]}\n"
     ]
    }
   ],
   "source": [
    "# here we split the results into high and low similarity lists and also gather them all in lists where the order is relevant,\n",
    "# so they can be compared using  Spearman correlation.\n",
    "\n",
    "high_pairs = {'average': [], 'addition': [], 'multiplication': [], 'weighted_addition': [], 'combined': []}\n",
    "low_pairs = {'average': [], 'addition': [], 'multiplication': [], 'weighted_addition': [], 'combined': []}\n",
    "full_lists = {'average': [], 'addition': [], 'multiplication': [], 'weighted_addition': [], 'combined': []}\n",
    "\n",
    "for k,v in model_pred_dict.items():\n",
    "    word_tuple_1, word_tuple_2 = k\n",
    "    average = v['average_input']\n",
    "    addition = v['addition']\n",
    "    multiplication = v['multiplication']\n",
    "    weighted_addition = v['weighted_addition']\n",
    "    combined = v['combined']\n",
    "    hilo = v['hilo']\n",
    "    \n",
    "    \n",
    "    if hilo == 'high':\n",
    "        high_pairs['average'].append(average)\n",
    "        full_lists['average'].append(average)\n",
    "        \n",
    "        high_pairs['addition'].append(addition)\n",
    "        full_lists['addition'].append(addition)\n",
    "        \n",
    "        high_pairs['multiplication'].append(multiplication)\n",
    "        full_lists['multiplication'].append(multiplication)\n",
    "        \n",
    "        high_pairs['weighted_addition'].append(weighted_addition)\n",
    "        full_lists['weighted_addition'].append(weighted_addition)\n",
    "        \n",
    "        high_pairs['combined'].append(combined)\n",
    "        full_lists['combined'].append(combined)\n",
    "    \n",
    "    else:\n",
    "        low_pairs['average'].append(average)\n",
    "        full_lists['average'].append(average)\n",
    "        \n",
    "        low_pairs['addition'].append(addition)\n",
    "        full_lists['addition'].append(addition)\n",
    "        \n",
    "        low_pairs['multiplication'].append(multiplication)\n",
    "        full_lists['multiplication'].append(multiplication)\n",
    "        \n",
    "        low_pairs['weighted_addition'].append(weighted_addition)\n",
    "        full_lists['weighted_addition'].append(weighted_addition)\n",
    "        \n",
    "        low_pairs['combined'].append(combined)\n",
    "        full_lists['combined'].append(combined)\n",
    "        \n",
    "print(high_pairs)\n",
    "print(low_pairs)\n",
    "print(full_lists)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['addition', 'multiplication', 'weighted_addition', 'combined', 'average']\n",
    "scores_list = []\n",
    "\n",
    "for name in types:\n",
    "    high = high_pairs[name]\n",
    "    low = low_pairs[name]\n",
    "    full = full_lists[name]\n",
    "    average = full_lists['average']\n",
    "    \n",
    "    average_high = sum(high) / len(high)\n",
    "    average_low = sum(low) / len(low)\n",
    "       \n",
    "    rho, pval = stats.spearmanr(full, average)\n",
    "\n",
    "    score_list = [name, round(average_high, 2), round(average_low, 2), round(rho, 2), round(pval, 2)]\n",
    "    scores_list.append(score_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  High   Low   Rho p-score\n",
      "0           addition  0.93  0.88  0.57    0.14\n",
      "1     multiplication  0.93  0.93  0.05    0.91\n",
      "2  weighted_addition   0.7  0.64   0.4    0.32\n",
      "3           combined  0.25  0.29  -0.1    0.82\n",
      "4            average  5.36  2.94   1.0     0.0\n"
     ]
    }
   ],
   "source": [
    "# this part is done just to print it out in a pretty format\n",
    "print(pd.DataFrame(np.array(scores_list), columns=[\"Model\", \"High\", \"Low\", \"Rho\", \"p-score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:** (Maria) I was actually really surprised to see that these results do not align at all with the ones from the Mitchell and Lapata paper, but I assume this is due to there being so few pairs that we could compare stuff for. Clearly, the combined method works the worst here, and, surprisingly, addition or weighted addition work the best. I am not sure I did everything right though so it will be good to compare my results with those of my classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "\n",
      "participant20 stray thought roam 7 low\n",
      "\n",
      "participant20 stray discussion digress 6 high\n",
      "\n",
      "participant20 stray eye roam 7 high\n",
      "\n",
      "participant20 stray child digress 1 low\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = 'mitchell_lapata_acl08.txt'\n",
    "with open(file_name) as f:\n",
    "    lines = f.readlines()\n",
    "print('\\n'.join(lines[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>verb</th>\n",
       "      <th>noun</th>\n",
       "      <th>landmark</th>\n",
       "      <th>input</th>\n",
       "      <th>hilo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>participant20</td>\n",
       "      <td>subside</td>\n",
       "      <td>flood</td>\n",
       "      <td>lessen</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>participant20</td>\n",
       "      <td>subside</td>\n",
       "      <td>fear</td>\n",
       "      <td>lessen</td>\n",
       "      <td>5</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>participant20</td>\n",
       "      <td>subside</td>\n",
       "      <td>symptom</td>\n",
       "      <td>sink</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>participant20</td>\n",
       "      <td>subside</td>\n",
       "      <td>island</td>\n",
       "      <td>sink</td>\n",
       "      <td>2</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>participant20</td>\n",
       "      <td>slump</td>\n",
       "      <td>shoulder</td>\n",
       "      <td>slouch</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>participant20</td>\n",
       "      <td>slump</td>\n",
       "      <td>sale</td>\n",
       "      <td>slouch</td>\n",
       "      <td>6</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>participant20</td>\n",
       "      <td>slump</td>\n",
       "      <td>value</td>\n",
       "      <td>decline</td>\n",
       "      <td>7</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>participant20</td>\n",
       "      <td>slump</td>\n",
       "      <td>man</td>\n",
       "      <td>decline</td>\n",
       "      <td>1</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>participant20</td>\n",
       "      <td>bow</td>\n",
       "      <td>butler</td>\n",
       "      <td>submit</td>\n",
       "      <td>3</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>participant20</td>\n",
       "      <td>bow</td>\n",
       "      <td>company</td>\n",
       "      <td>submit</td>\n",
       "      <td>5</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>participant20</td>\n",
       "      <td>bow</td>\n",
       "      <td>head</td>\n",
       "      <td>stoop</td>\n",
       "      <td>5</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>participant20</td>\n",
       "      <td>bow</td>\n",
       "      <td>government</td>\n",
       "      <td>stoop</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>participant20</td>\n",
       "      <td>erupt</td>\n",
       "      <td>fountain</td>\n",
       "      <td>burst</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>participant20</td>\n",
       "      <td>erupt</td>\n",
       "      <td>temper</td>\n",
       "      <td>flare</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>participant20</td>\n",
       "      <td>erupt</td>\n",
       "      <td>storm</td>\n",
       "      <td>flare</td>\n",
       "      <td>6</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>participant20</td>\n",
       "      <td>erupt</td>\n",
       "      <td>conflict</td>\n",
       "      <td>burst</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>participant20</td>\n",
       "      <td>recoil</td>\n",
       "      <td>heart</td>\n",
       "      <td>flinch</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>participant20</td>\n",
       "      <td>recoil</td>\n",
       "      <td>rifle</td>\n",
       "      <td>kick</td>\n",
       "      <td>7</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>participant20</td>\n",
       "      <td>recoil</td>\n",
       "      <td>eye</td>\n",
       "      <td>flinch</td>\n",
       "      <td>5</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>participant20</td>\n",
       "      <td>recoil</td>\n",
       "      <td>hand</td>\n",
       "      <td>kick</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      participant     verb        noun landmark  input  hilo\n",
       "20  participant20  subside       flood   lessen      6  high\n",
       "21  participant20  subside        fear   lessen      5  high\n",
       "22  participant20  subside     symptom     sink      4   low\n",
       "23  participant20  subside      island     sink      2  high\n",
       "24  participant20    slump    shoulder   slouch      6  high\n",
       "25  participant20    slump        sale   slouch      6   low\n",
       "26  participant20    slump       value  decline      7  high\n",
       "27  participant20    slump         man  decline      1   low\n",
       "28  participant20      bow      butler   submit      3   low\n",
       "29  participant20      bow     company   submit      5  high\n",
       "30  participant20      bow        head    stoop      5  high\n",
       "31  participant20      bow  government    stoop      4   low\n",
       "32  participant20    erupt    fountain    burst      1  high\n",
       "33  participant20    erupt      temper    flare      6  high\n",
       "34  participant20    erupt       storm    flare      6   low\n",
       "35  participant20    erupt    conflict    burst      5   low\n",
       "36  participant20   recoil       heart   flinch      6  high\n",
       "37  participant20   recoil       rifle     kick      7  high\n",
       "38  participant20   recoil         eye   flinch      5  high\n",
       "39  participant20   recoil        hand     kick      5   low"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can load this using pandas\n",
    "data = pd.read_csv(file_name, sep=' ', header=0)\n",
    "data.head()\n",
    "data.iloc[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add([1,2],[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (noun, verb) and (noun, landmark)\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Define representations\n",
    "def multiply(verb, subject, space):\n",
    "    return np.multiply(space[subject],space[verb])\n",
    "\n",
    "def add(verb, subject, space):\n",
    "    return np.add(space[subject],space[verb])\n",
    "\n",
    "def weighted_add(verb, subject, space):\n",
    "    weight_s=0.2\n",
    "    weight_v=0.8\n",
    "    return np.add(np.multiply(weight_s, space[subject]), np.multiply(weight_v, space[verb]))\n",
    "\n",
    "def combined(verb, subject, space):\n",
    "    weight_1 = 0.0\n",
    "    weight_2 = 0.95\n",
    "    weight_3 = 0.05\n",
    "    out = np.multiply(weight_1,  space[subject]) + np.multiply(weight_2,  space[verb]) + weight_3*np.multiply( space[subject],  space[verb])\n",
    "    return out\n",
    "    \n",
    "repr_lookup = {'Add':add, 'Multiply':multiply, 'WeightedAdd':weighted_add, 'Combined':combined}\n",
    "\n",
    "# cosine similarity\n",
    "def cosine_sim(v1, v2):\n",
    "    # we use 1-cosine because we want similar vectors to get a high value\n",
    "    return 1-distance.cosine(v1, v2)\n",
    "\n",
    "def spearman(cosine_sim, human_sim):\n",
    "    return stats.spearmanr(cosine_sim, human_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, space, verbose=True):\n",
    "    \n",
    "    # remove any row in the dataset  that contains a word that is not present in the space\n",
    "    data_2 = data.copy().loc[data.loc[:, ['verb','noun','landmark']].isin(space.keys()).all(axis=1)]\n",
    "    # The same phrase-pairs can be evaluate by multiple participants. We need to average these scores\n",
    "    data_3 = data_2.copy().drop_duplicates(subset=['verb','noun','landmark'], keep='first', inplace=False, ignore_index=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Data size before removal', len(data))\n",
    "        print('Data size after removal', len(data_2))\n",
    "        print('N_unique phrase pairs', len(data_3))\n",
    "\n",
    "    # average scores\n",
    "    for v_n_l, group in data_2.groupby(['verb','noun','landmark']):\n",
    "        idx = (data_3.loc[:,['verb','noun','landmark']] == list(v_n_l)).all(axis=1)\n",
    "        data_3.loc[idx,'input'] = np.mean(group['input'])\n",
    "\n",
    "    #drop participant\n",
    "    data_3.drop('participant',axis=1,inplace=True)\n",
    "    \n",
    "    #data_3['hilo_binary'] = (data_3['hilo'] == 'high').astype(int)\n",
    "    \n",
    "    return data_3 #preprocessed_data\n",
    "\n",
    "def create_results(data_3, space, repr_lookup=repr_lookup):\n",
    "    # \"calculate Spearman correlation between each model's predictions and human judgements\"\n",
    "    # Note this slightly differs from the paper. In the paper they calculate the correlation between scores and true.\n",
    "\n",
    "    result_matrix = []\n",
    "    #UpperBound / Human\n",
    "    high = np.mean(data_3[data_3['hilo']=='high']['input'])\n",
    "    low = np.mean(data_3[data_3['hilo']=='low']['input'])\n",
    "    result_matrix.append(['Human', high, low, 1.0, 0.0])\n",
    "\n",
    "    #Representation functions\n",
    "    for repr_name, repr_func in repr_lookup.items():\n",
    "\n",
    "        cos_sims = pd.Series(index=data_3.index, dtype='float')\n",
    "        for idx, row in data_3.loc[:, ['verb','noun','landmark']].iterrows():\n",
    "            # (noun, verb) and (noun, landmark)\n",
    "            repr_vec1 =repr_func(row['verb'], row['noun'], space)\n",
    "            repr_vec2 = repr_func(row['landmark'], row['noun'], space)\n",
    "            cos_sim = cosine_sim(repr_vec1, repr_vec2)\n",
    "            cos_sims[idx] = cos_sim\n",
    "\n",
    "        rho, pval = stats.spearmanr(cos_sims, data_3['input'])\n",
    "        high = np.mean(cos_sims[data_3['hilo']=='high'])\n",
    "        low = np.mean(cos_sims[data_3['hilo']=='low'])\n",
    "        result_matrix.append([repr_name, high, low, rho, pval])\n",
    "\n",
    "    result_matrix = pd.DataFrame(result_matrix, columns=['Model', 'High', 'Low', 'Rho', 'p-value'])\n",
    "    result_matrix['relative_diff'] = (result_matrix['High']-result_matrix['Low'])/result_matrix['High']\n",
    "    result_matrix = round(result_matrix,3)\n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space: space_10k\n",
      "Data size before removal 3600\n",
      "Data size after removal 240\n",
      "N_unique phrase pairs 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Rho</th>\n",
       "      <th>p-value</th>\n",
       "      <th>relative_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human</td>\n",
       "      <td>5.360</td>\n",
       "      <td>2.943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiply</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedAdd</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model   High    Low    Rho  p-value  relative_diff\n",
       "0        Human  5.360  2.943  1.000    0.000          0.451\n",
       "1          Add  0.986  0.964  0.452    0.260          0.023\n",
       "2     Multiply  0.908  0.896  0.167    0.693          0.014\n",
       "3  WeightedAdd  0.942  0.861  0.405    0.320          0.086\n",
       "4     Combined  0.903  0.879  0.286    0.493          0.026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Space: ppmispace_10k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Rho</th>\n",
       "      <th>p-value</th>\n",
       "      <th>relative_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human</td>\n",
       "      <td>5.360</td>\n",
       "      <td>2.943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiply</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedAdd</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model   High    Low    Rho  p-value  relative_diff\n",
       "0        Human  5.360  2.943  1.000    0.000          0.451\n",
       "1          Add  0.629  0.558  0.643    0.086          0.113\n",
       "2     Multiply  0.077  0.035  0.571    0.139          0.541\n",
       "3  WeightedAdd  0.157  0.131  0.690    0.058          0.167\n",
       "4     Combined  0.054  0.053  0.381    0.352          0.018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Space: svdspace_10k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Rho</th>\n",
       "      <th>p-value</th>\n",
       "      <th>relative_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human</td>\n",
       "      <td>5.360</td>\n",
       "      <td>2.943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiply</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedAdd</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-0.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model   High    Low    Rho  p-value  relative_diff\n",
       "0        Human  5.360  2.943  1.000    0.000          0.451\n",
       "1          Add  0.927  0.877  0.571    0.139          0.054\n",
       "2     Multiply  0.935  0.932  0.048    0.911          0.004\n",
       "3  WeightedAdd  0.703  0.645  0.405    0.320          0.083\n",
       "4     Combined  0.251  0.293 -0.095    0.823         -0.166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Space\n",
    "space = ppmispace_10k #we determined earlier that this was the best\n",
    "\n",
    "print('Space: space_10k')\n",
    "preprocessed_data = preprocess(data, space_10k)\n",
    "res = create_results(preprocessed_data, space_10k)\n",
    "display(res)\n",
    "print()\n",
    "\n",
    "print('Space: ppmispace_10k')\n",
    "preprocessed_data = preprocess(data, ppmispace_10k, verbose=False)\n",
    "res = create_results(preprocessed_data, ppmispace_10k)\n",
    "display(res)\n",
    "print()\n",
    "\n",
    "print('Space: svdspace_10k')\n",
    "preprocessed_data = preprocess(data, svdspace_10k, verbose=False)\n",
    "res = create_results(preprocessed_data, svdspace_10k)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:** (Isac) None of the models or represenations fall under the 5% p-value threshold. The closest is WeightedAdd (5.8%) and Add (8.6%) using PMI-space.  \n",
    "\n",
    "Surprisingly relative difference can be high but the p-value can still be poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "  - [1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23–25 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "  - [2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236–244). Association for Computational Linguistics.\n",
    "  \n",
    "  - [3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 60 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
