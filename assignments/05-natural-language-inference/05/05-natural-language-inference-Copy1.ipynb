{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "\n",
    "#other packages that we are going to use\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and unzip: https://github.com/sdobnik/computational-semantics/blob/master/assignments/05-natural-language-inference/simple_snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('simple_snli_1.0_train.csv', header=None, sep='\\t')\n",
    "train_data.columns = ['premise', 'hypothesis', 'relation']\n",
    "test_data = pd.read_csv('simple_snli_1.0_test.csv', header=None, sep='\\t')\n",
    "test_data.columns = ['premise', 'hypothesis', 'relation']\n",
    "dev_data = pd.read_csv('simple_snli_1.0_dev.csv', header=None, sep='\\t')\n",
    "dev_data.columns = ['premise', 'hypothesis', 'relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "        # The tokenizer was given as a whitespace tokenizer\n",
    "        return string.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implement a Dataset to keep track of vocab, word2idx, idx2word\n",
    "# Dataset can also be used in DataLoader which gives batch loading, etc, for free.\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, unk_label='<unk>', pad_label='<pad>'):\n",
    "        \n",
    "        self.unk_idx, self.unk_label = 0, unk_label\n",
    "        self.pad_idx, self.pad_label = 1, pad_label\n",
    "\n",
    "        self.data = data.copy()\n",
    "        self.data['premise'] = self.data['premise'].apply(self.tokenize)\n",
    "        self.data['hypothesis'] = self.data['hypothesis'].apply(self.tokenize)\n",
    "\n",
    "\n",
    "        self.vocab = self.__unique_words()\n",
    "        \n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word2idx[self.unk_label] = self.unk_idx\n",
    "        self.word2idx[self.pad_label] = self.pad_idx\n",
    "        self.word2idx.update({word:idx+max(self.word2idx.values())+1 for idx, word in enumerate(self.vocab)})\n",
    "\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "\n",
    "        self.labels = list(np.unique(self.data['relation']))\n",
    "\n",
    "    def __unique_words(self):\n",
    "        all_words = []\n",
    "        for s in self.data['premise']:\n",
    "            all_words += s\n",
    "        for s in self.data['hypothesis']:\n",
    "            all_words += s\n",
    "        return np.unique(all_words)\n",
    "        \n",
    "    def tokenize(self, string):\n",
    "        if isinstance(string, str): \n",
    "            # The tokenizer was given as a whitespace tokenizer\n",
    "            return string.lower().split()\n",
    "        else:  # for NaN\n",
    "            return \"<unk>\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #x = self.data.iloc[0] #for test\n",
    "        x = self.data.iloc[idx]\n",
    "        out = (x['premise'], x['hypothesis'], x['relation'])\n",
    "        return out\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InferenceDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane.'], ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.'], 'neutral')\n",
      "550152\n",
      "labels ['-', 'contradiction', 'entailment', 'neutral']\n",
      "56258\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(len(train_dataset))\n",
    "print(\"labels\", (train_dataset.labels))\n",
    "print(len(train_dataset.word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "relation_to_idx = {k:v for v,k in enumerate(sorted(np.unique(train_data['relation'])))}\n",
    "idx_relation = {v:k for k,v in relation_to_idx.items()}\n",
    "\n",
    "class Collate():\n",
    "    def __init__(self, word_to_idx, pad_idx=1, unk_idx=0, relation_to_idx=relation_to_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.unk_idx = unk_idx\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.relation_to_idx = relation_to_idx\n",
    "    def __call__(self, batch):\n",
    "        batch = np.transpose(batch)\n",
    "        \n",
    "        premises = np.transpose(batch[0])\n",
    "        premises = [torch.tensor([self.word_to_idx.get(w, self.unk_idx) for w in s], device=device) for s in premises]\n",
    "        premises = pad_sequence(premises, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        hypothesis = np.transpose(batch[1]) #batch first\n",
    "        hypothesis = [torch.tensor([self.word_to_idx.get(w, self.unk_idx) for w in s], device=device) for s in hypothesis]\n",
    "        hypothesis = pad_sequence(hypothesis, batch_first=True, padding_value=self.pad_idx)\n",
    "        \n",
    "        relations = [self.relation_to_idx[rel] for rel in batch[2]]\n",
    "\n",
    "        return premises, hypothesis, relations\n",
    "\n",
    "\n",
    "def dataloader(dataset, word2idx, pad_idx, unk_idx, batch_size=32, shuffle=True): # Need word2idx etc to match between train and test. Id probably do this is another wya in hindsight.\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=Collate(word2idx, pad_idx, unk_idx) )\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataloader(train_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8061, 0.2281, 0.5062, 0.6049, 0.7887],\n",
      "         [0.2986, 0.7589, 0.3305, 0.5077, 0.6510],\n",
      "         [0.6243, 0.8474, 0.4927, 0.0142, 0.0017]],\n",
      "\n",
      "        [[0.2019, 0.0107, 0.8083, 0.0365, 0.5661],\n",
      "         [0.1913, 0.8099, 0.0156, 0.5976, 0.6845],\n",
      "         [0.0220, 0.8118, 0.6238, 0.6299, 0.1341]],\n",
      "\n",
      "        [[0.1902, 0.2143, 0.8930, 0.1624, 0.5947],\n",
      "         [0.8394, 0.1865, 0.9838, 0.6364, 0.4700],\n",
      "         [0.7046, 0.0562, 0.7640, 0.4350, 0.6576]],\n",
      "\n",
      "        [[0.9125, 0.8473, 0.7251, 0.2109, 0.4149],\n",
      "         [0.6755, 0.2177, 0.6164, 0.5084, 0.3705],\n",
      "         [0.5049, 0.1899, 0.7938, 0.5299, 0.4353]],\n",
      "\n",
      "        [[0.4814, 0.3094, 0.8386, 0.5047, 0.4816],\n",
      "         [0.0770, 0.5450, 0.1650, 0.1687, 0.6292],\n",
      "         [0.4714, 0.7477, 0.4895, 0.9195, 0.0179]],\n",
      "\n",
      "        [[0.7469, 0.5814, 0.7323, 0.9269, 0.4896],\n",
      "         [0.7308, 0.0728, 0.1488, 0.7182, 0.4672],\n",
      "         [0.7275, 0.8991, 0.3934, 0.1995, 0.9469]],\n",
      "\n",
      "        [[0.9286, 0.7681, 0.3522, 0.0741, 0.5673],\n",
      "         [0.7792, 0.7142, 0.4630, 0.6053, 0.0915],\n",
      "         [0.2007, 0.7455, 0.5235, 0.9810, 0.0111]],\n",
      "\n",
      "        [[0.6589, 0.4528, 0.5201, 0.6184, 0.5839],\n",
      "         [0.0269, 0.8666, 0.3958, 0.2549, 0.1806],\n",
      "         [0.3155, 0.6060, 0.1713, 0.7905, 0.7835]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "dimensions = 5\n",
    "# A tensor for testing\n",
    "test_tensor = torch.rand([batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(input_tensor):\n",
    "    batches = len(input_tensor)\n",
    "    words = len(input_tensor[0])\n",
    "    dims = len(input_tensor[0][0])\n",
    "\n",
    "    new_tensors = []\n",
    "\n",
    "    for i in range(0,batches):\n",
    "        new_tensor = []\n",
    "\n",
    "        for j in range(0,dims):\n",
    "            temp_tensor = []\n",
    "        \n",
    "            for k in range(0,words):\n",
    "                temp_tensor.append(input_tensor[i][k][j])\n",
    "            \n",
    "            max_val = max(temp_tensor)\n",
    "            new_tensor.append(max_val)\n",
    "    \n",
    "        actual_new_tensor = new_tensor[0].unsqueeze(0)\n",
    "    \n",
    "        for l in range(1,len(new_tensor)):\n",
    "            actual_new_tensor = torch.cat((actual_new_tensor, new_tensor[l].unsqueeze(0)))\n",
    "\n",
    "        new_tensors.append(actual_new_tensor)\n",
    "    \n",
    "    output_tensor = torch.stack(new_tensors)\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8061, 0.8474, 0.5062, 0.6049, 0.7887],\n",
       "        [0.2019, 0.8118, 0.8083, 0.6299, 0.6845],\n",
       "        [0.8394, 0.2143, 0.9838, 0.6364, 0.6576],\n",
       "        [0.9125, 0.8473, 0.7938, 0.5299, 0.4353],\n",
       "        [0.4814, 0.7477, 0.8386, 0.9195, 0.6292],\n",
       "        [0.7469, 0.8991, 0.7323, 0.9269, 0.9469],\n",
       "        [0.9286, 0.7681, 0.5235, 0.9810, 0.5673],\n",
       "        [0.6589, 0.8666, 0.5201, 0.7905, 0.7835]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7854, 0.5951, 0.9977, 0.9915, 0.3051],\n",
      "        [0.6959, 0.7494, 0.8796, 0.8771, 0.6148],\n",
      "        [0.8451, 0.4234, 0.8094, 0.1745, 0.8213],\n",
      "        [0.7048, 0.8210, 0.9782, 0.8134, 0.5554],\n",
      "        [0.8686, 0.7258, 0.9975, 0.6675, 0.7278],\n",
      "        [0.9578, 0.8795, 0.6341, 0.7726, 0.9834],\n",
      "        [0.8506, 0.9707, 0.9655, 0.8623, 0.7997],\n",
      "        [0.6671, 0.7804, 0.6940, 0.4311, 0.6858]], dtype=torch.float64)\n",
      "tensor([[0.9325, 0.3408, 0.8460, 0.9052, 0.8642],\n",
      "        [0.6949, 0.7839, 0.8212, 0.7429, 0.9528],\n",
      "        [0.7171, 0.9547, 0.7250, 0.3005, 0.9764],\n",
      "        [0.6507, 0.8414, 0.9914, 0.6650, 0.9458],\n",
      "        [0.8940, 0.8808, 0.7559, 0.6979, 0.5078],\n",
      "        [0.9085, 0.6848, 0.8746, 0.9807, 0.7672],\n",
      "        [0.8020, 0.8979, 0.9812, 0.8814, 0.6962],\n",
      "        [0.7778, 0.7521, 0.9830, 0.3091, 0.9889]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Test tensors (size of batch, num, dim)\n",
    "t = torch.rand([2*batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "t1, t2 = torch.split(t, batch_size)\n",
    "# Pooled test tensors (size of batch, dim)\n",
    "pt1 = pooling(t1)\n",
    "pt2 = pooling(t2)\n",
    "print(pt1)\n",
    "print(pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    \n",
    "    batches = len(hypothesis)\n",
    "    dims = len(hypothesis[0])\n",
    "    final_dims = 4*dims\n",
    "\n",
    "    new_tensors = []\n",
    "\n",
    "    for i in range(0,batches):\n",
    "        hyp = hypothesis[i]\n",
    "        pre = premise[i]\n",
    "    \n",
    "        summed = torch.cat((pre,hyp))\n",
    "        subtracted = pre - hyp\n",
    "        multiplied = torch.mul(pre, hyp)\n",
    "    \n",
    "        new_tensors.append(torch.cat((summed, subtracted, multiplied)))\n",
    "    \n",
    "    output = torch.stack(new_tensors)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9325,  0.3408,  0.8460,  0.9052,  0.8642,  0.7854,  0.5951,  0.9977,\n",
       "          0.9915,  0.3051,  0.1471, -0.2544, -0.1516, -0.0863,  0.5591,  0.7324,\n",
       "          0.2028,  0.8441,  0.8975,  0.2637],\n",
       "        [ 0.6949,  0.7839,  0.8212,  0.7429,  0.9528,  0.6959,  0.7494,  0.8796,\n",
       "          0.8771,  0.6148, -0.0010,  0.0345, -0.0584, -0.1342,  0.3380,  0.4835,\n",
       "          0.5875,  0.7224,  0.6516,  0.5858],\n",
       "        [ 0.7171,  0.9547,  0.7250,  0.3005,  0.9764,  0.8451,  0.4234,  0.8094,\n",
       "          0.1745,  0.8213, -0.1280,  0.5313, -0.0844,  0.1260,  0.1552,  0.6060,\n",
       "          0.4042,  0.5868,  0.0524,  0.8019],\n",
       "        [ 0.6507,  0.8414,  0.9914,  0.6650,  0.9458,  0.7048,  0.8210,  0.9782,\n",
       "          0.8134,  0.5554, -0.0541,  0.0204,  0.0131, -0.1484,  0.3904,  0.4586,\n",
       "          0.6908,  0.9698,  0.5409,  0.5253],\n",
       "        [ 0.8940,  0.8808,  0.7559,  0.6979,  0.5078,  0.8686,  0.7258,  0.9975,\n",
       "          0.6675,  0.7278,  0.0253,  0.1550, -0.2416,  0.0305, -0.2200,  0.7765,\n",
       "          0.6392,  0.7540,  0.4658,  0.3696],\n",
       "        [ 0.9085,  0.6848,  0.8746,  0.9807,  0.7672,  0.9578,  0.8795,  0.6341,\n",
       "          0.7726,  0.9834, -0.0493, -0.1947,  0.2404,  0.2081, -0.2162,  0.8701,\n",
       "          0.6023,  0.5546,  0.7577,  0.7545],\n",
       "        [ 0.8020,  0.8979,  0.9812,  0.8814,  0.6962,  0.8506,  0.9707,  0.9655,\n",
       "          0.8623,  0.7997, -0.0486, -0.0728,  0.0157,  0.0191, -0.1035,  0.6821,\n",
       "          0.8715,  0.9474,  0.7600,  0.5568],\n",
       "        [ 0.7778,  0.7521,  0.9830,  0.3091,  0.9889,  0.6671,  0.7804,  0.6940,\n",
       "          0.4311,  0.6858,  0.1107, -0.0282,  0.2890, -0.1220,  0.3031,  0.5189,\n",
       "          0.5870,  0.6822,  0.1332,  0.6782]], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_premise_and_hypothesis(pt1, pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, word2idx, relation2idx, embedding_dim=32, hidden_size=128, padding_idx=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(word2idx)\n",
    "        self.output_dim = len(relation2idx)\n",
    "        self.hidden_size = hidden_size\n",
    "        # your code goes here\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, embedding_dim, padding_idx=padding_idx) #\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size*8, self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = self.embeddings(premise)\n",
    "        h = self.embeddings(hypothesis)\n",
    "        \n",
    "        lstm_p, (hidden, c) = self.LSTM(p)\n",
    "        lstm_h, (hidden, c) = self.LSTM(h)\n",
    "        \n",
    "        p_pooled = pooling(lstm_p)\n",
    "        h_pooled = pooling(lstm_h)\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(h_pooled,p_pooled)\n",
    "        ph_representation = self.dropout(ph_representation)  # is this at the right stage??\n",
    "        \n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InferenceDataset(train_data)\n",
    "test_dataset = InferenceDataset(test_data)\n",
    "dev_dataset = InferenceDataset(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember change batch size to 8\n",
    "#do we need a new loader for every epoch? it worked here\n",
    "loader = dataloader(train_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=5,shuffle=False)\n",
    "test_loader = dataloader(test_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=5,shuffle=False)\n",
    "dev_loader = dataloader(dev_dataset, train_dataset.word2idx, train_dataset.pad_idx, train_dataset.unk_idx, batch_size=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Loss = 1.40449\n",
      " Batch 10 : Average Loss = 1.29644\n",
      " Batch 20 : Average Loss = 1.25519\n",
      " Batch 30 : Average Loss = 1.21706\n",
      " Batch 40 : Average Loss = 1.21594\n",
      " Batch 50 : Average Loss = 1.20818\n",
      " Batch 60 : Average Loss = 1.20633\n",
      " Batch 70 : Average Loss = 1.20285\n",
      " Batch 80 : Average Loss = 1.20869\n",
      " Batch 90 : Average Loss = 1.21662\n",
      " Batch 100 : Average Loss = 1.2063\n",
      " Batch 110 : Average Loss = 1.20213\n",
      " Batch 120 : Average Loss = 1.19442\n",
      " Batch 130 : Average Loss = 1.19616\n",
      " Batch 140 : Average Loss = 1.18865\n",
      " Batch 150 : Average Loss = 1.20131\n",
      " Batch 160 : Average Loss = 1.19774\n",
      " Batch 170 : Average Loss = 1.19363\n",
      " Batch 180 : Average Loss = 1.19312\n",
      " Batch 190 : Average Loss = 1.18933\n",
      " Batch 200 : Average Loss = 1.19558\n",
      " Batch 210 : Average Loss = 1.19424\n",
      " Batch 220 : Average Loss = 1.19504\n",
      " Batch 230 : Average Loss = 1.19602\n",
      " Batch 240 : Average Loss = 1.19247\n",
      " Batch 250 : Average Loss = 1.18865\n",
      " Batch 260 : Average Loss = 1.18718\n",
      " Batch 270 : Average Loss = 1.18634\n",
      " Batch 280 : Average Loss = 1.18879\n",
      " Batch 290 : Average Loss = 1.18539\n",
      " Batch 300 : Average Loss = 1.18295\n",
      " Batch 310 : Average Loss = 1.17994\n",
      " Batch 320 : Average Loss = 1.17974\n",
      " Batch 330 : Average Loss = 1.17731\n",
      " Batch 340 : Average Loss = 1.17386\n",
      " Batch 350 : Average Loss = 1.17298\n",
      " Batch 360 : Average Loss = 1.17322\n",
      " Batch 370 : Average Loss = 1.17302\n",
      " Batch 380 : Average Loss = 1.1708\n",
      " Batch 390 : Average Loss = 1.16801\n",
      " Batch 400 : Average Loss = 1.16532\n",
      " Batch 410 : Average Loss = 1.16376\n",
      " Batch 420 : Average Loss = 1.16302\n",
      " Batch 430 : Average Loss = 1.16061\n",
      " Batch 440 : Average Loss = 1.15774\n",
      " Batch 450 : Average Loss = 1.15481\n",
      " Batch 460 : Average Loss = 1.15388\n",
      " Batch 470 : Average Loss = 1.15059\n",
      " Batch 480 : Average Loss = 1.14955\n",
      " Batch 490 : Average Loss = 1.14942\n",
      " Batch 500 : Average Loss = 1.14682\n",
      " Batch 510 : Average Loss = 1.14391\n",
      " Batch 520 : Average Loss = 1.14278\n",
      " Batch 530 : Average Loss = 1.14216\n",
      " Batch 540 : Average Loss = 1.142\n",
      " Batch 550 : Average Loss = 1.14327\n",
      " Batch 560 : Average Loss = 1.14172\n",
      " Batch 570 : Average Loss = 1.14707\n",
      " Batch 580 : Average Loss = 1.14862\n",
      " Batch 590 : Average Loss = 1.14781\n",
      " Batch 600 : Average Loss = 1.14762\n",
      " Batch 610 : Average Loss = 1.14644\n",
      " Batch 620 : Average Loss = 1.14355\n",
      " Batch 630 : Average Loss = 1.14275\n",
      " Batch 640 : Average Loss = 1.14341\n",
      " Batch 650 : Average Loss = 1.1421\n",
      " Batch 660 : Average Loss = 1.14122\n",
      " Batch 670 : Average Loss = 1.14247\n",
      " Batch 680 : Average Loss = 1.14088\n",
      " Batch 690 : Average Loss = 1.14119\n",
      " Batch 700 : Average Loss = 1.13995\n",
      " Batch 710 : Average Loss = 1.1405\n",
      " Batch 720 : Average Loss = 1.1404\n",
      " Batch 730 : Average Loss = 1.13906\n",
      " Batch 740 : Average Loss = 1.13981\n",
      " Batch 750 : Average Loss = 1.1388\n",
      " Batch 760 : Average Loss = 1.13788\n",
      " Batch 770 : Average Loss = 1.13742\n",
      " Batch 780 : Average Loss = 1.13775\n",
      " Batch 790 : Average Loss = 1.13879\n",
      " Batch 800 : Average Loss = 1.13806\n",
      " Batch 810 : Average Loss = 1.13758\n",
      " Batch 820 : Average Loss = 1.1378\n",
      " Batch 830 : Average Loss = 1.13846\n",
      " Batch 840 : Average Loss = 1.13909\n",
      " Batch 850 : Average Loss = 1.1397\n",
      " Batch 860 : Average Loss = 1.14123\n",
      " Batch 870 : Average Loss = 1.14073\n",
      " Batch 880 : Average Loss = 1.13958\n",
      " Batch 890 : Average Loss = 1.14087\n",
      " Batch 900 : Average Loss = 1.14064\n",
      " Batch 910 : Average Loss = 1.14031\n",
      " Batch 920 : Average Loss = 1.14138\n",
      " Batch 930 : Average Loss = 1.13965\n",
      " Batch 940 : Average Loss = 1.13781\n",
      " Batch 950 : Average Loss = 1.13898\n",
      " Batch 960 : Average Loss = 1.13775\n",
      " Batch 970 : Average Loss = 1.13706\n",
      " Batch 980 : Average Loss = 1.1359\n",
      " Batch 990 : Average Loss = 1.13511\n",
      " Batch 1000 : Average Loss = 1.13417\n",
      " Batch 1010 : Average Loss = 1.13304\n",
      " Batch 1020 : Average Loss = 1.13221\n",
      " Batch 1030 : Average Loss = 1.13285\n",
      " Batch 1040 : Average Loss = 1.13256\n",
      " Batch 1050 : Average Loss = 1.13239\n",
      " Batch 1060 : Average Loss = 1.13103\n",
      " Batch 1070 : Average Loss = 1.13048\n",
      " Batch 1080 : Average Loss = 1.13196\n",
      " Batch 1090 : Average Loss = 1.13109\n",
      " Batch 1100 : Average Loss = 1.132\n",
      " Batch 1110 : Average Loss = 1.13136\n",
      " Batch 1120 : Average Loss = 1.13098\n",
      " Batch 1130 : Average Loss = 1.13137\n",
      " Batch 1140 : Average Loss = 1.13085\n",
      " Batch 1150 : Average Loss = 1.13062\n",
      " Batch 1160 : Average Loss = 1.12965\n",
      " Batch 1170 : Average Loss = 1.12886\n",
      " Batch 1180 : Average Loss = 1.12786\n",
      " Batch 1190 : Average Loss = 1.12696\n",
      " Batch 1200 : Average Loss = 1.1282\n",
      " Batch 1210 : Average Loss = 1.12783\n",
      " Batch 1220 : Average Loss = 1.12663\n",
      " Batch 1230 : Average Loss = 1.12568\n",
      " Batch 1240 : Average Loss = 1.12457\n",
      " Batch 1250 : Average Loss = 1.12393\n",
      " Batch 1260 : Average Loss = 1.12409\n",
      " Batch 1270 : Average Loss = 1.12553\n",
      " Batch 1280 : Average Loss = 1.12474\n",
      " Batch 1290 : Average Loss = 1.1245\n",
      " Batch 1300 : Average Loss = 1.12431\n",
      " Batch 1310 : Average Loss = 1.12425\n",
      " Batch 1320 : Average Loss = 1.12321\n",
      " Batch 1330 : Average Loss = 1.12269\n",
      " Batch 1340 : Average Loss = 1.12275\n",
      " Batch 1350 : Average Loss = 1.12195\n",
      " Batch 1360 : Average Loss = 1.12104\n",
      " Batch 1370 : Average Loss = 1.12159\n",
      " Batch 1380 : Average Loss = 1.12074\n",
      " Batch 1390 : Average Loss = 1.11952\n",
      " Batch 1400 : Average Loss = 1.11856\n",
      " Batch 1410 : Average Loss = 1.11772\n",
      " Batch 1420 : Average Loss = 1.1174\n",
      " Batch 1430 : Average Loss = 1.11708\n",
      " Batch 1440 : Average Loss = 1.11582\n",
      " Batch 1450 : Average Loss = 1.1166\n",
      " Batch 1460 : Average Loss = 1.11579\n",
      " Batch 1470 : Average Loss = 1.11554\n",
      " Batch 1480 : Average Loss = 1.11461\n",
      " Batch 1490 : Average Loss = 1.11509\n",
      " Batch 1500 : Average Loss = 1.11587\n",
      " Batch 1510 : Average Loss = 1.11609\n",
      " Batch 1520 : Average Loss = 1.1159\n",
      " Batch 1530 : Average Loss = 1.11533\n",
      " Batch 1540 : Average Loss = 1.11482\n",
      " Batch 1550 : Average Loss = 1.11496\n",
      " Batch 1560 : Average Loss = 1.11442\n",
      " Batch 1570 : Average Loss = 1.11429\n",
      " Batch 1580 : Average Loss = 1.11435\n",
      " Batch 1590 : Average Loss = 1.11393\n",
      " Batch 1600 : Average Loss = 1.11459\n",
      " Batch 1610 : Average Loss = 1.11456\n",
      " Batch 1620 : Average Loss = 1.11429\n",
      " Batch 1630 : Average Loss = 1.11318\n",
      " Batch 1640 : Average Loss = 1.11258\n",
      " Batch 1650 : Average Loss = 1.1122\n",
      " Batch 1660 : Average Loss = 1.11131\n",
      " Batch 1670 : Average Loss = 1.11095\n",
      " Batch 1680 : Average Loss = 1.11212\n",
      " Batch 1690 : Average Loss = 1.11168\n",
      " Batch 1700 : Average Loss = 1.11252\n",
      " Batch 1710 : Average Loss = 1.11147\n",
      " Batch 1720 : Average Loss = 1.11085\n",
      " Batch 1730 : Average Loss = 1.1103\n",
      " Batch 1740 : Average Loss = 1.10996\n",
      " Batch 1750 : Average Loss = 1.11015\n",
      " Batch 1760 : Average Loss = 1.11086\n",
      " Batch 1770 : Average Loss = 1.11012\n",
      " Batch 1780 : Average Loss = 1.10958\n",
      " Batch 1790 : Average Loss = 1.10885\n",
      " Batch 1800 : Average Loss = 1.10761\n",
      " Batch 1810 : Average Loss = 1.10735\n",
      " Batch 1820 : Average Loss = 1.10707\n",
      " Batch 1830 : Average Loss = 1.10695\n",
      " Batch 1840 : Average Loss = 1.10699\n",
      " Batch 1850 : Average Loss = 1.10622\n",
      " Batch 1860 : Average Loss = 1.10627\n",
      " Batch 1870 : Average Loss = 1.10558\n",
      " Batch 1880 : Average Loss = 1.10594\n",
      " Batch 1890 : Average Loss = 1.10538\n",
      " Batch 1900 : Average Loss = 1.10408\n",
      " Batch 1910 : Average Loss = 1.10337\n",
      " Batch 1920 : Average Loss = 1.10306\n",
      " Batch 1930 : Average Loss = 1.10312\n",
      " Batch 1940 : Average Loss = 1.10235\n",
      " Batch 1950 : Average Loss = 1.10191\n",
      " Batch 1960 : Average Loss = 1.10141\n",
      " Batch 1970 : Average Loss = 1.10251\n",
      " Batch 1980 : Average Loss = 1.10215\n",
      " Batch 1990 : Average Loss = 1.10141\n",
      " Batch 0 : Average Loss = 0.85987\n",
      " Batch 10 : Average Loss = 1.02259\n",
      " Batch 20 : Average Loss = 1.0391\n",
      " Batch 30 : Average Loss = 1.02831\n",
      " Batch 40 : Average Loss = 1.04127\n",
      " Batch 50 : Average Loss = 1.0342\n",
      " Batch 60 : Average Loss = 1.04513\n",
      " Batch 70 : Average Loss = 1.04747\n",
      " Batch 80 : Average Loss = 1.05583\n",
      " Batch 90 : Average Loss = 1.05597\n",
      " Batch 100 : Average Loss = 1.05183\n",
      " Batch 110 : Average Loss = 1.0489\n",
      " Batch 120 : Average Loss = 1.03505\n",
      " Batch 130 : Average Loss = 1.03818\n",
      " Batch 140 : Average Loss = 1.02682\n",
      " Batch 150 : Average Loss = 1.04329\n",
      " Batch 160 : Average Loss = 1.04881\n",
      " Batch 170 : Average Loss = 1.04437\n",
      " Batch 180 : Average Loss = 1.04719\n",
      " Batch 190 : Average Loss = 1.04445\n",
      " Batch 200 : Average Loss = 1.05732\n",
      " Batch 210 : Average Loss = 1.05928\n",
      " Batch 220 : Average Loss = 1.06004\n",
      " Batch 230 : Average Loss = 1.06398\n",
      " Batch 240 : Average Loss = 1.06001\n",
      " Batch 250 : Average Loss = 1.05144\n",
      " Batch 260 : Average Loss = 1.0493\n",
      " Batch 270 : Average Loss = 1.04777\n",
      " Batch 280 : Average Loss = 1.04705\n",
      " Batch 290 : Average Loss = 1.04639\n",
      " Batch 300 : Average Loss = 1.04431\n",
      " Batch 310 : Average Loss = 1.04385\n",
      " Batch 320 : Average Loss = 1.04429\n",
      " Batch 330 : Average Loss = 1.03959\n",
      " Batch 340 : Average Loss = 1.03639\n",
      " Batch 350 : Average Loss = 1.03566\n",
      " Batch 360 : Average Loss = 1.03727\n",
      " Batch 370 : Average Loss = 1.03769\n",
      " Batch 380 : Average Loss = 1.0347\n",
      " Batch 390 : Average Loss = 1.03323\n",
      " Batch 400 : Average Loss = 1.02967\n",
      " Batch 410 : Average Loss = 1.02945\n",
      " Batch 420 : Average Loss = 1.02966\n",
      " Batch 430 : Average Loss = 1.02835\n",
      " Batch 440 : Average Loss = 1.028\n",
      " Batch 450 : Average Loss = 1.02578\n",
      " Batch 460 : Average Loss = 1.02416\n",
      " Batch 470 : Average Loss = 1.02233\n",
      " Batch 480 : Average Loss = 1.02063\n",
      " Batch 490 : Average Loss = 1.0201\n",
      " Batch 500 : Average Loss = 1.01472\n",
      " Batch 510 : Average Loss = 1.01123\n",
      " Batch 520 : Average Loss = 1.01128\n",
      " Batch 530 : Average Loss = 1.01086\n",
      " Batch 540 : Average Loss = 1.01123\n",
      " Batch 550 : Average Loss = 1.01052\n",
      " Batch 560 : Average Loss = 1.00961\n",
      " Batch 570 : Average Loss = 1.01506\n",
      " Batch 580 : Average Loss = 1.01776\n",
      " Batch 590 : Average Loss = 1.01613\n",
      " Batch 600 : Average Loss = 1.01561\n",
      " Batch 610 : Average Loss = 1.01353\n",
      " Batch 620 : Average Loss = 1.01043\n",
      " Batch 630 : Average Loss = 1.00912\n",
      " Batch 640 : Average Loss = 1.00981\n",
      " Batch 650 : Average Loss = 1.00934\n",
      " Batch 660 : Average Loss = 1.00925\n",
      " Batch 670 : Average Loss = 1.01112\n",
      " Batch 680 : Average Loss = 1.00941\n",
      " Batch 690 : Average Loss = 1.0093\n",
      " Batch 700 : Average Loss = 1.00757\n",
      " Batch 710 : Average Loss = 1.00808\n",
      " Batch 720 : Average Loss = 1.0071\n",
      " Batch 730 : Average Loss = 1.0047\n",
      " Batch 740 : Average Loss = 1.00715\n",
      " Batch 750 : Average Loss = 1.00643\n",
      " Batch 760 : Average Loss = 1.00676\n",
      " Batch 770 : Average Loss = 1.00508\n",
      " Batch 780 : Average Loss = 1.00612\n",
      " Batch 790 : Average Loss = 1.00846\n",
      " Batch 800 : Average Loss = 1.00796\n",
      " Batch 810 : Average Loss = 1.0078\n",
      " Batch 820 : Average Loss = 1.00832\n",
      " Batch 830 : Average Loss = 1.00954\n",
      " Batch 840 : Average Loss = 1.00902\n",
      " Batch 850 : Average Loss = 1.00822\n",
      " Batch 860 : Average Loss = 1.0102\n",
      " Batch 870 : Average Loss = 1.00988\n",
      " Batch 880 : Average Loss = 1.00855\n",
      " Batch 890 : Average Loss = 1.00956\n",
      " Batch 900 : Average Loss = 1.00862\n",
      " Batch 910 : Average Loss = 1.00849\n",
      " Batch 920 : Average Loss = 1.01005\n",
      " Batch 930 : Average Loss = 1.00797\n",
      " Batch 940 : Average Loss = 1.00459\n",
      " Batch 950 : Average Loss = 1.00608\n",
      " Batch 960 : Average Loss = 1.0047\n",
      " Batch 970 : Average Loss = 1.00461\n",
      " Batch 980 : Average Loss = 1.00404\n",
      " Batch 990 : Average Loss = 1.00266\n",
      " Batch 1000 : Average Loss = 1.00167\n",
      " Batch 1010 : Average Loss = 1.00069\n",
      " Batch 1020 : Average Loss = 0.99942\n",
      " Batch 1030 : Average Loss = 1.00019\n",
      " Batch 1040 : Average Loss = 0.99977\n",
      " Batch 1050 : Average Loss = 1.00065\n",
      " Batch 1060 : Average Loss = 0.99949\n",
      " Batch 1070 : Average Loss = 0.99952\n",
      " Batch 1080 : Average Loss = 1.00042\n",
      " Batch 1090 : Average Loss = 0.99944\n",
      " Batch 1100 : Average Loss = 1.00064\n",
      " Batch 1110 : Average Loss = 0.99969\n",
      " Batch 1120 : Average Loss = 0.99976\n",
      " Batch 1130 : Average Loss = 1.0013\n",
      " Batch 1140 : Average Loss = 1.00047\n",
      " Batch 1150 : Average Loss = 0.99966\n",
      " Batch 1160 : Average Loss = 0.99854\n",
      " Batch 1170 : Average Loss = 0.99742\n",
      " Batch 1180 : Average Loss = 0.99691\n",
      " Batch 1190 : Average Loss = 0.99554\n",
      " Batch 1200 : Average Loss = 0.99676\n",
      " Batch 1210 : Average Loss = 0.9963\n",
      " Batch 1220 : Average Loss = 0.9952\n",
      " Batch 1230 : Average Loss = 0.9947\n",
      " Batch 1240 : Average Loss = 0.99303\n",
      " Batch 1250 : Average Loss = 0.99222\n",
      " Batch 1260 : Average Loss = 0.99186\n",
      " Batch 1270 : Average Loss = 0.99281\n",
      " Batch 1280 : Average Loss = 0.99178\n",
      " Batch 1290 : Average Loss = 0.99182\n",
      " Batch 1300 : Average Loss = 0.99155\n",
      " Batch 1310 : Average Loss = 0.99151\n",
      " Batch 1320 : Average Loss = 0.98993\n",
      " Batch 1330 : Average Loss = 0.98936\n",
      " Batch 1340 : Average Loss = 0.99017\n",
      " Batch 1350 : Average Loss = 0.98912\n",
      " Batch 1360 : Average Loss = 0.98848\n",
      " Batch 1370 : Average Loss = 0.98862\n",
      " Batch 1380 : Average Loss = 0.98745\n",
      " Batch 1390 : Average Loss = 0.9859\n",
      " Batch 1400 : Average Loss = 0.98495\n",
      " Batch 1410 : Average Loss = 0.9838\n",
      " Batch 1420 : Average Loss = 0.9833\n",
      " Batch 1430 : Average Loss = 0.98299\n",
      " Batch 1440 : Average Loss = 0.98151\n",
      " Batch 1450 : Average Loss = 0.98218\n",
      " Batch 1460 : Average Loss = 0.98156\n",
      " Batch 1470 : Average Loss = 0.9819\n",
      " Batch 1480 : Average Loss = 0.98082\n",
      " Batch 1490 : Average Loss = 0.98121\n",
      " Batch 1500 : Average Loss = 0.98216\n",
      " Batch 1510 : Average Loss = 0.98204\n",
      " Batch 1520 : Average Loss = 0.98132\n",
      " Batch 1530 : Average Loss = 0.9803\n",
      " Batch 1540 : Average Loss = 0.97968\n",
      " Batch 1550 : Average Loss = 0.97972\n",
      " Batch 1560 : Average Loss = 0.97896\n",
      " Batch 1570 : Average Loss = 0.97922\n",
      " Batch 1580 : Average Loss = 0.97904\n",
      " Batch 1590 : Average Loss = 0.97869\n",
      " Batch 1600 : Average Loss = 0.97951\n",
      " Batch 1610 : Average Loss = 0.9794\n",
      " Batch 1620 : Average Loss = 0.9792\n",
      " Batch 1630 : Average Loss = 0.9776\n",
      " Batch 1640 : Average Loss = 0.97678\n",
      " Batch 1650 : Average Loss = 0.97665\n",
      " Batch 1660 : Average Loss = 0.97558\n",
      " Batch 1670 : Average Loss = 0.97486\n",
      " Batch 1680 : Average Loss = 0.97586\n",
      " Batch 1690 : Average Loss = 0.97521\n",
      " Batch 1700 : Average Loss = 0.97612\n",
      " Batch 1710 : Average Loss = 0.9751\n",
      " Batch 1720 : Average Loss = 0.97489\n",
      " Batch 1730 : Average Loss = 0.97438\n",
      " Batch 1740 : Average Loss = 0.97448\n",
      " Batch 1750 : Average Loss = 0.97468\n",
      " Batch 1760 : Average Loss = 0.97517\n",
      " Batch 1770 : Average Loss = 0.97422\n",
      " Batch 1780 : Average Loss = 0.97353\n",
      " Batch 1790 : Average Loss = 0.97242\n",
      " Batch 1800 : Average Loss = 0.97144\n",
      " Batch 1810 : Average Loss = 0.9712\n",
      " Batch 1820 : Average Loss = 0.97087\n",
      " Batch 1830 : Average Loss = 0.97093\n",
      " Batch 1840 : Average Loss = 0.97111\n",
      " Batch 1850 : Average Loss = 0.97013\n",
      " Batch 1860 : Average Loss = 0.96997\n",
      " Batch 1870 : Average Loss = 0.96902\n",
      " Batch 1880 : Average Loss = 0.96916\n",
      " Batch 1890 : Average Loss = 0.9685\n",
      " Batch 1900 : Average Loss = 0.96686\n",
      " Batch 1910 : Average Loss = 0.96613\n",
      " Batch 1920 : Average Loss = 0.96576\n",
      " Batch 1930 : Average Loss = 0.96603\n",
      " Batch 1940 : Average Loss = 0.96554\n",
      " Batch 1950 : Average Loss = 0.96461\n",
      " Batch 1960 : Average Loss = 0.96421\n",
      " Batch 1970 : Average Loss = 0.96532\n",
      " Batch 1980 : Average Loss = 0.96509\n",
      " Batch 1990 : Average Loss = 0.96409\n",
      " Batch 0 : Average Loss = 0.72565\n",
      " Batch 10 : Average Loss = 0.88771\n",
      " Batch 20 : Average Loss = 0.90345\n",
      " Batch 30 : Average Loss = 0.8848\n",
      " Batch 40 : Average Loss = 0.8923\n",
      " Batch 50 : Average Loss = 0.87055\n",
      " Batch 60 : Average Loss = 0.87943\n",
      " Batch 70 : Average Loss = 0.89933\n",
      " Batch 80 : Average Loss = 0.90123\n",
      " Batch 90 : Average Loss = 0.89426\n",
      " Batch 100 : Average Loss = 0.89216\n",
      " Batch 110 : Average Loss = 0.88344\n",
      " Batch 120 : Average Loss = 0.8702\n",
      " Batch 130 : Average Loss = 0.87688\n",
      " Batch 140 : Average Loss = 0.86107\n",
      " Batch 150 : Average Loss = 0.88083\n",
      " Batch 160 : Average Loss = 0.88447\n",
      " Batch 170 : Average Loss = 0.87917\n",
      " Batch 180 : Average Loss = 0.88519\n",
      " Batch 190 : Average Loss = 0.88327\n",
      " Batch 200 : Average Loss = 0.90511\n",
      " Batch 210 : Average Loss = 0.91252\n",
      " Batch 220 : Average Loss = 0.90942\n",
      " Batch 230 : Average Loss = 0.91808\n",
      " Batch 240 : Average Loss = 0.91553\n",
      " Batch 250 : Average Loss = 0.90308\n",
      " Batch 260 : Average Loss = 0.89986\n",
      " Batch 270 : Average Loss = 0.89894\n",
      " Batch 280 : Average Loss = 0.89888\n",
      " Batch 290 : Average Loss = 0.89914\n",
      " Batch 300 : Average Loss = 0.89437\n",
      " Batch 310 : Average Loss = 0.892\n",
      " Batch 320 : Average Loss = 0.89182\n",
      " Batch 330 : Average Loss = 0.88654\n",
      " Batch 340 : Average Loss = 0.88481\n",
      " Batch 350 : Average Loss = 0.88467\n",
      " Batch 360 : Average Loss = 0.88824\n",
      " Batch 370 : Average Loss = 0.88931\n",
      " Batch 380 : Average Loss = 0.88589\n",
      " Batch 390 : Average Loss = 0.88444\n",
      " Batch 400 : Average Loss = 0.87872\n",
      " Batch 410 : Average Loss = 0.8803\n",
      " Batch 420 : Average Loss = 0.88174\n",
      " Batch 430 : Average Loss = 0.88232\n",
      " Batch 440 : Average Loss = 0.88394\n",
      " Batch 450 : Average Loss = 0.88221\n",
      " Batch 460 : Average Loss = 0.87902\n",
      " Batch 470 : Average Loss = 0.87607\n",
      " Batch 480 : Average Loss = 0.87433\n",
      " Batch 490 : Average Loss = 0.87319\n",
      " Batch 500 : Average Loss = 0.8664\n",
      " Batch 510 : Average Loss = 0.86236\n",
      " Batch 520 : Average Loss = 0.86205\n",
      " Batch 530 : Average Loss = 0.86225\n",
      " Batch 540 : Average Loss = 0.86287\n",
      " Batch 550 : Average Loss = 0.86048\n",
      " Batch 560 : Average Loss = 0.85815\n",
      " Batch 570 : Average Loss = 0.86287\n",
      " Batch 580 : Average Loss = 0.86532\n",
      " Batch 590 : Average Loss = 0.86321\n",
      " Batch 600 : Average Loss = 0.86128\n",
      " Batch 610 : Average Loss = 0.85859\n",
      " Batch 620 : Average Loss = 0.85605\n",
      " Batch 630 : Average Loss = 0.85243\n",
      " Batch 640 : Average Loss = 0.85344\n",
      " Batch 650 : Average Loss = 0.85334\n",
      " Batch 660 : Average Loss = 0.85433\n",
      " Batch 670 : Average Loss = 0.85556\n",
      " Batch 680 : Average Loss = 0.85483\n",
      " Batch 690 : Average Loss = 0.85524\n",
      " Batch 700 : Average Loss = 0.85349\n",
      " Batch 710 : Average Loss = 0.85382\n",
      " Batch 720 : Average Loss = 0.85301\n",
      " Batch 730 : Average Loss = 0.85044\n",
      " Batch 740 : Average Loss = 0.85368\n",
      " Batch 750 : Average Loss = 0.85304\n",
      " Batch 760 : Average Loss = 0.85332\n",
      " Batch 770 : Average Loss = 0.85219\n",
      " Batch 780 : Average Loss = 0.85392\n",
      " Batch 790 : Average Loss = 0.85712\n",
      " Batch 800 : Average Loss = 0.85569\n",
      " Batch 810 : Average Loss = 0.85529\n",
      " Batch 820 : Average Loss = 0.85654\n",
      " Batch 830 : Average Loss = 0.85695\n",
      " Batch 840 : Average Loss = 0.85676\n",
      " Batch 850 : Average Loss = 0.85565\n",
      " Batch 860 : Average Loss = 0.85728\n",
      " Batch 870 : Average Loss = 0.85644\n",
      " Batch 880 : Average Loss = 0.85503\n",
      " Batch 890 : Average Loss = 0.85501\n",
      " Batch 900 : Average Loss = 0.8543\n",
      " Batch 910 : Average Loss = 0.85414\n",
      " Batch 920 : Average Loss = 0.85588\n",
      " Batch 930 : Average Loss = 0.85384\n",
      " Batch 940 : Average Loss = 0.85034\n",
      " Batch 950 : Average Loss = 0.85119\n",
      " Batch 960 : Average Loss = 0.84914\n",
      " Batch 970 : Average Loss = 0.84969\n",
      " Batch 980 : Average Loss = 0.84825\n",
      " Batch 990 : Average Loss = 0.84724\n",
      " Batch 1000 : Average Loss = 0.84635\n",
      " Batch 1010 : Average Loss = 0.84517\n",
      " Batch 1020 : Average Loss = 0.84416\n",
      " Batch 1030 : Average Loss = 0.84433\n",
      " Batch 1040 : Average Loss = 0.84431\n",
      " Batch 1050 : Average Loss = 0.84534\n",
      " Batch 1060 : Average Loss = 0.8433\n",
      " Batch 1070 : Average Loss = 0.84312\n",
      " Batch 1080 : Average Loss = 0.84473\n",
      " Batch 1090 : Average Loss = 0.84321\n",
      " Batch 1100 : Average Loss = 0.84385\n",
      " Batch 1110 : Average Loss = 0.84233\n",
      " Batch 1120 : Average Loss = 0.84285\n",
      " Batch 1130 : Average Loss = 0.84481\n",
      " Batch 1140 : Average Loss = 0.84405\n",
      " Batch 1150 : Average Loss = 0.84304\n",
      " Batch 1160 : Average Loss = 0.84171\n",
      " Batch 1170 : Average Loss = 0.84054\n",
      " Batch 1180 : Average Loss = 0.83961\n",
      " Batch 1190 : Average Loss = 0.83815\n",
      " Batch 1200 : Average Loss = 0.83943\n",
      " Batch 1210 : Average Loss = 0.83879\n",
      " Batch 1220 : Average Loss = 0.83766\n",
      " Batch 1230 : Average Loss = 0.8369\n",
      " Batch 1240 : Average Loss = 0.83488\n",
      " Batch 1250 : Average Loss = 0.8343\n",
      " Batch 1260 : Average Loss = 0.83361\n",
      " Batch 1270 : Average Loss = 0.83439\n",
      " Batch 1280 : Average Loss = 0.83313\n",
      " Batch 1290 : Average Loss = 0.83318\n",
      " Batch 1300 : Average Loss = 0.83295\n",
      " Batch 1310 : Average Loss = 0.83265\n",
      " Batch 1320 : Average Loss = 0.831\n",
      " Batch 1330 : Average Loss = 0.83042\n",
      " Batch 1340 : Average Loss = 0.83084\n",
      " Batch 1350 : Average Loss = 0.82953\n",
      " Batch 1360 : Average Loss = 0.82823\n",
      " Batch 1370 : Average Loss = 0.82819\n",
      " Batch 1380 : Average Loss = 0.82694\n",
      " Batch 1390 : Average Loss = 0.82591\n",
      " Batch 1400 : Average Loss = 0.8246\n",
      " Batch 1410 : Average Loss = 0.82383\n",
      " Batch 1420 : Average Loss = 0.82331\n",
      " Batch 1430 : Average Loss = 0.82262\n",
      " Batch 1440 : Average Loss = 0.82127\n",
      " Batch 1450 : Average Loss = 0.82138\n",
      " Batch 1460 : Average Loss = 0.82088\n",
      " Batch 1470 : Average Loss = 0.82127\n",
      " Batch 1480 : Average Loss = 0.82008\n",
      " Batch 1490 : Average Loss = 0.81999\n",
      " Batch 1500 : Average Loss = 0.82039\n",
      " Batch 1510 : Average Loss = 0.81968\n",
      " Batch 1520 : Average Loss = 0.81877\n",
      " Batch 1530 : Average Loss = 0.81771\n",
      " Batch 1540 : Average Loss = 0.81728\n",
      " Batch 1550 : Average Loss = 0.81701\n",
      " Batch 1560 : Average Loss = 0.81601\n",
      " Batch 1570 : Average Loss = 0.81647\n",
      " Batch 1580 : Average Loss = 0.81605\n",
      " Batch 1590 : Average Loss = 0.81555\n",
      " Batch 1600 : Average Loss = 0.81614\n",
      " Batch 1610 : Average Loss = 0.8156\n",
      " Batch 1620 : Average Loss = 0.81479\n",
      " Batch 1630 : Average Loss = 0.81276\n",
      " Batch 1640 : Average Loss = 0.8124\n",
      " Batch 1650 : Average Loss = 0.81208\n",
      " Batch 1660 : Average Loss = 0.81085\n",
      " Batch 1670 : Average Loss = 0.80961\n",
      " Batch 1680 : Average Loss = 0.81048\n",
      " Batch 1690 : Average Loss = 0.80993\n",
      " Batch 1700 : Average Loss = 0.81062\n",
      " Batch 1710 : Average Loss = 0.80945\n",
      " Batch 1720 : Average Loss = 0.80942\n",
      " Batch 1730 : Average Loss = 0.80856\n",
      " Batch 1740 : Average Loss = 0.80885\n",
      " Batch 1750 : Average Loss = 0.80868\n",
      " Batch 1760 : Average Loss = 0.80881\n",
      " Batch 1770 : Average Loss = 0.8081\n",
      " Batch 1780 : Average Loss = 0.80741\n",
      " Batch 1790 : Average Loss = 0.80626\n",
      " Batch 1800 : Average Loss = 0.80547\n",
      " Batch 1810 : Average Loss = 0.80516\n",
      " Batch 1820 : Average Loss = 0.80472\n",
      " Batch 1830 : Average Loss = 0.8049\n",
      " Batch 1840 : Average Loss = 0.8053\n",
      " Batch 1850 : Average Loss = 0.80448\n",
      " Batch 1860 : Average Loss = 0.80454\n",
      " Batch 1870 : Average Loss = 0.80363\n",
      " Batch 1880 : Average Loss = 0.80328\n",
      " Batch 1890 : Average Loss = 0.80229\n",
      " Batch 1900 : Average Loss = 0.80097\n",
      " Batch 1910 : Average Loss = 0.8006\n",
      " Batch 1920 : Average Loss = 0.80027\n",
      " Batch 1930 : Average Loss = 0.8002\n",
      " Batch 1940 : Average Loss = 0.79986\n",
      " Batch 1950 : Average Loss = 0.7989\n",
      " Batch 1960 : Average Loss = 0.79859\n",
      " Batch 1970 : Average Loss = 0.79906\n",
      " Batch 1980 : Average Loss = 0.79873\n",
      " Batch 1990 : Average Loss = 0.79808\n"
     ]
    }
   ],
   "source": [
    "# train_iter, test_iter = dataloader(path_to_folder)\n",
    "# train, dev, test, premises, hypotheses, relations = dataloader('./')\n",
    "\n",
    "model = SNLIModel(train_dataset.vocab, train_dataset.labels).to(device)\n",
    "loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train model\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dev_loader):\n",
    "        prems = batch[0]\n",
    "        hyps = batch[1]\n",
    "        rels = torch.Tensor(batch[2]).long().to(device)\n",
    "\n",
    "        output = model(prems, hyps)\n",
    "        \n",
    "        loss = loss_function(output, rels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i%10==0:\n",
    "            print(f' Batch {i} : Average Loss = {round(total_loss/(i+1),5)}')#, end='\\r')\n",
    "            \n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "# test model after all epochs are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'inference_dev.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#so that when we load it back in, we can have access to the same word2idx etc.\n",
    "with open(\"train_dataset.pickle\",\"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_dataset.pickle\", 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "model = torch.load('inference_dev.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0 : Average Test Accuracy = 0.4\n",
      " Batch 10 : Average Test Accuracy = 0.50909\n",
      " Batch 20 : Average Test Accuracy = 0.51429\n",
      " Batch 30 : Average Test Accuracy = 0.52903\n",
      " Batch 40 : Average Test Accuracy = 0.54634\n",
      " Batch 50 : Average Test Accuracy = 0.53333\n",
      " Batch 60 : Average Test Accuracy = 0.54426\n",
      " Batch 70 : Average Test Accuracy = 0.52113\n",
      " Batch 80 : Average Test Accuracy = 0.5358\n",
      " Batch 90 : Average Test Accuracy = 0.53846\n",
      " Batch 100 : Average Test Accuracy = 0.52475\n",
      " Batch 110 : Average Test Accuracy = 0.52793\n",
      " Batch 120 : Average Test Accuracy = 0.52397\n",
      " Batch 130 : Average Test Accuracy = 0.51603\n",
      " Batch 140 : Average Test Accuracy = 0.52766\n",
      " Batch 150 : Average Test Accuracy = 0.52583\n",
      " Batch 160 : Average Test Accuracy = 0.52671\n",
      " Batch 170 : Average Test Accuracy = 0.52398\n",
      " Batch 180 : Average Test Accuracy = 0.52265\n",
      " Batch 190 : Average Test Accuracy = 0.51728\n",
      " Batch 200 : Average Test Accuracy = 0.51642\n",
      " Batch 210 : Average Test Accuracy = 0.51943\n",
      " Batch 220 : Average Test Accuracy = 0.51765\n",
      " Batch 230 : Average Test Accuracy = 0.51429\n",
      " Batch 240 : Average Test Accuracy = 0.51618\n",
      " Batch 250 : Average Test Accuracy = 0.51793\n",
      " Batch 260 : Average Test Accuracy = 0.51877\n",
      " Batch 270 : Average Test Accuracy = 0.52325\n",
      " Batch 280 : Average Test Accuracy = 0.52598\n",
      " Batch 290 : Average Test Accuracy = 0.52852\n",
      " Batch 300 : Average Test Accuracy = 0.52757\n",
      " Batch 310 : Average Test Accuracy = 0.52669\n",
      " Batch 320 : Average Test Accuracy = 0.52773\n",
      " Batch 330 : Average Test Accuracy = 0.52749\n",
      " Batch 340 : Average Test Accuracy = 0.53021\n",
      " Batch 350 : Average Test Accuracy = 0.52707\n",
      " Batch 360 : Average Test Accuracy = 0.52964\n",
      " Batch 370 : Average Test Accuracy = 0.53046\n",
      " Batch 380 : Average Test Accuracy = 0.53281\n",
      " Batch 390 : Average Test Accuracy = 0.53043\n",
      " Batch 400 : Average Test Accuracy = 0.53067\n",
      " Batch 410 : Average Test Accuracy = 0.53285\n",
      " Batch 420 : Average Test Accuracy = 0.53159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2273374/522323021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# test_output = model(batch[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2273374/1505274885.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlstm_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mp_pooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mh_pooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2273374/909463369.py\u001b[0m in \u001b[0;36mpooling\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtemp_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnew_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I interrupted this because it is slow and with the 400+ batches we get a good idea of the accuracy either way.\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct = 0\n",
    "    counter = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        test_output = model(batch[0], batch[1])\n",
    "        # test_output = model(batch[1])\n",
    "        test_output = torch.argmax(test_output, dim=1)\n",
    "        targets = torch.tensor(batch[2], device=device)\n",
    "        correct += torch.sum(test_output == targets)\n",
    "        counter += len(test_output)\n",
    "\n",
    "        test_accu = correct/counter\n",
    "        \n",
    "        if i%10==0:\n",
    "            print(f' Batch {i} : Average Test Accuracy = {round(float(test_accu), 5)}')\n",
    "\n",
    "    print(f'Total Test Accuracy = {round(float(test_accu), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, given that we have 4 classes ('entailment', 'contradiction', 'neutral', '-'), if we guess blindly, we should have 25% accuracy. However, this conclusion assumes that the classes are evenly spread out, which is not the case. While inspecting the files one can notice that the '-' class is rather rare (and should probably have been excluded). Thus, given the three basic classes (mentioned as such in the task description above, by the way), guessing the same class every time should give us a **33% accuracy** (as these three are rather evenly spread out). We would suggest this to be our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[4 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We could look at the model's performance per class / category, as we know that they a) differ in frequency, with '-' being a unique case and b) the 'neutral' case is notoriously difficult for NLI models to predict. Thus, we could see if this is indeed the case here.\n",
    "+ Having done that, we could compare that performance to other BiLSTM models' performance in general and in these particular classes.\n",
    "+ We could look at class-wise precision and recall to see if our model is underpredicting or overpredicting certain classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We are pretty sure that we should remove the problematic lines from the data - the ones where the class is marked as '-' as well as the ones where one of the sentences is marked as N/A, which translated to NaN when the file was loaded in and caused all sorts of issues (and was replaced by one unknown token). \n",
    "+ We are unsure if the dropout rate we set was a good one, or if we set it in the right place.\n",
    "+ We could test different hyperparameters and see which ones are best for the model.\n",
    "+ We could make use of pre-trained word embeddings instead of initializing them from scratch.\n",
    "+ We could experiment with the number of different layers aside from LSTM and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
