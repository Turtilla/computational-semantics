{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "\n",
    "#other packages that we are going to use\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and unzip: https://github.com/sdobnik/computational-semantics/blob/master/assignments/05-natural-language-inference/simple_snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path_to_snli):\n",
    "    # your code goes here\n",
    "    field_premise = torchtext.data.Field(lower=True, batch_first=True)\n",
    "    field_hypothesis = torchtext.data.Field(lower=True, batch_first=True)\n",
    "    field_relation = torchtext.data.Field(lower=True, batch_first=True, pad_token=False)\n",
    "    \n",
    "    train_dataset, dev_dataset, test_dataset = torchtext.data.TabularDataset.splits(path_to_snli,\n",
    "                                                   train = 'simple_snli_1.0_train.csv', \n",
    "                                                   validation = 'simple_snli_1.0_dev.csv',\n",
    "                                                   test = 'simple_snli_1.0_test.csv',\n",
    "                                                   format = 'csv', \n",
    "                                                   fields = [('premise', field_premise),\n",
    "                                                    ('hypothesis', field_hypothesis),\n",
    "                                                    ('relation', field_relation)],\n",
    "                                                   csv_reader_params = {'delimiter':'\\t', 'quotechar': 'Â½'})\n",
    "    # adding the csv_reader_params as per your POS tutorial caused issues, so we decided not to include that in this notebook.\n",
    "    \n",
    "    field_premise.build_vocab(train_dataset)\n",
    "    field_hypothesis.build_vocab(train_dataset)\n",
    "    field_relation.build_vocab(train_dataset)\n",
    "    \n",
    "    train_iterator, dev_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "                                                                        (train_dataset, dev_dataset, test_dataset), \n",
    "                                                                         batch_size = batch_size,\n",
    "                                                                         sort=False,\n",
    "                                                                         repeat=True, \n",
    "                                                                         shuffle=True, \n",
    "                                                                         device=device)\n",
    "    \n",
    "    return train_iterator, dev_iterator, test_iterator, field_premise, field_hypothesis, field_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test, premises, hypotheses, relations = dataloader('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relations.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contradiction', 'neutral', '-', 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "# I do not understand why the relations.vocab has the length of 6: as you can see below, there are only 4 relations + pad token\n",
    "# (setting it to False in dataloader dropped the size to 5, but I still do not get what the last element is). I also cannot find\n",
    "# a working way to check what is INSIDE the vocabulary, the methods of torchtext.vocab.Vocab do not work with this.\n",
    "import csv\n",
    "relation = set()\n",
    "with open('simple_snli_1.0_dev.csv', newline='') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "     for row in spamreader:\n",
    "        relation.add(row[2])\n",
    "        \n",
    "with open('simple_snli_1.0_test.csv', newline='') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "     for row in spamreader:\n",
    "        relation.add(row[2])\n",
    "        \n",
    "with open('simple_snli_1.0_train.csv', newline='') as csvfile:\n",
    "     spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "     for row in spamreader:\n",
    "        relation.add(row[2])\n",
    "        \n",
    "print(relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0018e-01, 9.3254e-01, 1.5991e-01, 5.4113e-01, 4.4711e-01],\n",
      "         [1.8128e-01, 5.7962e-02, 8.4768e-01, 6.8903e-01, 3.0194e-01],\n",
      "         [1.8579e-01, 6.8336e-01, 3.2044e-01, 8.5323e-01, 3.0906e-01]],\n",
      "\n",
      "        [[2.6510e-02, 1.0137e-01, 8.2373e-01, 3.3546e-01, 1.8819e-01],\n",
      "         [2.1763e-01, 2.1628e-01, 2.4722e-01, 6.7042e-01, 3.1387e-01],\n",
      "         [9.9254e-01, 6.1488e-01, 7.0384e-01, 7.8223e-01, 1.4766e-01]],\n",
      "\n",
      "        [[2.2560e-01, 9.4026e-01, 3.6296e-01, 3.3529e-02, 5.8864e-01],\n",
      "         [9.2015e-01, 8.4432e-01, 4.7033e-01, 9.9194e-01, 6.3490e-01],\n",
      "         [7.0116e-01, 5.0775e-01, 4.4205e-01, 8.1613e-01, 2.9596e-01]],\n",
      "\n",
      "        [[6.1439e-01, 7.6977e-01, 4.4447e-01, 9.8468e-01, 9.3032e-01],\n",
      "         [5.9445e-01, 5.2541e-02, 8.8084e-01, 3.3992e-01, 5.5538e-02],\n",
      "         [1.5560e-01, 4.9737e-02, 5.3115e-01, 8.9348e-02, 8.8493e-01]],\n",
      "\n",
      "        [[1.4212e-01, 1.8661e-01, 1.9368e-02, 2.4984e-01, 2.1182e-01],\n",
      "         [9.1856e-04, 6.3179e-01, 4.9441e-01, 9.0327e-01, 9.1718e-01],\n",
      "         [2.6776e-01, 7.5228e-01, 4.4630e-02, 8.4669e-01, 8.8142e-01]],\n",
      "\n",
      "        [[4.2764e-01, 6.7957e-01, 2.9370e-01, 9.3417e-01, 3.3417e-01],\n",
      "         [1.0673e-01, 5.8799e-01, 2.5525e-01, 7.9136e-01, 8.2208e-02],\n",
      "         [2.5013e-01, 8.3441e-01, 5.8773e-01, 4.9604e-01, 4.5520e-01]],\n",
      "\n",
      "        [[1.3755e-01, 6.7474e-01, 8.0271e-01, 2.1290e-01, 8.5851e-01],\n",
      "         [4.9293e-02, 9.5266e-02, 8.2555e-01, 1.1215e-01, 4.6641e-01],\n",
      "         [3.7104e-01, 4.1070e-01, 7.2219e-01, 7.8004e-01, 4.1558e-01]],\n",
      "\n",
      "        [[3.9889e-01, 8.1014e-01, 2.4395e-01, 7.7284e-01, 9.7572e-01],\n",
      "         [3.0810e-02, 8.2109e-01, 7.5926e-01, 3.2611e-01, 4.6215e-01],\n",
      "         [8.6430e-02, 7.6193e-01, 6.5723e-01, 7.1794e-01, 5.3225e-01]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "dimensions = 5\n",
    "# A tensor for testing\n",
    "test_tensor = torch.rand([batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(input_tensor):\n",
    "    batches = len(input_tensor)\n",
    "    words = len(input_tensor[0])\n",
    "    dims = len(input_tensor[0][0])\n",
    "\n",
    "    new_tensors = []\n",
    "\n",
    "    for i in range(0,batches):\n",
    "        new_tensor = []\n",
    "\n",
    "        for j in range(0,dims):\n",
    "            temp_tensor = []\n",
    "        \n",
    "            for k in range(0,words):\n",
    "                temp_tensor.append(input_tensor[i][k][j])\n",
    "            \n",
    "            max_val = max(temp_tensor)\n",
    "            new_tensor.append(max_val)\n",
    "    \n",
    "        actual_new_tensor = new_tensor[0].unsqueeze(0)\n",
    "    \n",
    "        for l in range(1,len(new_tensor)):\n",
    "            actual_new_tensor = torch.cat((actual_new_tensor, new_tensor[l].unsqueeze(0)))\n",
    "\n",
    "        new_tensors.append(actual_new_tensor)\n",
    "    \n",
    "    output_tensor = torch.stack(new_tensors)\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6002, 0.9325, 0.8477, 0.8532, 0.4471],\n",
       "        [0.9925, 0.6149, 0.8237, 0.7822, 0.3139],\n",
       "        [0.9201, 0.9403, 0.4703, 0.9919, 0.6349],\n",
       "        [0.6144, 0.7698, 0.8808, 0.9847, 0.9303],\n",
       "        [0.2678, 0.7523, 0.4944, 0.9033, 0.9172],\n",
       "        [0.4276, 0.8344, 0.5877, 0.9342, 0.4552],\n",
       "        [0.3710, 0.6747, 0.8255, 0.7800, 0.8585],\n",
       "        [0.3989, 0.8211, 0.7593, 0.7728, 0.9757]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7285, 0.5180, 0.6645, 0.9100, 0.9208],\n",
      "        [0.9668, 0.9276, 0.4489, 0.7350, 0.8521],\n",
      "        [0.7451, 0.9273, 0.6983, 0.8307, 0.8093],\n",
      "        [0.8488, 0.6891, 0.7451, 0.9290, 0.5603],\n",
      "        [0.9258, 0.8266, 0.5683, 0.6003, 0.1685],\n",
      "        [0.4605, 0.6107, 0.6603, 0.6723, 0.1351],\n",
      "        [0.9333, 0.6884, 0.6262, 0.6547, 0.6435],\n",
      "        [0.3485, 0.7087, 0.6973, 0.8247, 0.7439]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([[0.8100, 0.7544, 0.9747, 0.9113, 0.9875],\n",
      "        [0.6869, 0.9990, 0.9677, 0.9382, 0.8886],\n",
      "        [0.9550, 0.4418, 0.3069, 0.9933, 0.6600],\n",
      "        [0.9891, 0.9923, 0.8862, 0.4801, 0.8949],\n",
      "        [0.8439, 0.9095, 0.9705, 0.9383, 0.8354],\n",
      "        [0.6751, 0.8652, 0.3123, 0.5073, 0.4134],\n",
      "        [0.7555, 0.7993, 0.2765, 0.8148, 0.7798],\n",
      "        [0.9715, 0.6143, 0.4070, 0.9172, 0.7147]], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Test tensors (size of batch, num, dim)\n",
    "t = torch.rand([2*batch_size, num_words, dimensions], dtype=torch.float64, device=device)\n",
    "t1, t2 = torch.split(t, batch_size)\n",
    "# Pooled test tensors (size of batch, dim)\n",
    "pt1 = pooling(t1)\n",
    "pt2 = pooling(t2)\n",
    "print(pt1)\n",
    "print(pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    \n",
    "    batches = len(hypothesis)\n",
    "    dims = len(hypothesis[0])\n",
    "    final_dims = 4*dims\n",
    "\n",
    "    new_tensors = []\n",
    "\n",
    "    for i in range(0,batches):\n",
    "        hyp = hypothesis[i]\n",
    "        pre = premise[i]\n",
    "    \n",
    "        summed = torch.cat((pre,hyp))\n",
    "        subtracted = pre - hyp\n",
    "        multiplied = torch.mul(pre, hyp)\n",
    "    \n",
    "        new_tensors.append(torch.cat((summed, subtracted, multiplied)))\n",
    "    \n",
    "    output = torch.stack(new_tensors)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8100,  0.7544,  0.9747,  0.9113,  0.9875,  0.7285,  0.5180,  0.6645,\n",
       "          0.9100,  0.9208,  0.0815,  0.2364,  0.3102,  0.0013,  0.0667,  0.5901,\n",
       "          0.3908,  0.6477,  0.8293,  0.9092],\n",
       "        [ 0.6869,  0.9990,  0.9677,  0.9382,  0.8886,  0.9668,  0.9276,  0.4489,\n",
       "          0.7350,  0.8521, -0.2799,  0.0714,  0.5188,  0.2032,  0.0366,  0.6641,\n",
       "          0.9267,  0.4344,  0.6896,  0.7571],\n",
       "        [ 0.9550,  0.4418,  0.3069,  0.9933,  0.6600,  0.7451,  0.9273,  0.6983,\n",
       "          0.8307,  0.8093,  0.2099, -0.4856, -0.3914,  0.1626, -0.1493,  0.7115,\n",
       "          0.4097,  0.2143,  0.8251,  0.5341],\n",
       "        [ 0.9891,  0.9923,  0.8862,  0.4801,  0.8949,  0.8488,  0.6891,  0.7451,\n",
       "          0.9290,  0.5603,  0.1404,  0.3033,  0.1410, -0.4490,  0.3347,  0.8396,\n",
       "          0.6838,  0.6603,  0.4460,  0.5014],\n",
       "        [ 0.8439,  0.9095,  0.9705,  0.9383,  0.8354,  0.9258,  0.8266,  0.5683,\n",
       "          0.6003,  0.1685, -0.0818,  0.0829,  0.4023,  0.3380,  0.6670,  0.7813,\n",
       "          0.7517,  0.5515,  0.5632,  0.1408],\n",
       "        [ 0.6751,  0.8652,  0.3123,  0.5073,  0.4134,  0.4605,  0.6107,  0.6603,\n",
       "          0.6723,  0.1351,  0.2146,  0.2545, -0.3479, -0.1651,  0.2782,  0.3108,\n",
       "          0.5284,  0.2062,  0.3410,  0.0559],\n",
       "        [ 0.7555,  0.7993,  0.2765,  0.8148,  0.7798,  0.9333,  0.6884,  0.6262,\n",
       "          0.6547,  0.6435, -0.1778,  0.1110, -0.3497,  0.1601,  0.1363,  0.7051,\n",
       "          0.5502,  0.1731,  0.5334,  0.5018],\n",
       "        [ 0.9715,  0.6143,  0.4070,  0.9172,  0.7147,  0.3485,  0.7087,  0.6973,\n",
       "          0.8247,  0.7439,  0.6230, -0.0944, -0.2902,  0.0925, -0.0292,  0.3385,\n",
       "          0.4354,  0.2838,  0.7564,  0.5317]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_premise_and_hypothesis(pt1, pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        # your code goes here\n",
    "        self.embeddings = ...\n",
    "        self.rnn = ...\n",
    "        self.classifier = ...\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = ...\n",
    "        h = ...\n",
    "        \n",
    "        p_pooled = pooling(...)\n",
    "        h_pooled = pooling(...)\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(...)\n",
    "        predictions = ...\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = dataloader(path_to_folder)\n",
    "\n",
    "loss_function = ...\n",
    "optimizer = ...\n",
    "model = ...\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # train model\n",
    "    ...\n",
    "    \n",
    "# test model after all epochs are completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[4 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
